{
  "hash": "43d74d6a8b0f42505931d44113f7cd59",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute:\n  eval: false\n\nengine: knitr\n---\n\n\n# Workflow for sequence classification\n\n## Project description\n\nNanoClass2 is used to classify 16S rRNA amplicon sequences from 21 samples. DNA samples were taken from Winogradksy columns with wood or paper as substrate. \n\nFor each substrate 3 independent columns were sampled and for each columns there are 3-4 replicates. \nDNA was sampled on two days, by two different groups of students (i.e. Practical_groups 1 and 2):\n\n\n| #NAME | Sample  | Carbon_source | Wino_Column | Practical_group |\n| ----- | ------- | ------------- | ----------- | --------------- |\n| BC22  | GM11    | paper         | No          | 2               |\n| BC17  | KB-P1   | paper         | P1          | 1               |\n| BC18  | WB-P1   | paper         | P1          | 1               |\n| BC2   | SB-P1   | paper         | P1          | 2               |\n| BC1   | LK-P2   | paper         | P2          | 2               |\n| BC16  | RvO-P2  | paper         | P2          | 1               |\n| BC20  | IV-P2   | paper         | P2          | 1               |\n| BC15  | EK-P3   | paper         | P3          | 1               |\n| BC19  | NK-P3   | paper         | P3          | 1               |\n| BC3   | PM-P3   | paper         | P3          | 2               |\n| BC10  | EP-SD1  | wood          | SD1         | 1               |\n| BC12  | RK-SD1  | wood          | SD1         | 1               |\n| BC23  | EW-SD1  | wood          | SD1         | 2               |\n| BC24  | DG-SD1  | wood          | SD1         | 2               |\n| BC5   | AW-SD2  | wood          | SD2         | 2               |\n| BC6   | UNZ-SD2 | wood          | SD2         | 2               |\n| BC7   | DH-SD2  | wood          | SD2         | 2               |\n| BC11  | AL-SD3  | wood          | SD3         | 1               |\n| BC13  | BS-SD3  | wood          | SD3         | 1               |\n| BC14  | DSR-SD3 | wood          | SD3         | 1               |\n| BC9   | BF-SD3  | wood          | SD3         | 1               |\n\n\n## Dependencies\n\n-   python v3.10.12\n-   sed (GNU sed) 4.5\n-   GNU Awk 4.2.1, API: 2.0 (GNU MPFR 3.1.6-p2, GNU MP 6.1.2)\n-   seqkit v2.7.0 \n-   Nanoplot v1.42.0\n-   NanoClass2 v0.1\n\n\n## Folder setup\n\nData was analysed on the Uva Crunchomics HPC:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Go to working directory\nwdir=\"/home/ndombro/personal/teaching/2024/miceco\"\ncd $wdir\n```\n:::\n\n\n\n## Prepare input files\n\n### Inspect data\n\nWe work with the following data:\n\n- Sequencing data was provided as zipped folder by Peter Kuperus Oktober 22, 2024 via SurfDrive. \n- The file `filelists/barcode_to_sample` was generated manually from the mapping file provided by Gerard Muyzer and links the barcode to the sample ID.\n\nThe data was uploaded to Crunchomics and the barcode IDs were extracted from the file names to be able to loop through individual samples:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Make data folders\nmkdir data \nmkdir filelists \n\n# Upload sequencing data from local PC to Crunchomics\nscp data/mic2024.zip crunchomics:/home/ndombro/personal/teaching/2024/miceco/data\n\n# Unzip data folder\nunzip data/mic2024.zip\nmv mic2024 data\n\n# Make a list of barcodes\nls data/mic2024/*/fastq_pass/barcode*/*fastq.gz | \\\n  sed 's/.*barcode\\([0-9]\\+\\).*/barcode\\1/' \\\n  | sort -u > filelists/barcodes.txt\n\n# We work with so many barcodes: 21\nwc -l filelists/barcodes.txt\n\n# Count number of files we work with per barcode\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/mic2024/*/fastq_pass/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n```\n:::\n\n\n**Results**\n\nWe work with 21 barcodes and for each barcode we work with several fastq.gz files:\n\n```\nBarcode barcode01 has 145 fastq files\nBarcode barcode02 has 145 fastq files\nBarcode barcode03 has 145 fastq files\nBarcode barcode05 has 145 fastq files\nBarcode barcode06 has 144 fastq files\nBarcode barcode07 has 145 fastq files\nBarcode barcode09 has 132 fastq files\nBarcode barcode10 has 132 fastq files\nBarcode barcode11 has 132 fastq files\nBarcode barcode12 has 132 fastq files\nBarcode barcode13 has 132 fastq files\nBarcode barcode14 has 132 fastq files\nBarcode barcode15 has 132 fastq files\nBarcode barcode16 has 132 fastq files\nBarcode barcode17 has 132 fastq files\nBarcode barcode18 has 132 fastq files\nBarcode barcode19 has 3 fastq files\nBarcode barcode20 has 132 fastq files\nBarcode barcode22 has 89 fastq files\nBarcode barcode23 has 145 fastq files\nBarcode barcode24 has 144 fastq files\n```\n\nWe can see that:\n\n- for each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\n- Barcode19 seems to have less files, thus we need to keep this in mind and observe read counts more carefully for that sample\n\n\n\n### Combine individual files\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Generate folders\nmkdir data/combined_data\n\n# Combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/mic2024/*/fastq_pass/${i}/*fastq.gz > data/combined_data/${i}.fastq.gz\ndone\n\n# Sanity check: We work with 21 combined files\nll data/combined_data/* | wc -l\n```\n:::\n\n\n\n\n## Do quality control\n\n### Calculate read counts\n\nNext, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/mic2024/*/fastq_pass/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n```\n:::\n\n\n**Results**\n\n- We have so many samples: 21\n- We have so many reads in total: 413,614\n- On average we have: 19,695 reads\n- Notice that for barcode 19 and 22 we retain only very few read counts\n\nRead counts before and after merging individual files:\n\n```\nTotal reads for barcode01 - Before: 60759, After: 60759\nTotal reads for barcode02 - Before: 78436, After: 78436\nTotal reads for barcode03 - Before: 28991, After: 28991\nTotal reads for barcode05 - Before: 4852, After: 4852\nTotal reads for barcode06 - Before: 17883, After: 17883\nTotal reads for barcode07 - Before: 18625, After: 18625\nTotal reads for barcode09 - Before: 12756, After: 12756\nTotal reads for barcode10 - Before: 15666, After: 15666\nTotal reads for barcode11 - Before: 15038, After: 15038\nTotal reads for barcode12 - Before: 7512, After: 7512\nTotal reads for barcode13 - Before: 11099, After: 11099\nTotal reads for barcode14 - Before: 6846, After: 6846\nTotal reads for barcode15 - Before: 14785, After: 14785\nTotal reads for barcode16 - Before: 18654, After: 18654\nTotal reads for barcode17 - Before: 16957, After: 16957\nTotal reads for barcode18 - Before: 30101, After: 30101\nTotal reads for barcode19 - Before: 3, After: 3\nTotal reads for barcode20 - Before: 16945, After: 16945\nTotal reads for barcode22 - Before: 109, After: 109\nTotal reads for barcode23 - Before: 31428, After: 31428\nTotal reads for barcode24 - Before: 6169, After: 6169\n``` \n\n\n### Generate summary statistics\n\nNext, we calculate the quality statistics (total read count, average read count, read length, etc) to be able to screen the data for potential issues:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Make data folders\nmkdir -p results/seqkit\nmkdir -p results/nanoplot\n\n# Run seqkit\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/combined_data/*fastq.gz --threads 10\n\n# Generate plots to visualize the statistics better\nconda activate nanoplot_1.42.0\n\nfor file in data/combined_data/*fastq.gz; do\n  sample=$(echo $file | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\")\n  echo \"Starting analysis for \"$file\"\"\n  mkdir results/nanoplot/\"$sample\"\n  srun --cpus-per-task 10 --mem=50G \\\n    NanoPlot --fastq $file -o results/nanoplot/\"$sample\" --threads 10\ndone\n\nconda deactivate\n```\n:::\n\n\n**Summary**\n\n- Reads are on average 1358 bp long with Q1: 1374, Q2: 1425 and Q3: 1454. \n- Visualizing the plots, \n  - the majority of the data was hovering around 1400 bp\n  - The read quality was more spread and and went from 10 to max 18\n- Conclusion: \n  - No sample had shorter than expected amplicons\n  - Two samples had less than 200 reads and need to be removed at some point of the pipeline\n  - I will use phred score cutoff ~10\n  - I will use length cutoffs of 1100 bp (min) and 1600 bp (max) based on the Q1 and Q3 information from seqkit\n\n\n## Run NanoClass2\n\nNanoClass2 will be used for quality cleaning and to classify the sequence reads for each sample.\n\nTo run NanoClass2 we need:\n\n- The sequence files in fastq.gz format (one file per sample, which we already generated)\n- A csv mapping file that lists what samples you want to have analysed and how the samples are named\n- A config file in which we specify with what parameters we want to run NanoClass2\n- A bash file that allows us to run NanoClass2 on an HPC\n\n\n### Prepare the mapping file\n\nThe csv mapping file needs to include the following:\n\n- A run name\n- A unique sample name\n- The barcode name (optional)\n- The path to your fastq.gz files (should be already demultiplexed)\n- Notice: Sample and run labels can only contain letters and numbers\n\nFor this analysis the file looks something like this:\n\n```\nrun,sample,barcode,path\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode01.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode02.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode03.fastq.gz\n```\n\nThis file can be generatd in excel, but we can also extract all the info we need from the file path:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\necho \"run,sample,barcode,path\" > filelists/mapping_file.csv\n\nls data/combined_data/*fastq.gz | \\\nwhile read path; do\n  run=\"mic2024\" # Set static run name\n  sample=$(echo \"$path\" | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\") # Extract sampleID\n  barcode=$(echo \"$sample\" | sed \"s/barcode/BC/g\")\n  fullpath=$(realpath \"$path\")  # To get the full absolute path\n  echo \"$run,$barcode,,$fullpath\" # Combine all data\ndone >> filelists/mapping_file.csv\n```\n:::\n\n\n\n### Prepare the config file\n\nThe Snakemake configuration file, i.e. `config.yaml`, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\n- The location of your sample mapping file under `samples:`\n- The methods you want to use \n- The min, max read length to keep\n- The quality phred score to keep\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Copy the required files from the NanoClass2 software folder\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/jobscript.sh .\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/config.yaml .\n```\n:::\n\n\nChanges made to the config file:\n\n```\nsamples:                           \"filelists/mapping_file.csv\"\n\nmethods:                           [\"minimap\"]\n```\n\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\n\n```\nsubsample   \n  skip:                          true\n``` \n\nBased on the quality checking I also made some changes. I used the lowest/highest seqkit Q1/Q3 data to set thresholds for the length.\n\n```\n    minlen:                        1100\n    maxlen:                        1600\n    quality:                       10\n```\n\n\n### Run NanoClass2\n\nTo run NanoClass2 on the Crunchomics HPC, the jobscript.sh file can be used without any edits. This script is pre-configured to do everything for you such as:\n\n- Load the correct environments that have all required tools installed\n- Start NanoClass2 using snakemake, a job scheduler that takes care of what software is run when. Snakemake is useful as it allows to run things in parallel as long as there are enough resources available. Snakemake also allows to re-start a job and it will simply pick things up from whereever things went wrong.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Do a dry-run to see if we set everything correctly \n# Since this is just a test, we can run this on the headnode \nconda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\n\nsnakemake \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml --use-conda \\\n    --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --cores 1 --nolock --rerun-incomplete -np\n\n# Submit job via a job script to the compute node: job id is 74581\nsbatch jobscript.sh\n\n# Generate a report out of this\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile\n\nconda deactivate \n\n# Run seqkit on the cleaned data\nseqkit stats -a -To results/seqkit/seqkit_stats_filtered.tsv results/data/mic2024/chopper/BC*gz --threads 10\n\n# Replace barcode with sample IDs in the otu table\nawk -v OFS=\"\\t\" 'NR==FNR {mapping[$1]=$2; next} {for (i=1; i<=NF; i++) if ($i in mapping) $i=mapping[$i]; print}' \\\n  filelists/barcode_to_sample \\\n  <(sed \"s/mic2024_minimap_//g\" results/tables/otu-table.tsv) \\\n  > results/tables/otu-table-updated.txt\n\n# Make the header compatible for MicrobiomeAnalyst\nsed -i \"s/taxid/\\#NAME/g\" results/tables/otu-table-updated.txt\n\n# Remove Gerards sample\npython scripts/filter_columns.py -i results/tables/otu-table-updated.txt \\\n  -c GM11 \\\n  -o results/tables/otu-table-forMA.txt\n```\n:::\n\n\n**Comments**\n\n- Looking at the seqkit stats we went from:\n  - on average 19,695 to 17,513 reads\n  - total 413,614 to 367,773 reads\n  - total 561,031,890 to 520,739,003 bp\n- Link to the report is [here](../../results/nanoclass/report.html)",
    "supporting": [
      "nanoclass_workflow_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}