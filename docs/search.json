[
  {
    "objectID": "code/nanoclass_workflow.html",
    "href": "code/nanoclass_workflow.html",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "NanoClass2 is used to classify 16S rRNA amplicon sequences from 21 samples. DNA samples were taken from Winogradksy columns with wood or paper as substrate.\nFor each substrate 3 independent columns were sampled and for each columns there are 3-4 replicates. DNA was sampled on two days, by two different groups of students (i.e. Practical_groups 1 and 2):\n\n\n\n#NAME\nSample\nCarbon_source\nWino_Column\nPractical_group\n\n\n\n\nBC22\nGM11\npaper\nNo\n2\n\n\nBC17\nKB-P1\npaper\nP1\n1\n\n\nBC18\nWB-P1\npaper\nP1\n1\n\n\nBC2\nSB-P1\npaper\nP1\n2\n\n\nBC1\nLK-P2\npaper\nP2\n2\n\n\nBC16\nRvO-P2\npaper\nP2\n1\n\n\nBC20\nIV-P2\npaper\nP2\n1\n\n\nBC15\nEK-P3\npaper\nP3\n1\n\n\nBC19\nNK-P3\npaper\nP3\n1\n\n\nBC3\nPM-P3\npaper\nP3\n2\n\n\nBC10\nEP-SD1\nwood\nSD1\n1\n\n\nBC12\nRK-SD1\nwood\nSD1\n1\n\n\nBC23\nEW-SD1\nwood\nSD1\n2\n\n\nBC24\nDG-SD1\nwood\nSD1\n2\n\n\nBC5\nAW-SD2\nwood\nSD2\n2\n\n\nBC6\nUNZ-SD2\nwood\nSD2\n2\n\n\nBC7\nDH-SD2\nwood\nSD2\n2\n\n\nBC11\nAL-SD3\nwood\nSD3\n1\n\n\nBC13\nBS-SD3\nwood\nSD3\n1\n\n\nBC14\nDSR-SD3\nwood\nSD3\n1\n\n\nBC9\nBF-SD3\nwood\nSD3\n1\n\n\n\n\n\n\n\npython v3.10.12\nsed (GNU sed) 4.5\nGNU Awk 4.2.1, API: 2.0 (GNU MPFR 3.1.6-p2, GNU MP 6.1.2)\nseqkit v2.7.0\nNanoplot v1.42.0\nNanoClass2 v0.1\n\n\n\n\nData was analysed on the Uva Crunchomics HPC:\n\n# Go to working directory\nwdir=\"/home/ndombro/personal/teaching/2024/miceco\"\ncd $wdir\n\n\n\n\n\n\nWe work with the following data:\n\nSequencing data was provided as zipped folder by Peter Kuperus Oktober 22, 2024 via SurfDrive.\nThe file filelists/barcode_to_sample was generated manually from the mapping file provided by Gerard Muyzer and links the barcode to the sample ID.\n\nThe data was uploaded to Crunchomics and the barcode IDs were extracted from the file names to be able to loop through individual samples:\n\n# Make data folders\nmkdir data \nmkdir filelists \n\n# Upload sequencing data from local PC to Crunchomics\nscp data/mic2024.zip crunchomics:/home/ndombro/personal/teaching/2024/miceco/data\n\n# Unzip data folder\nunzip data/mic2024.zip\nmv mic2024 data\n\n# Make a list of barcodes\nls data/mic2024/*/fastq_pass/barcode*/*fastq.gz | \\\n  sed 's/.*barcode\\([0-9]\\+\\).*/barcode\\1/' \\\n  | sort -u &gt; filelists/barcodes.txt\n\n# We work with so many barcodes: 21\nwc -l filelists/barcodes.txt\n\n# Count number of files we work with per barcode\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/mic2024/*/fastq_pass/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\nWe work with 21 barcodes and for each barcode we work with several fastq.gz files:\nBarcode barcode01 has 145 fastq files\nBarcode barcode02 has 145 fastq files\nBarcode barcode03 has 145 fastq files\nBarcode barcode05 has 145 fastq files\nBarcode barcode06 has 144 fastq files\nBarcode barcode07 has 145 fastq files\nBarcode barcode09 has 132 fastq files\nBarcode barcode10 has 132 fastq files\nBarcode barcode11 has 132 fastq files\nBarcode barcode12 has 132 fastq files\nBarcode barcode13 has 132 fastq files\nBarcode barcode14 has 132 fastq files\nBarcode barcode15 has 132 fastq files\nBarcode barcode16 has 132 fastq files\nBarcode barcode17 has 132 fastq files\nBarcode barcode18 has 132 fastq files\nBarcode barcode19 has 3 fastq files\nBarcode barcode20 has 132 fastq files\nBarcode barcode22 has 89 fastq files\nBarcode barcode23 has 145 fastq files\nBarcode barcode24 has 144 fastq files\nWe can see that:\n\nfor each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\nBarcode19 seems to have less files, thus we need to keep this in mind and observe read counts more carefully for that sample\n\n\n\n\n\n# Generate folders\nmkdir data/combined_data\n\n# Combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/mic2024/*/fastq_pass/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n# Sanity check: We work with 21 combined files\nll data/combined_data/* | wc -l\n\n\n\n\n\n\n\nNext, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/mic2024/*/fastq_pass/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 21\nWe have so many reads in total: 413,614\nOn average we have: 19,695 reads\nNotice that for barcode 19 and 22 we retain only very few read counts\n\nRead counts before and after merging individual files:\nTotal reads for barcode01 - Before: 60759, After: 60759\nTotal reads for barcode02 - Before: 78436, After: 78436\nTotal reads for barcode03 - Before: 28991, After: 28991\nTotal reads for barcode05 - Before: 4852, After: 4852\nTotal reads for barcode06 - Before: 17883, After: 17883\nTotal reads for barcode07 - Before: 18625, After: 18625\nTotal reads for barcode09 - Before: 12756, After: 12756\nTotal reads for barcode10 - Before: 15666, After: 15666\nTotal reads for barcode11 - Before: 15038, After: 15038\nTotal reads for barcode12 - Before: 7512, After: 7512\nTotal reads for barcode13 - Before: 11099, After: 11099\nTotal reads for barcode14 - Before: 6846, After: 6846\nTotal reads for barcode15 - Before: 14785, After: 14785\nTotal reads for barcode16 - Before: 18654, After: 18654\nTotal reads for barcode17 - Before: 16957, After: 16957\nTotal reads for barcode18 - Before: 30101, After: 30101\nTotal reads for barcode19 - Before: 3, After: 3\nTotal reads for barcode20 - Before: 16945, After: 16945\nTotal reads for barcode22 - Before: 109, After: 109\nTotal reads for barcode23 - Before: 31428, After: 31428\nTotal reads for barcode24 - Before: 6169, After: 6169\n\n\n\nNext, we calculate the quality statistics (total read count, average read count, read length, etc) to be able to screen the data for potential issues:\n\n# Make data folders\nmkdir -p results/seqkit\nmkdir -p results/nanoplot\n\n# Run seqkit\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/combined_data/*fastq.gz --threads 10\n\n# Generate plots to visualize the statistics better\nconda activate nanoplot_1.42.0\n\nfor file in data/combined_data/*fastq.gz; do\n  sample=$(echo $file | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\")\n  echo \"Starting analysis for \"$file\"\"\n  mkdir results/nanoplot/\"$sample\"\n  srun --cpus-per-task 10 --mem=50G \\\n    NanoPlot --fastq $file -o results/nanoplot/\"$sample\" --threads 10\ndone\n\nconda deactivate\n\nSummary\n\nReads are on average 1358 bp long with Q1: 1374, Q2: 1425 and Q3: 1454\nVisualizing the plots,\n\nthe majority of the data was hovering around 1400 bp\nThe read quality was more spread and and went from 10 to max 18\n\n\n\n\n\n\nNanoClass2 will be used for quality cleaning and to classify the sequence reads for each sample.\nTo run NanoClass2 we need:\n\nThe sequence files in fastq.gz format (one file per sample, which we already generated)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\nA config file in which we specify with what parameters we want to run NanoClass2\nA bash file that allows us to run NanoClass2 on an HPC\n\n\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor this analysis the file looks something like this:\nrun,sample,barcode,path\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode01.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode02.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode03.fastq.gz\nThis file can be generatd in excel, but we can also extract all the info we need from the file path:\n\necho \"run,sample,barcode,path\" &gt; filelists/mapping_file.csv\n\nls data/combined_data/*fastq.gz | \\\nwhile read path; do\n  run=\"mic2024\" # Set static run name\n  sample=$(echo \"$path\" | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\") # Extract sampleID\n  barcode=$(echo \"$sample\" | sed \"s/barcode/BC/g\")\n  fullpath=$(realpath \"$path\")  # To get the full absolute path\n  echo \"$run,$barcode,,$fullpath\" # Combine all data\ndone &gt;&gt; filelists/mapping_file.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use\nThe min, max read length to keep\nThe quality phred score to keep\n\n\n# Copy the required files from the NanoClass2 software folder\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/jobscript.sh .\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/config.yaml .\n\nChanges made to the config file:\nsamples:                           \"filelists/mapping_file.csv\"\n\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\nBased on the quality checking I also made some changes. I used the lowest/highest seqkit Q1/Q3 data to set thresholds for the length.\n    minlen:                        1100\n    maxlen:                        1600\n    quality:                       10\n\n\n\nTo run NanoClass2 on the Crunchomics HPC, the jobscript.sh file can be used without any edits. This script is pre-configured to do everything for you such as:\n\nLoad the correct environments that have all required tools installed\nStart NanoClass2 using snakemake, a job scheduler that takes care of what software is run when. Snakemake is useful as it allows to run things in parallel as long as there are enough resources available. Snakemake also allows to re-start a job and it will simply pick things up from whereever things went wrong.\n\n\n# Do a dry-run to see if we set everything correctly \n# Since this is just a test, we can run this on the headnode \nconda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\n\nsnakemake \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml --use-conda \\\n    --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --cores 1 --nolock --rerun-incomplete -np\n\n# Submit job via a job script to the compute node: job id is 74581\nsbatch jobscript.sh\n\n# Generate a report out of this\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile\n\nconda deactivate \n\n# Run seqkit on the cleaned data\nseqkit stats -a -To results/seqkit/seqkit_stats_filtered.tsv results/data/mic2024/chopper/BC*gz --threads 10\n\n# Replace barcode with sample IDs in the otu table\nawk -v OFS=\"\\t\" 'NR==FNR {mapping[$1]=$2; next} {for (i=1; i&lt;=NF; i++) if ($i in mapping) $i=mapping[$i]; print}' \\\n  filelists/barcode_to_sample \\\n  &lt;(sed \"s/mic2024_minimap_//g\" results/tables/otu-table.tsv) \\\n  &gt; results/tables/otu-table-updated.txt\n\n# Make the header compatible for MicrobiomeAnalyst\nsed -i \"s/taxid/\\#NAME/g\" results/tables/otu-table-updated.txt\n\n# Remove Gerards sample\npython scripts/filter_columns.py -i results/tables/otu-table-updated.txt \\\n  -c GM11 \\\n  -o results/tables/otu-table-forMA.txt\n\nComments\n\nLooking at the seqkit stats we went from:\n\non average 19,695 to 17,513 reads\ntotal 413,614 to 367,773 reads\ntotal 561,031,890 to 520,739,003 bp\n\nLink to the report is here",
    "crumbs": [
      "Workflow for sequence classification"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#project-description",
    "href": "code/nanoclass_workflow.html#project-description",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "NanoClass2 is used to classify 16S rRNA amplicon sequences from 21 samples. DNA samples were taken from Winogradksy columns with wood or paper as substrate.\nFor each substrate 3 independent columns were sampled and for each columns there are 3-4 replicates. DNA was sampled on two days, by two different groups of students (i.e. Practical_groups 1 and 2):\n\n\n\n#NAME\nSample\nCarbon_source\nWino_Column\nPractical_group\n\n\n\n\nBC22\nGM11\npaper\nNo\n2\n\n\nBC17\nKB-P1\npaper\nP1\n1\n\n\nBC18\nWB-P1\npaper\nP1\n1\n\n\nBC2\nSB-P1\npaper\nP1\n2\n\n\nBC1\nLK-P2\npaper\nP2\n2\n\n\nBC16\nRvO-P2\npaper\nP2\n1\n\n\nBC20\nIV-P2\npaper\nP2\n1\n\n\nBC15\nEK-P3\npaper\nP3\n1\n\n\nBC19\nNK-P3\npaper\nP3\n1\n\n\nBC3\nPM-P3\npaper\nP3\n2\n\n\nBC10\nEP-SD1\nwood\nSD1\n1\n\n\nBC12\nRK-SD1\nwood\nSD1\n1\n\n\nBC23\nEW-SD1\nwood\nSD1\n2\n\n\nBC24\nDG-SD1\nwood\nSD1\n2\n\n\nBC5\nAW-SD2\nwood\nSD2\n2\n\n\nBC6\nUNZ-SD2\nwood\nSD2\n2\n\n\nBC7\nDH-SD2\nwood\nSD2\n2\n\n\nBC11\nAL-SD3\nwood\nSD3\n1\n\n\nBC13\nBS-SD3\nwood\nSD3\n1\n\n\nBC14\nDSR-SD3\nwood\nSD3\n1\n\n\nBC9\nBF-SD3\nwood\nSD3\n1",
    "crumbs": [
      "Workflow for sequence classification"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#folder-setup",
    "href": "code/nanoclass_workflow.html#folder-setup",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "Data was analysed on the Uva Crunchomics HPC:\n\n# Go to working directory\nwdir=\"/home/ndombro/personal/teaching/2024/miceco\"\ncd $wdir",
    "crumbs": [
      "Workflow for sequence classification"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#prepare-input-files",
    "href": "code/nanoclass_workflow.html#prepare-input-files",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "We work with the following data:\n\nSequencing data was provided as zipped folder by Peter Kuperus Oktober 22, 2024 via SurfDrive.\nThe file filelists/barcode_to_sample was generated manually from the mapping file provided by Gerard Muyzer and links the barcode to the sample ID.\n\nThe data was uploaded to Crunchomics and the barcode IDs were extracted from the file names to be able to loop through individual samples:\n\n# Make data folders\nmkdir data \nmkdir filelists \n\n# Upload sequencing data from local PC to Crunchomics\nscp data/mic2024.zip crunchomics:/home/ndombro/personal/teaching/2024/miceco/data\n\n# Unzip data folder\nunzip data/mic2024.zip\nmv mic2024 data\n\n# Make a list of barcodes\nls data/mic2024/*/fastq_pass/barcode*/*fastq.gz | \\\n  sed 's/.*barcode\\([0-9]\\+\\).*/barcode\\1/' \\\n  | sort -u &gt; filelists/barcodes.txt\n\n# We work with so many barcodes: 21\nwc -l filelists/barcodes.txt\n\n# Count number of files we work with per barcode\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/mic2024/*/fastq_pass/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\nWe work with 21 barcodes and for each barcode we work with several fastq.gz files:\nBarcode barcode01 has 145 fastq files\nBarcode barcode02 has 145 fastq files\nBarcode barcode03 has 145 fastq files\nBarcode barcode05 has 145 fastq files\nBarcode barcode06 has 144 fastq files\nBarcode barcode07 has 145 fastq files\nBarcode barcode09 has 132 fastq files\nBarcode barcode10 has 132 fastq files\nBarcode barcode11 has 132 fastq files\nBarcode barcode12 has 132 fastq files\nBarcode barcode13 has 132 fastq files\nBarcode barcode14 has 132 fastq files\nBarcode barcode15 has 132 fastq files\nBarcode barcode16 has 132 fastq files\nBarcode barcode17 has 132 fastq files\nBarcode barcode18 has 132 fastq files\nBarcode barcode19 has 3 fastq files\nBarcode barcode20 has 132 fastq files\nBarcode barcode22 has 89 fastq files\nBarcode barcode23 has 145 fastq files\nBarcode barcode24 has 144 fastq files\nWe can see that:\n\nfor each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\nBarcode19 seems to have less files, thus we need to keep this in mind and observe read counts more carefully for that sample\n\n\n\n\n\n# Generate folders\nmkdir data/combined_data\n\n# Combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/mic2024/*/fastq_pass/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n# Sanity check: We work with 21 combined files\nll data/combined_data/* | wc -l",
    "crumbs": [
      "Workflow for sequence classification"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#run-nanoclass",
    "href": "code/nanoclass_workflow.html#run-nanoclass",
    "title": "Classify Winogradksy samples",
    "section": "",
    "text": "Some steps, such as the initial quality filtering and some classifiers, require more computational resources. Therefore, it is beneficial to run NanoClass on a HPC with more memory and cpus and not on your own computer. The code should also run on a standard computer, however, the analyses might take quite some time.\nOn crunchomics we start NanoClass by simply submitting the job script we copied early. This script is pre-configured to do everything for you such as:\n\nLoad the correct environments that have all required tools installed\nStart NanoClass2 using snakemake, a job scheduler that takes care of what software is run when. Snakemake is useful as it allows to run things in parallel as long as there are enough resources available. Snakemake also allows to re-start a job and it will simply pick things up from whereever things went wrong.\nStart job: Tue Oct 24 16:07:12\nFinished job: Tue Oct 24 18:53:32\n\n\n# Do a dry-run to see if we set everything correctly \n# Since this is just a test, we can run this on the headnode \nconda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\n\nsnakemake \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml --use-conda \\\n    --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --cores 1 --nolock --rerun-incomplete -np\n\n\n# Submit job via a job script to the compute node: job id is 74581\nsbatch jobscript.sh\n\n# Generate a report out of this\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile\n\nconda deactivate \n\n# Run seqkit on the cleaned data\nseqkit stats -a -To results/seqkit/seqkit_stats_filtered.tsv results/data/mic2024/chopper/BC*gz --threads 10\n\n# cleanup (not needed)\n# rm -r results/benchmarks\n# rm -r results/data\n# rm -r results/logs\n# rm -r results/plots\n# rm -r results/stats\n\nComments\n\nLooking at the seqkit stats we went from:\n\non average 19,695 to 17,513 reads\ntotal 413,614 to 367,773 reads\ntotal 561,031,890 to 520,739,003 bp\n\nLink to the report is here",
    "crumbs": [
      "Classify Winogradksy samples"
    ]
  },
  {
    "objectID": "old/nanoclass_readme.html",
    "href": "old/nanoclass_readme.html",
    "title": "NanoClass",
    "section": "",
    "text": "NanoClass is a taxonomic meta-classifier for meta-barcoding sequencing data generated with the Oxford Nanopore Technology. With a single command, this Snakemake pipeline installs all programs and databases and runs and evaluates 10 popular taxonomic classification tools.\nThe tool was written by Evelyn Jongepier. For the github code go here and check out the documentation found here.\nThe notes found here are specific instructions to set up Nanoclass on the UvA Crunchomics HPC. In theory this should run on your own computers as well.\n\n\n\n\nNanoClass uses the workflow manager Snakemake. To run the Nanoclass we need to download Nanoclass and install Snakemake. Afterwards, Snakemake will install any other required dependencies by itself. Note, that Snakemake requires conda/mamba to manage dependenices and need to be installed as well if they are not already available on your computing system.\n\nwdir=\"/home/ndombro/personal/teaching/2023/miceco\"\ncd $wdir\n\n#download nanoclass\n#git clone https://github.com/ejongepier/NanoClass\n\n#install snakemake (if not already installed)\n#mamba create -c conda-forge -c bioconda -n snakemake_nanoclass python=3.9.7 snakemake=6.8.0 tabulate=0.8\n\n\n\n\n\n\nFirst, let us inspect what we have for each sample:\n\n#create a list of barcodes we work with (based on the excel sheet)\n#nano filelists/barcodes.txt\n\n#combine individual files\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/raw_data/*/*/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\n**Group 1**\nBarcode Abarcode01 has 4 fastq files\nBarcode Abarcode02 has 9 fastq files\nBarcode Abarcode03 has 10 fastq files\nBarcode Abarcode04 has 7 fastq files\nBarcode Abarcode05 has 12 fastq files\nBarcode Abarcode06 has 1 fastq files\nBarcode Abarcode07 has 10 fastq files\nBarcode Abarcode14 has 4 fastq files\nBarcode Abarcode15 has 3 fastq files\nBarcode Abarcode19 has 11 fastq files\nBarcode Abarcode20 has 11 fastq files\nBarcode Abarcode21 has 12 fastq files\nBarcode Abarcode23 has 11 fastq files\nBarcode Abarcode24 has 13 fastq files\n\n**Group 2**\nBarcode Bbarcode02 has 25 fastq files\nBarcode Bbarcode03 has 37 fastq files\nBarcode Bbarcode04 has 8 fastq files\nBarcode Bbarcode05 has 9 fastq files\nBarcode Bbarcode08 has 10 fastq files\nBarcode Bbarcode13 has 47 fastq files\nBarcode Bbarcode15 has 26 fastq files\nBarcode Bbarcode16 has 37 fastq files\nBarcode Bbarcode17 has 5 fastq files\nBarcode Bbarcode18 has 1 fastq files\nBarcode Bbarcode19 has 10 fastq files\nBarcode Bbarcode20 has 25 fastq files\nBarcode Bbarcode21 has 2 fastq files\nBarcode Bbarcode22 has 8 fastq files\nWe can see that for each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\n\n\n\n\n#generate folders\nmkdir -p data/raw_data \nmkdir data/combined_data\nmkdir NanoClass/cluster\nmkdir filelists\n\n#move data from local to crunchomics\nscp -r mic23_* crunchomics:/home/ndombro/personal/teaching/2023/miceco/data/raw_data\n\n#give unique ids for barcodes for different days (A=day1 , B=day2)\nfind data/raw_data/mic23_1*/fastq_pass/ -type d -exec sh -c 'mv \"$0\" \"${0%/*}/A${0##*/}\"' {} \\;\nfind data/raw_data/mic23_2*/fastq_pass/ -type d -exec sh -c 'mv \"$0\" \"${0%/*}/B${0##*/}\"' {} \\;\n\n#combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/raw_data/*/*/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n#control file content\nll data/combined_data/* \n\n#rm not existing/empty files\nrm data/combined_data/Abarcode08.fastq.gz\nrm data/combined_data/Abarcode22.fastq.gz\nrm data/combined_data/Bbarcode07.fastq.gz\n\n#count number of final files: 28 \nll data/combined_data/* \nll data/combined_data/* | wc -l\n\nNote:\nWe had missing files for:\ncat: data/raw_data/*/*/Abarcode08/*fastq.gz: No such file or directory (no seq data, deleted)\ncat: data/raw_data/*/*/Abarcode22/*fastq.gz: No such file or directory (no seq data, deleted)\ncat: data/raw_data/*/*/Bbarcode07/*fastq.gz: No such file or directory (not in list, deleted)\nThese files are missing because the sequencing yielded now results. Therefore, the IDs will be deleted from the mapping file and proceeded with the workflow.\n\n\n\nNext, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/raw_data/*/*/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 28\nWe have so many reads in total: 353,448\nOn average we have: 12,623 reads\n\nRead counts before and after merging individual files:\n**Group 1**\nTotal reads for Abarcode01 - Before: 3858, After: 3858\nTotal reads for Abarcode02 - Before: 8730, After: 8730\nTotal reads for Abarcode03 - Before: 9531, After: 9531\nTotal reads for Abarcode04 - Before: 6267, After: 6267\nTotal reads for Abarcode05 - Before: 11550, After: 11550\nTotal reads for Abarcode06 - Before: 2, After: 2\nTotal reads for Abarcode07 - Before: 9787, After: 9787\nTotal reads for Abarcode14 - Before: 3822, After: 3822\nTotal reads for Abarcode15 - Before: 2648, After: 2648\nTotal reads for Abarcode19 - Before: 10097, After: 10097\nTotal reads for Abarcode20 - Before: 10168, After: 10168\nTotal reads for Abarcode21 - Before: 11911, After: 11911\nTotal reads for Abarcode23 - Before: 10520, After: 10520\nTotal reads for Abarcode24 - Before: 12009, After: 12009\n\n**Group 2**\nTotal reads for Bbarcode02 - Before: 24355, After: 24355\nTotal reads for Bbarcode03 - Before: 36716, After: 36716\nTotal reads for Bbarcode04 - Before: 7327, After: 7327\nTotal reads for Bbarcode05 - Before: 8748, After: 8748\nTotal reads for Bbarcode08 - Before: 9199, After: 9199\nTotal reads for Bbarcode13 - Before: 46139, After: 46139\nTotal reads for Bbarcode15 - Before: 25380, After: 25380\nTotal reads for Bbarcode16 - Before: 36357, After: 36357\nTotal reads for Bbarcode17 - Before: 4911, After: 4911\nTotal reads for Bbarcode18 - Before: 26, After: 26\nTotal reads for Bbarcode19 - Before: 9899, After: 9899\nTotal reads for Bbarcode20 - Before: 24549, After: 24549\nTotal reads for Bbarcode21 - Before: 1358, After: 1358\nTotal reads for Bbarcode22 - Before: 7584, After: 7584\n\n\n\nTo run NanoClass we need:\n\nThe sequence files in fastq.gz format (one file per sample)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor our analysis the file looks something like this:\nrun,sample,barcode,path\nmiceco,EP1910,bc01,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode01.fastq.gz\nmiceco,RMT,bc02,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode02.fastq.gz\nmiceco,KJB3,bc03,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode03.fastq.gz\nSince I copy/pasted that information from excel, I ran a small line of code to convert it from a tab to a comma separated file:\n\nsed -i 's/\\t/,/g'  filelists/samples.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use (more details on that a bit later)\n\nFor this data I change the following line:\nsamples:                           \"/home/ndombro/personal/teaching/2023/miceco/filelists/samples.csv\"\nSince I am only running one out of the different classifiers I also change the contents of:\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\n\n\n\nThe jobscript.sh file allows to run jobs in an efficient manner on an HPC and make use of the high numbers of CPUs available on HPCs. I edited the file as follows:\n\nadd a # before configfile: “config.yaml” (this we need to separate the generated output files better from the workflow)\nchange cmd=\"srun --cores $SLURM_CPUS_ON_NODE snakemake --use-conda --cores $SLURM_CPUS_ON_NODE --nolock --rerun-incomplete\" to cmd=\"srun --cores $SLURM_CPUS_ON_NODE snakemake --use-conda -s Snakefile --configfile config.yaml --conda-prefix .snakemake/conda --cores 1 --nolock --rerun-incomplete\" (change was done for different file organization)\nIf you use mamba and do not have conda installed: change instances of miniconda3 to mambaforge (since I do not have conda installed)\nChange path to snakemake from conda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake-ampl to where your snakemake tool is installed, i.e. conda activate /home/ndombro/personal/mambaforge/envs/snakemake_nanoclass.\nadd LC_ALL info to bash script (this solves an issue with running R) if LC_ALL is not set in your HPC environment. To check if LC_ALL is set run echo $LC_ALL. If it returns something you do not need to add the following lines to your jobscript.\n\n# Set LC_ALL and export it\nexport LC_ALL=en_US.UTF-8\n\n\n\n\nEdited the envs/R4.0.yml to include the python version and rlang\n\nname: R4.0\nchannels:\n  - defaults\n  - bioconda\n  - conda-forge\ndependencies:\n  - python=3.9.7\n  - r-essentials\n  - r-base\n  - r-rlang=1.0.5\n  - r=4.0\n  - bioconductor-phyloseq\n  - bioconductor-dada2\n  - r-seqinr\n  - bioconductor-decipher\n  - r-vroom\n  - pip\n  - pip:\n    - biopython==1.78\nEdits for scripts/tomat.py (this ensures that the taxonomy strings in the OTU table are separated by a ;)\n\nin def get_otumat_dict(self) change taxid = '_'.join(tax) to taxid = ';'.join(tax)\nin def get_taxmat_dict(self) change taxid = '_'.join(tax) to taxid = ';'.join(tax)\n\n\n\n\n\nSome steps, such as the initial quality filtering and some classifiers, require more computational resources. Therefore, it is beneficial to run NanoClass on a HPC with more memory and cpus. The code should also run on a standard computer, however, the analyses might take quite some time.\nOn crunchomics:\n\nStart job: Tue Oct 24 16:07:12\nFinished job: Tue Oct 24 18:53:32\n\n\n#go into the folder with the pipeline\ncd NanoClass \n\n#test run to see if everything works\n#if returns no error run jobscript\nsnakemake --use-conda -s Snakefile --configfile config.yaml --conda-prefix .snakemake/conda --cores 1 --nolock --rerun-incomplete -np\n\n#submit job\nsbatch jobscript.sh\n\n#create report\nsnakemake --report\nconda deactivate\n\n#organize the files (i.e. separate workflow from output)\nmkdir ../results \nmv classifications ../results/\nmv benchmarks ../results/\nmv logs ../results/\nmv plots ../results/\nmv stats ../results/"
  },
  {
    "objectID": "old/nanoclass_readme.html#description",
    "href": "old/nanoclass_readme.html#description",
    "title": "NanoClass",
    "section": "",
    "text": "NanoClass is a taxonomic meta-classifier for meta-barcoding sequencing data generated with the Oxford Nanopore Technology. With a single command, this Snakemake pipeline installs all programs and databases and runs and evaluates 10 popular taxonomic classification tools.\nThe tool was written by Evelyn Jongepier. For the github code go here and check out the documentation found here.\nThe notes found here are specific instructions to set up Nanoclass on the UvA Crunchomics HPC. In theory this should run on your own computers as well."
  },
  {
    "objectID": "old/nanoclass_readme.html#installation",
    "href": "old/nanoclass_readme.html#installation",
    "title": "NanoClass",
    "section": "",
    "text": "NanoClass uses the workflow manager Snakemake. To run the Nanoclass we need to download Nanoclass and install Snakemake. Afterwards, Snakemake will install any other required dependencies by itself. Note, that Snakemake requires conda/mamba to manage dependenices and need to be installed as well if they are not already available on your computing system.\n\nwdir=\"/home/ndombro/personal/teaching/2023/miceco\"\ncd $wdir\n\n#download nanoclass\n#git clone https://github.com/ejongepier/NanoClass\n\n#install snakemake (if not already installed)\n#mamba create -c conda-forge -c bioconda -n snakemake_nanoclass python=3.9.7 snakemake=6.8.0 tabulate=0.8"
  },
  {
    "objectID": "old/nanoclass_readme.html#prepare-input-files",
    "href": "old/nanoclass_readme.html#prepare-input-files",
    "title": "NanoClass",
    "section": "",
    "text": "First, let us inspect what we have for each sample:\n\n#create a list of barcodes we work with (based on the excel sheet)\n#nano filelists/barcodes.txt\n\n#combine individual files\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/raw_data/*/*/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\n**Group 1**\nBarcode Abarcode01 has 4 fastq files\nBarcode Abarcode02 has 9 fastq files\nBarcode Abarcode03 has 10 fastq files\nBarcode Abarcode04 has 7 fastq files\nBarcode Abarcode05 has 12 fastq files\nBarcode Abarcode06 has 1 fastq files\nBarcode Abarcode07 has 10 fastq files\nBarcode Abarcode14 has 4 fastq files\nBarcode Abarcode15 has 3 fastq files\nBarcode Abarcode19 has 11 fastq files\nBarcode Abarcode20 has 11 fastq files\nBarcode Abarcode21 has 12 fastq files\nBarcode Abarcode23 has 11 fastq files\nBarcode Abarcode24 has 13 fastq files\n\n**Group 2**\nBarcode Bbarcode02 has 25 fastq files\nBarcode Bbarcode03 has 37 fastq files\nBarcode Bbarcode04 has 8 fastq files\nBarcode Bbarcode05 has 9 fastq files\nBarcode Bbarcode08 has 10 fastq files\nBarcode Bbarcode13 has 47 fastq files\nBarcode Bbarcode15 has 26 fastq files\nBarcode Bbarcode16 has 37 fastq files\nBarcode Bbarcode17 has 5 fastq files\nBarcode Bbarcode18 has 1 fastq files\nBarcode Bbarcode19 has 10 fastq files\nBarcode Bbarcode20 has 25 fastq files\nBarcode Bbarcode21 has 2 fastq files\nBarcode Bbarcode22 has 8 fastq files\nWe can see that for each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\n\n\n\n\n#generate folders\nmkdir -p data/raw_data \nmkdir data/combined_data\nmkdir NanoClass/cluster\nmkdir filelists\n\n#move data from local to crunchomics\nscp -r mic23_* crunchomics:/home/ndombro/personal/teaching/2023/miceco/data/raw_data\n\n#give unique ids for barcodes for different days (A=day1 , B=day2)\nfind data/raw_data/mic23_1*/fastq_pass/ -type d -exec sh -c 'mv \"$0\" \"${0%/*}/A${0##*/}\"' {} \\;\nfind data/raw_data/mic23_2*/fastq_pass/ -type d -exec sh -c 'mv \"$0\" \"${0%/*}/B${0##*/}\"' {} \\;\n\n#combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/raw_data/*/*/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n#control file content\nll data/combined_data/* \n\n#rm not existing/empty files\nrm data/combined_data/Abarcode08.fastq.gz\nrm data/combined_data/Abarcode22.fastq.gz\nrm data/combined_data/Bbarcode07.fastq.gz\n\n#count number of final files: 28 \nll data/combined_data/* \nll data/combined_data/* | wc -l\n\nNote:\nWe had missing files for:\ncat: data/raw_data/*/*/Abarcode08/*fastq.gz: No such file or directory (no seq data, deleted)\ncat: data/raw_data/*/*/Abarcode22/*fastq.gz: No such file or directory (no seq data, deleted)\ncat: data/raw_data/*/*/Bbarcode07/*fastq.gz: No such file or directory (not in list, deleted)\nThese files are missing because the sequencing yielded now results. Therefore, the IDs will be deleted from the mapping file and proceeded with the workflow.\n\n\n\nNext, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/raw_data/*/*/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 28\nWe have so many reads in total: 353,448\nOn average we have: 12,623 reads\n\nRead counts before and after merging individual files:\n**Group 1**\nTotal reads for Abarcode01 - Before: 3858, After: 3858\nTotal reads for Abarcode02 - Before: 8730, After: 8730\nTotal reads for Abarcode03 - Before: 9531, After: 9531\nTotal reads for Abarcode04 - Before: 6267, After: 6267\nTotal reads for Abarcode05 - Before: 11550, After: 11550\nTotal reads for Abarcode06 - Before: 2, After: 2\nTotal reads for Abarcode07 - Before: 9787, After: 9787\nTotal reads for Abarcode14 - Before: 3822, After: 3822\nTotal reads for Abarcode15 - Before: 2648, After: 2648\nTotal reads for Abarcode19 - Before: 10097, After: 10097\nTotal reads for Abarcode20 - Before: 10168, After: 10168\nTotal reads for Abarcode21 - Before: 11911, After: 11911\nTotal reads for Abarcode23 - Before: 10520, After: 10520\nTotal reads for Abarcode24 - Before: 12009, After: 12009\n\n**Group 2**\nTotal reads for Bbarcode02 - Before: 24355, After: 24355\nTotal reads for Bbarcode03 - Before: 36716, After: 36716\nTotal reads for Bbarcode04 - Before: 7327, After: 7327\nTotal reads for Bbarcode05 - Before: 8748, After: 8748\nTotal reads for Bbarcode08 - Before: 9199, After: 9199\nTotal reads for Bbarcode13 - Before: 46139, After: 46139\nTotal reads for Bbarcode15 - Before: 25380, After: 25380\nTotal reads for Bbarcode16 - Before: 36357, After: 36357\nTotal reads for Bbarcode17 - Before: 4911, After: 4911\nTotal reads for Bbarcode18 - Before: 26, After: 26\nTotal reads for Bbarcode19 - Before: 9899, After: 9899\nTotal reads for Bbarcode20 - Before: 24549, After: 24549\nTotal reads for Bbarcode21 - Before: 1358, After: 1358\nTotal reads for Bbarcode22 - Before: 7584, After: 7584\n\n\n\nTo run NanoClass we need:\n\nThe sequence files in fastq.gz format (one file per sample)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor our analysis the file looks something like this:\nrun,sample,barcode,path\nmiceco,EP1910,bc01,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode01.fastq.gz\nmiceco,RMT,bc02,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode02.fastq.gz\nmiceco,KJB3,bc03,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode03.fastq.gz\nSince I copy/pasted that information from excel, I ran a small line of code to convert it from a tab to a comma separated file:\n\nsed -i 's/\\t/,/g'  filelists/samples.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use (more details on that a bit later)\n\nFor this data I change the following line:\nsamples:                           \"/home/ndombro/personal/teaching/2023/miceco/filelists/samples.csv\"\nSince I am only running one out of the different classifiers I also change the contents of:\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\n\n\n\nThe jobscript.sh file allows to run jobs in an efficient manner on an HPC and make use of the high numbers of CPUs available on HPCs. I edited the file as follows:\n\nadd a # before configfile: “config.yaml” (this we need to separate the generated output files better from the workflow)\nchange cmd=\"srun --cores $SLURM_CPUS_ON_NODE snakemake --use-conda --cores $SLURM_CPUS_ON_NODE --nolock --rerun-incomplete\" to cmd=\"srun --cores $SLURM_CPUS_ON_NODE snakemake --use-conda -s Snakefile --configfile config.yaml --conda-prefix .snakemake/conda --cores 1 --nolock --rerun-incomplete\" (change was done for different file organization)\nIf you use mamba and do not have conda installed: change instances of miniconda3 to mambaforge (since I do not have conda installed)\nChange path to snakemake from conda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake-ampl to where your snakemake tool is installed, i.e. conda activate /home/ndombro/personal/mambaforge/envs/snakemake_nanoclass.\nadd LC_ALL info to bash script (this solves an issue with running R) if LC_ALL is not set in your HPC environment. To check if LC_ALL is set run echo $LC_ALL. If it returns something you do not need to add the following lines to your jobscript.\n\n# Set LC_ALL and export it\nexport LC_ALL=en_US.UTF-8\n\n\n\n\nEdited the envs/R4.0.yml to include the python version and rlang\n\nname: R4.0\nchannels:\n  - defaults\n  - bioconda\n  - conda-forge\ndependencies:\n  - python=3.9.7\n  - r-essentials\n  - r-base\n  - r-rlang=1.0.5\n  - r=4.0\n  - bioconductor-phyloseq\n  - bioconductor-dada2\n  - r-seqinr\n  - bioconductor-decipher\n  - r-vroom\n  - pip\n  - pip:\n    - biopython==1.78\nEdits for scripts/tomat.py (this ensures that the taxonomy strings in the OTU table are separated by a ;)\n\nin def get_otumat_dict(self) change taxid = '_'.join(tax) to taxid = ';'.join(tax)\nin def get_taxmat_dict(self) change taxid = '_'.join(tax) to taxid = ';'.join(tax)"
  },
  {
    "objectID": "old/nanoclass_readme.html#run-nanoclass",
    "href": "old/nanoclass_readme.html#run-nanoclass",
    "title": "NanoClass",
    "section": "",
    "text": "Some steps, such as the initial quality filtering and some classifiers, require more computational resources. Therefore, it is beneficial to run NanoClass on a HPC with more memory and cpus. The code should also run on a standard computer, however, the analyses might take quite some time.\nOn crunchomics:\n\nStart job: Tue Oct 24 16:07:12\nFinished job: Tue Oct 24 18:53:32\n\n\n#go into the folder with the pipeline\ncd NanoClass \n\n#test run to see if everything works\n#if returns no error run jobscript\nsnakemake --use-conda -s Snakefile --configfile config.yaml --conda-prefix .snakemake/conda --cores 1 --nolock --rerun-incomplete -np\n\n#submit job\nsbatch jobscript.sh\n\n#create report\nsnakemake --report\nconda deactivate\n\n#organize the files (i.e. separate workflow from output)\nmkdir ../results \nmv classifications ../results/\nmv benchmarks ../results/\nmv logs ../results/\nmv plots ../results/\nmv stats ../results/"
  },
  {
    "objectID": "code/nanoclass_workflow.html#dependencies",
    "href": "code/nanoclass_workflow.html#dependencies",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "python v3.10.12\nsed (GNU sed) 4.5\nGNU Awk 4.2.1, API: 2.0 (GNU MPFR 3.1.6-p2, GNU MP 6.1.2)\nseqkit v2.7.0\nNanoplot v1.42.0\nNanoClass2 v0.1",
    "crumbs": [
      "Workflow for sequence classification"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#run-nanoclass2",
    "href": "code/nanoclass_workflow.html#run-nanoclass2",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "NanoClass2 will be used for quality cleaning and to classify the sequence reads for each sample.\nTo run NanoClass2 we need:\n\nThe sequence files in fastq.gz format (one file per sample, which we already generated)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\nA config file in which we specify with what parameters we want to run NanoClass2\nA bash file that allows us to run NanoClass2 on an HPC\n\n\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor this analysis the file looks something like this:\nrun,sample,barcode,path\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode01.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode02.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode03.fastq.gz\nThis file can be generatd in excel, but we can also extract all the info we need from the file path:\n\necho \"run,sample,barcode,path\" &gt; filelists/mapping_file.csv\n\nls data/combined_data/*fastq.gz | \\\nwhile read path; do\n  run=\"mic2024\" # Set static run name\n  sample=$(echo \"$path\" | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\") # Extract sampleID\n  barcode=$(echo \"$sample\" | sed \"s/barcode/BC/g\")\n  fullpath=$(realpath \"$path\")  # To get the full absolute path\n  echo \"$run,$barcode,,$fullpath\" # Combine all data\ndone &gt;&gt; filelists/mapping_file.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use\nThe min, max read length to keep\nThe quality phred score to keep\n\n\n# Copy the required files from the NanoClass2 software folder\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/jobscript.sh .\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/config.yaml .\n\nChanges made to the config file:\nsamples:                           \"filelists/mapping_file.csv\"\n\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\nBased on the quality checking I also made some changes. I used the lowest/highest seqkit Q1/Q3 data to set thresholds for the length.\n    minlen:                        1100\n    maxlen:                        1600\n    quality:                       10\n\n\n\nTo run NanoClass2 on the Crunchomics HPC, the jobscript.sh file can be used without any edits. This script is pre-configured to do everything for you such as:\n\nLoad the correct environments that have all required tools installed\nStart NanoClass2 using snakemake, a job scheduler that takes care of what software is run when. Snakemake is useful as it allows to run things in parallel as long as there are enough resources available. Snakemake also allows to re-start a job and it will simply pick things up from whereever things went wrong.\n\n\n# Do a dry-run to see if we set everything correctly \n# Since this is just a test, we can run this on the headnode \nconda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\n\nsnakemake \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml --use-conda \\\n    --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --cores 1 --nolock --rerun-incomplete -np\n\n# Submit job via a job script to the compute node: job id is 74581\nsbatch jobscript.sh\n\n# Generate a report out of this\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile\n\nconda deactivate \n\n# Run seqkit on the cleaned data\nseqkit stats -a -To results/seqkit/seqkit_stats_filtered.tsv results/data/mic2024/chopper/BC*gz --threads 10\n\n# Replace barcode with sample IDs in the otu table\nawk -v OFS=\"\\t\" 'NR==FNR {mapping[$1]=$2; next} {for (i=1; i&lt;=NF; i++) if ($i in mapping) $i=mapping[$i]; print}' \\\n  filelists/barcode_to_sample \\\n  &lt;(sed \"s/mic2024_minimap_//g\" results/tables/otu-table.tsv) \\\n  &gt; results/tables/otu-table-updated.txt\n\n# Make the header compatible for MicrobiomeAnalyst\nsed -i \"s/taxid/\\#NAME/g\" results/tables/otu-table-updated.txt\n\n# Remove Gerards sample\npython scripts/filter_columns.py -i results/tables/otu-table-updated.txt \\\n  -c GM11 \\\n  -o results/tables/otu-table-forMA.txt\n\nComments\n\nLooking at the seqkit stats we went from:\n\non average 19,695 to 17,513 reads\ntotal 413,614 to 367,773 reads\ntotal 561,031,890 to 520,739,003 bp\n\nLink to the report is here",
    "crumbs": [
      "Workflow for sequence classification"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#do-quality-control",
    "href": "code/nanoclass_workflow.html#do-quality-control",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "Next, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/mic2024/*/fastq_pass/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 21\nWe have so many reads in total: 413,614\nOn average we have: 19,695 reads\nNotice that for barcode 19 and 22 we retain only very few read counts\n\nRead counts before and after merging individual files:\nTotal reads for barcode01 - Before: 60759, After: 60759\nTotal reads for barcode02 - Before: 78436, After: 78436\nTotal reads for barcode03 - Before: 28991, After: 28991\nTotal reads for barcode05 - Before: 4852, After: 4852\nTotal reads for barcode06 - Before: 17883, After: 17883\nTotal reads for barcode07 - Before: 18625, After: 18625\nTotal reads for barcode09 - Before: 12756, After: 12756\nTotal reads for barcode10 - Before: 15666, After: 15666\nTotal reads for barcode11 - Before: 15038, After: 15038\nTotal reads for barcode12 - Before: 7512, After: 7512\nTotal reads for barcode13 - Before: 11099, After: 11099\nTotal reads for barcode14 - Before: 6846, After: 6846\nTotal reads for barcode15 - Before: 14785, After: 14785\nTotal reads for barcode16 - Before: 18654, After: 18654\nTotal reads for barcode17 - Before: 16957, After: 16957\nTotal reads for barcode18 - Before: 30101, After: 30101\nTotal reads for barcode19 - Before: 3, After: 3\nTotal reads for barcode20 - Before: 16945, After: 16945\nTotal reads for barcode22 - Before: 109, After: 109\nTotal reads for barcode23 - Before: 31428, After: 31428\nTotal reads for barcode24 - Before: 6169, After: 6169\n\n\n\nNext, we calculate the quality statistics (total read count, average read count, read length, etc) to be able to screen the data for potential issues:\n\n# Make data folders\nmkdir -p results/seqkit\nmkdir -p results/nanoplot\n\n# Run seqkit\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/combined_data/*fastq.gz --threads 10\n\n# Generate plots to visualize the statistics better\nconda activate nanoplot_1.42.0\n\nfor file in data/combined_data/*fastq.gz; do\n  sample=$(echo $file | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\")\n  echo \"Starting analysis for \"$file\"\"\n  mkdir results/nanoplot/\"$sample\"\n  srun --cpus-per-task 10 --mem=50G \\\n    NanoPlot --fastq $file -o results/nanoplot/\"$sample\" --threads 10\ndone\n\nconda deactivate\n\nSummary\n\nReads are on average 1358 bp long with Q1: 1374, Q2: 1425 and Q3: 1454\nVisualizing the plots,\n\nthe majority of the data was hovering around 1400 bp\nThe read quality was more spread and and went from 10 to max 18",
    "crumbs": [
      "Workflow for sequence classification"
    ]
  },
  {
    "objectID": "scripts/nanoclass_workflow.html",
    "href": "scripts/nanoclass_workflow.html",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "NanoClass2 is used to classify 16S rRNA amplicon sequences from 21 samples. DNA samples were taken from Winogradksy columns with wood or paper as substrate.\nFor each substrate 3 independent columns were sampled and for each columns there are 3-4 replicates. DNA was sampled on two days, by two different groups of students (i.e. Practical_groups 1 and 2):\n\n\n\n#NAME\nSample\nCarbon_source\nWino_Column\nPractical_group\n\n\n\n\nBC22\nGM11\npaper\nNo\n2\n\n\nBC17\nKB-P1\npaper\nP1\n1\n\n\nBC18\nWB-P1\npaper\nP1\n1\n\n\nBC2\nSB-P1\npaper\nP1\n2\n\n\nBC1\nLK-P2\npaper\nP2\n2\n\n\nBC16\nRvO-P2\npaper\nP2\n1\n\n\nBC20\nIV-P2\npaper\nP2\n1\n\n\nBC15\nEK-P3\npaper\nP3\n1\n\n\nBC19\nNK-P3\npaper\nP3\n1\n\n\nBC3\nPM-P3\npaper\nP3\n2\n\n\nBC10\nEP-SD1\nwood\nSD1\n1\n\n\nBC12\nRK-SD1\nwood\nSD1\n1\n\n\nBC23\nEW-SD1\nwood\nSD1\n2\n\n\nBC24\nDG-SD1\nwood\nSD1\n2\n\n\nBC5\nAW-SD2\nwood\nSD2\n2\n\n\nBC6\nUNZ-SD2\nwood\nSD2\n2\n\n\nBC7\nDH-SD2\nwood\nSD2\n2\n\n\nBC11\nAL-SD3\nwood\nSD3\n1\n\n\nBC13\nBS-SD3\nwood\nSD3\n1\n\n\nBC14\nDSR-SD3\nwood\nSD3\n1\n\n\nBC9\nBF-SD3\nwood\nSD3\n1\n\n\n\n\n\n\n\npython v3.10.12\nsed (GNU sed) 4.5\nGNU Awk 4.2.1, API: 2.0 (GNU MPFR 3.1.6-p2, GNU MP 6.1.2)\nseqkit v2.7.0\nNanoplot v1.42.0\nNanoClass2 v0.1\n\n\n\n\nData was analysed on the Uva Crunchomics HPC:\n\n# Go to working directory\nwdir=\"/home/ndombro/personal/teaching/2024/miceco\"\ncd $wdir\n\n\n\n\n\n\nWe work with the following data:\n\nSequencing data was provided as zipped folder by Peter Kuperus Oktober 22, 2024 via SurfDrive.\nThe file filelists/barcode_to_sample was generated manually from the mapping file provided by Gerard Muyzer and links the barcode to the sample ID.\n\nThe data was uploaded to Crunchomics and the barcode IDs were extracted from the file names to be able to loop through individual samples:\n\n# Make data folders\nmkdir data \nmkdir filelists \n\n# Upload sequencing data from local PC to Crunchomics\nscp data/mic2024.zip crunchomics:/home/ndombro/personal/teaching/2024/miceco/data\n\n# Unzip data folder\nunzip data/mic2024.zip\nmv mic2024 data\n\n# Make a list of barcodes\nls data/mic2024/*/fastq_pass/barcode*/*fastq.gz | \\\n  sed 's/.*barcode\\([0-9]\\+\\).*/barcode\\1/' \\\n  | sort -u &gt; filelists/barcodes.txt\n\n# We work with so many barcodes: 21\nwc -l filelists/barcodes.txt\n\n# Count number of files we work with per barcode\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/mic2024/*/fastq_pass/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\nWe work with 21 barcodes and for each barcode we work with several fastq.gz files:\nBarcode barcode01 has 145 fastq files\nBarcode barcode02 has 145 fastq files\nBarcode barcode03 has 145 fastq files\nBarcode barcode05 has 145 fastq files\nBarcode barcode06 has 144 fastq files\nBarcode barcode07 has 145 fastq files\nBarcode barcode09 has 132 fastq files\nBarcode barcode10 has 132 fastq files\nBarcode barcode11 has 132 fastq files\nBarcode barcode12 has 132 fastq files\nBarcode barcode13 has 132 fastq files\nBarcode barcode14 has 132 fastq files\nBarcode barcode15 has 132 fastq files\nBarcode barcode16 has 132 fastq files\nBarcode barcode17 has 132 fastq files\nBarcode barcode18 has 132 fastq files\nBarcode barcode19 has 3 fastq files\nBarcode barcode20 has 132 fastq files\nBarcode barcode22 has 89 fastq files\nBarcode barcode23 has 145 fastq files\nBarcode barcode24 has 144 fastq files\nWe can see that:\n\nfor each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\nBarcode19 seems to have less files, thus we need to keep this in mind and observe read counts more carefully for that sample\n\n\n\n\n\n# Generate folders\nmkdir data/combined_data\n\n# Combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/mic2024/*/fastq_pass/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n# Sanity check: We work with 21 combined files\nll data/combined_data/* | wc -l\n\n\n\n\n\n\n\nNext, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/mic2024/*/fastq_pass/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 21\nWe have so many reads in total: 413,614\nOn average we have: 19,695 reads\nNotice that for barcode 19 and 22 we retain only very few read counts\n\nRead counts before and after merging individual files:\nTotal reads for barcode01 - Before: 60759, After: 60759\nTotal reads for barcode02 - Before: 78436, After: 78436\nTotal reads for barcode03 - Before: 28991, After: 28991\nTotal reads for barcode05 - Before: 4852, After: 4852\nTotal reads for barcode06 - Before: 17883, After: 17883\nTotal reads for barcode07 - Before: 18625, After: 18625\nTotal reads for barcode09 - Before: 12756, After: 12756\nTotal reads for barcode10 - Before: 15666, After: 15666\nTotal reads for barcode11 - Before: 15038, After: 15038\nTotal reads for barcode12 - Before: 7512, After: 7512\nTotal reads for barcode13 - Before: 11099, After: 11099\nTotal reads for barcode14 - Before: 6846, After: 6846\nTotal reads for barcode15 - Before: 14785, After: 14785\nTotal reads for barcode16 - Before: 18654, After: 18654\nTotal reads for barcode17 - Before: 16957, After: 16957\nTotal reads for barcode18 - Before: 30101, After: 30101\nTotal reads for barcode19 - Before: 3, After: 3\nTotal reads for barcode20 - Before: 16945, After: 16945\nTotal reads for barcode22 - Before: 109, After: 109\nTotal reads for barcode23 - Before: 31428, After: 31428\nTotal reads for barcode24 - Before: 6169, After: 6169\n\n\n\nNext, we calculate the quality statistics (total read count, average read count, read length, etc) to be able to screen the data for potential issues:\n\n# Make data folders\nmkdir -p results/seqkit\nmkdir -p results/nanoplot\n\n# Run seqkit\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/combined_data/*fastq.gz --threads 10\n\n# Generate plots to visualize the statistics better\nconda activate nanoplot_1.42.0\n\nfor file in data/combined_data/*fastq.gz; do\n  sample=$(echo $file | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\")\n  echo \"Starting analysis for \"$file\"\"\n  mkdir results/nanoplot/\"$sample\"\n  srun --cpus-per-task 10 --mem=50G \\\n    NanoPlot --fastq $file -o results/nanoplot/\"$sample\" --threads 10\ndone\n\nconda deactivate\n\nSummary\n\nReads are on average 1358 bp long with Q1: 1374, Q2: 1425 and Q3: 1454\nVisualizing the plots,\n\nthe majority of the data was hovering around 1400 bp\nThe read quality was more spread and and went from 10 to max 18\n\n\n\n\n\n\nNanoClass2 will be used for quality cleaning and to classify the sequence reads for each sample.\nTo run NanoClass2 we need:\n\nThe sequence files in fastq.gz format (one file per sample, which we already generated)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\nA config file in which we specify with what parameters we want to run NanoClass2\nA bash file that allows us to run NanoClass2 on an HPC\n\n\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor this analysis the file looks something like this:\nrun,sample,barcode,path\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode01.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode02.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode03.fastq.gz\nThis file can be generatd in excel, but we can also extract all the info we need from the file path:\n\necho \"run,sample,barcode,path\" &gt; filelists/mapping_file.csv\n\nls data/combined_data/*fastq.gz | \\\nwhile read path; do\n  run=\"mic2024\" # Set static run name\n  sample=$(echo \"$path\" | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\") # Extract sampleID\n  barcode=$(echo \"$sample\" | sed \"s/barcode/BC/g\")\n  fullpath=$(realpath \"$path\")  # To get the full absolute path\n  echo \"$run,$barcode,,$fullpath\" # Combine all data\ndone &gt;&gt; filelists/mapping_file.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use\nThe min, max read length to keep\nThe quality phred score to keep\n\n\n# Copy the required files from the NanoClass2 software folder\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/jobscript.sh .\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/config.yaml .\n\nChanges made to the config file:\nsamples:                           \"filelists/mapping_file.csv\"\n\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\nBased on the quality checking I also made some changes. I used the lowest/highest seqkit Q1/Q3 data to set thresholds for the length.\n    minlen:                        1100\n    maxlen:                        1600\n    quality:                       10\n\n\n\nTo run NanoClass2 on the Crunchomics HPC, the jobscript.sh file can be used without any edits. This script is pre-configured to do everything for you such as:\n\nLoad the correct environments that have all required tools installed\nStart NanoClass2 using snakemake, a job scheduler that takes care of what software is run when. Snakemake is useful as it allows to run things in parallel as long as there are enough resources available. Snakemake also allows to re-start a job and it will simply pick things up from whereever things went wrong.\n\n\n# Do a dry-run to see if we set everything correctly \n# Since this is just a test, we can run this on the headnode \nconda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\n\nsnakemake \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml --use-conda \\\n    --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --cores 1 --nolock --rerun-incomplete -np\n\n# Submit job via a job script to the compute node: job id is 74581\nsbatch jobscript.sh\n\n# Generate a report out of this\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile\n\nconda deactivate \n\n# Run seqkit on the cleaned data\nseqkit stats -a -To results/seqkit/seqkit_stats_filtered.tsv results/data/mic2024/chopper/BC*gz --threads 10\n\n# Replace barcode with sample IDs in the otu table\nawk -v OFS=\"\\t\" 'NR==FNR {mapping[$1]=$2; next} {for (i=1; i&lt;=NF; i++) if ($i in mapping) $i=mapping[$i]; print}' \\\n  filelists/barcode_to_sample \\\n  &lt;(sed \"s/mic2024_minimap_//g\" results/tables/otu-table.tsv) \\\n  &gt; results/tables/otu-table-updated.txt\n\n# Make the header compatible for MicrobiomeAnalyst\nsed -i \"s/taxid/\\#NAME/g\" results/tables/otu-table-updated.txt\n\n# Remove Gerards sample\npython scripts/filter_columns.py -i results/tables/otu-table-updated.txt \\\n  -c GM11 \\\n  -o results/tables/otu-table-forMA.txt\n\nComments\n\nLooking at the seqkit stats we went from:\n\non average 19,695 to 17,513 reads\ntotal 413,614 to 367,773 reads\ntotal 561,031,890 to 520,739,003 bp\n\nLink to the report is here",
    "crumbs": [
      "Data analysis",
      "Code documentation"
    ]
  },
  {
    "objectID": "scripts/nanoclass_workflow.html#project-description",
    "href": "scripts/nanoclass_workflow.html#project-description",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "NanoClass2 is used to classify 16S rRNA amplicon sequences from 21 samples. DNA samples were taken from Winogradksy columns with wood or paper as substrate.\nFor each substrate 3 independent columns were sampled and for each columns there are 3-4 replicates. DNA was sampled on two days, by two different groups of students (i.e. Practical_groups 1 and 2):\n\n\n\n#NAME\nSample\nCarbon_source\nWino_Column\nPractical_group\n\n\n\n\nBC22\nGM11\npaper\nNo\n2\n\n\nBC17\nKB-P1\npaper\nP1\n1\n\n\nBC18\nWB-P1\npaper\nP1\n1\n\n\nBC2\nSB-P1\npaper\nP1\n2\n\n\nBC1\nLK-P2\npaper\nP2\n2\n\n\nBC16\nRvO-P2\npaper\nP2\n1\n\n\nBC20\nIV-P2\npaper\nP2\n1\n\n\nBC15\nEK-P3\npaper\nP3\n1\n\n\nBC19\nNK-P3\npaper\nP3\n1\n\n\nBC3\nPM-P3\npaper\nP3\n2\n\n\nBC10\nEP-SD1\nwood\nSD1\n1\n\n\nBC12\nRK-SD1\nwood\nSD1\n1\n\n\nBC23\nEW-SD1\nwood\nSD1\n2\n\n\nBC24\nDG-SD1\nwood\nSD1\n2\n\n\nBC5\nAW-SD2\nwood\nSD2\n2\n\n\nBC6\nUNZ-SD2\nwood\nSD2\n2\n\n\nBC7\nDH-SD2\nwood\nSD2\n2\n\n\nBC11\nAL-SD3\nwood\nSD3\n1\n\n\nBC13\nBS-SD3\nwood\nSD3\n1\n\n\nBC14\nDSR-SD3\nwood\nSD3\n1\n\n\nBC9\nBF-SD3\nwood\nSD3\n1",
    "crumbs": [
      "Data analysis",
      "Code documentation"
    ]
  },
  {
    "objectID": "scripts/nanoclass_workflow.html#dependencies",
    "href": "scripts/nanoclass_workflow.html#dependencies",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "python v3.10.12\nsed (GNU sed) 4.5\nGNU Awk 4.2.1, API: 2.0 (GNU MPFR 3.1.6-p2, GNU MP 6.1.2)\nseqkit v2.7.0\nNanoplot v1.42.0\nNanoClass2 v0.1",
    "crumbs": [
      "Data analysis",
      "Code documentation"
    ]
  },
  {
    "objectID": "scripts/nanoclass_workflow.html#folder-setup",
    "href": "scripts/nanoclass_workflow.html#folder-setup",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "Data was analysed on the Uva Crunchomics HPC:\n\n# Go to working directory\nwdir=\"/home/ndombro/personal/teaching/2024/miceco\"\ncd $wdir",
    "crumbs": [
      "Data analysis",
      "Code documentation"
    ]
  },
  {
    "objectID": "scripts/nanoclass_workflow.html#prepare-input-files",
    "href": "scripts/nanoclass_workflow.html#prepare-input-files",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "We work with the following data:\n\nSequencing data was provided as zipped folder by Peter Kuperus Oktober 22, 2024 via SurfDrive.\nThe file filelists/barcode_to_sample was generated manually from the mapping file provided by Gerard Muyzer and links the barcode to the sample ID.\n\nThe data was uploaded to Crunchomics and the barcode IDs were extracted from the file names to be able to loop through individual samples:\n\n# Make data folders\nmkdir data \nmkdir filelists \n\n# Upload sequencing data from local PC to Crunchomics\nscp data/mic2024.zip crunchomics:/home/ndombro/personal/teaching/2024/miceco/data\n\n# Unzip data folder\nunzip data/mic2024.zip\nmv mic2024 data\n\n# Make a list of barcodes\nls data/mic2024/*/fastq_pass/barcode*/*fastq.gz | \\\n  sed 's/.*barcode\\([0-9]\\+\\).*/barcode\\1/' \\\n  | sort -u &gt; filelists/barcodes.txt\n\n# We work with so many barcodes: 21\nwc -l filelists/barcodes.txt\n\n# Count number of files we work with per barcode\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/mic2024/*/fastq_pass/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\nWe work with 21 barcodes and for each barcode we work with several fastq.gz files:\nBarcode barcode01 has 145 fastq files\nBarcode barcode02 has 145 fastq files\nBarcode barcode03 has 145 fastq files\nBarcode barcode05 has 145 fastq files\nBarcode barcode06 has 144 fastq files\nBarcode barcode07 has 145 fastq files\nBarcode barcode09 has 132 fastq files\nBarcode barcode10 has 132 fastq files\nBarcode barcode11 has 132 fastq files\nBarcode barcode12 has 132 fastq files\nBarcode barcode13 has 132 fastq files\nBarcode barcode14 has 132 fastq files\nBarcode barcode15 has 132 fastq files\nBarcode barcode16 has 132 fastq files\nBarcode barcode17 has 132 fastq files\nBarcode barcode18 has 132 fastq files\nBarcode barcode19 has 3 fastq files\nBarcode barcode20 has 132 fastq files\nBarcode barcode22 has 89 fastq files\nBarcode barcode23 has 145 fastq files\nBarcode barcode24 has 144 fastq files\nWe can see that:\n\nfor each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\nBarcode19 seems to have less files, thus we need to keep this in mind and observe read counts more carefully for that sample\n\n\n\n\n\n# Generate folders\nmkdir data/combined_data\n\n# Combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/mic2024/*/fastq_pass/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n# Sanity check: We work with 21 combined files\nll data/combined_data/* | wc -l",
    "crumbs": [
      "Data analysis",
      "Code documentation"
    ]
  },
  {
    "objectID": "scripts/nanoclass_workflow.html#do-quality-control",
    "href": "scripts/nanoclass_workflow.html#do-quality-control",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "Next, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/mic2024/*/fastq_pass/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 21\nWe have so many reads in total: 413,614\nOn average we have: 19,695 reads\nNotice that for barcode 19 and 22 we retain only very few read counts\n\nRead counts before and after merging individual files:\nTotal reads for barcode01 - Before: 60759, After: 60759\nTotal reads for barcode02 - Before: 78436, After: 78436\nTotal reads for barcode03 - Before: 28991, After: 28991\nTotal reads for barcode05 - Before: 4852, After: 4852\nTotal reads for barcode06 - Before: 17883, After: 17883\nTotal reads for barcode07 - Before: 18625, After: 18625\nTotal reads for barcode09 - Before: 12756, After: 12756\nTotal reads for barcode10 - Before: 15666, After: 15666\nTotal reads for barcode11 - Before: 15038, After: 15038\nTotal reads for barcode12 - Before: 7512, After: 7512\nTotal reads for barcode13 - Before: 11099, After: 11099\nTotal reads for barcode14 - Before: 6846, After: 6846\nTotal reads for barcode15 - Before: 14785, After: 14785\nTotal reads for barcode16 - Before: 18654, After: 18654\nTotal reads for barcode17 - Before: 16957, After: 16957\nTotal reads for barcode18 - Before: 30101, After: 30101\nTotal reads for barcode19 - Before: 3, After: 3\nTotal reads for barcode20 - Before: 16945, After: 16945\nTotal reads for barcode22 - Before: 109, After: 109\nTotal reads for barcode23 - Before: 31428, After: 31428\nTotal reads for barcode24 - Before: 6169, After: 6169\n\n\n\nNext, we calculate the quality statistics (total read count, average read count, read length, etc) to be able to screen the data for potential issues:\n\n# Make data folders\nmkdir -p results/seqkit\nmkdir -p results/nanoplot\n\n# Run seqkit\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/combined_data/*fastq.gz --threads 10\n\n# Generate plots to visualize the statistics better\nconda activate nanoplot_1.42.0\n\nfor file in data/combined_data/*fastq.gz; do\n  sample=$(echo $file | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\")\n  echo \"Starting analysis for \"$file\"\"\n  mkdir results/nanoplot/\"$sample\"\n  srun --cpus-per-task 10 --mem=50G \\\n    NanoPlot --fastq $file -o results/nanoplot/\"$sample\" --threads 10\ndone\n\nconda deactivate\n\nSummary\n\nReads are on average 1358 bp long with Q1: 1374, Q2: 1425 and Q3: 1454\nVisualizing the plots,\n\nthe majority of the data was hovering around 1400 bp\nThe read quality was more spread and and went from 10 to max 18",
    "crumbs": [
      "Data analysis",
      "Code documentation"
    ]
  },
  {
    "objectID": "scripts/nanoclass_workflow.html#run-nanoclass2",
    "href": "scripts/nanoclass_workflow.html#run-nanoclass2",
    "title": "Workflow for sequence classification",
    "section": "",
    "text": "NanoClass2 will be used for quality cleaning and to classify the sequence reads for each sample.\nTo run NanoClass2 we need:\n\nThe sequence files in fastq.gz format (one file per sample, which we already generated)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\nA config file in which we specify with what parameters we want to run NanoClass2\nA bash file that allows us to run NanoClass2 on an HPC\n\n\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor this analysis the file looks something like this:\nrun,sample,barcode,path\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode01.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode02.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode03.fastq.gz\nThis file can be generatd in excel, but we can also extract all the info we need from the file path:\n\necho \"run,sample,barcode,path\" &gt; filelists/mapping_file.csv\n\nls data/combined_data/*fastq.gz | \\\nwhile read path; do\n  run=\"mic2024\" # Set static run name\n  sample=$(echo \"$path\" | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\") # Extract sampleID\n  barcode=$(echo \"$sample\" | sed \"s/barcode/BC/g\")\n  fullpath=$(realpath \"$path\")  # To get the full absolute path\n  echo \"$run,$barcode,,$fullpath\" # Combine all data\ndone &gt;&gt; filelists/mapping_file.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use\nThe min, max read length to keep\nThe quality phred score to keep\n\n\n# Copy the required files from the NanoClass2 software folder\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/jobscript.sh .\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/config.yaml .\n\nChanges made to the config file:\nsamples:                           \"filelists/mapping_file.csv\"\n\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\nBased on the quality checking I also made some changes. I used the lowest/highest seqkit Q1/Q3 data to set thresholds for the length.\n    minlen:                        1100\n    maxlen:                        1600\n    quality:                       10\n\n\n\nTo run NanoClass2 on the Crunchomics HPC, the jobscript.sh file can be used without any edits. This script is pre-configured to do everything for you such as:\n\nLoad the correct environments that have all required tools installed\nStart NanoClass2 using snakemake, a job scheduler that takes care of what software is run when. Snakemake is useful as it allows to run things in parallel as long as there are enough resources available. Snakemake also allows to re-start a job and it will simply pick things up from whereever things went wrong.\n\n\n# Do a dry-run to see if we set everything correctly \n# Since this is just a test, we can run this on the headnode \nconda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\n\nsnakemake \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml --use-conda \\\n    --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --cores 1 --nolock --rerun-incomplete -np\n\n# Submit job via a job script to the compute node: job id is 74581\nsbatch jobscript.sh\n\n# Generate a report out of this\nsnakemake --report report.html \\\n  --configfile config.yaml \\\n  -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile\n\nconda deactivate \n\n# Run seqkit on the cleaned data\nseqkit stats -a -To results/seqkit/seqkit_stats_filtered.tsv results/data/mic2024/chopper/BC*gz --threads 10\n\n# Replace barcode with sample IDs in the otu table\nawk -v OFS=\"\\t\" 'NR==FNR {mapping[$1]=$2; next} {for (i=1; i&lt;=NF; i++) if ($i in mapping) $i=mapping[$i]; print}' \\\n  filelists/barcode_to_sample \\\n  &lt;(sed \"s/mic2024_minimap_//g\" results/tables/otu-table.tsv) \\\n  &gt; results/tables/otu-table-updated.txt\n\n# Make the header compatible for MicrobiomeAnalyst\nsed -i \"s/taxid/\\#NAME/g\" results/tables/otu-table-updated.txt\n\n# Remove Gerards sample\npython scripts/filter_columns.py -i results/tables/otu-table-updated.txt \\\n  -c GM11 \\\n  -o results/tables/otu-table-forMA.txt\n\nComments\n\nLooking at the seqkit stats we went from:\n\non average 19,695 to 17,513 reads\ntotal 413,614 to 367,773 reads\ntotal 561,031,890 to 520,739,003 bp\n\nLink to the report is here",
    "crumbs": [
      "Data analysis",
      "Code documentation"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html",
    "href": "scripts/OTU_table_analysis.html",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "The goal of this tutorial is to analyse 16S rRNA gene data. Our input file is an OTU/ASV table that already contains some taxa information and will go through the following steps:\n\nReading the data into R\nFiltering the data\nNormalizing the data\nVisualizing the data (i.e. alpha/beta diversity, barplots, …)\n\nIn the example we look at an OTU table of 28 samples. These 28 samples represent three distinct microbiomes from three Winogradsky column experiments in which columns were created using wood, paper and a wood/paper mix as substrate. DNA was collected on two separate dates, so another category we can compare is the sampling date.\n\n\nWe start with setting a path to working directory and setting a seed seed for normalization protocol.\nSetting a set is not essential but this way we make sure that we get the same results when normalizing our OTU table. If we randomly select some observations for any task in R or in any statistical software it results in different values all the time and this happens because of randomization. If we want to keep the values that are produced at first random selection then we can do this by storing them in an object after randomization or we can fix the randomization procedure so that we get the same results all the time.\n\n\n\nSome packages required for this workflow are installed with BiocManager or devtools, if you need to install any of these tools, remove the # from the code and run it.\n\n#if (!require(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n\n#BiocManager::install(\"phyloseq\")\n#BiocManager::install(\"microbiome\")\n#BiocManager::install(\"ALDEx2\")\n#BiocManager::install(\"DESeq2\")\n\n\n\n\n\nlibrary(tidyverse) #general parsing\nlibrary(data.table) #general parsing\nlibrary(phyloseq) #phyloseq object loading\nlibrary(vegan) #rarefaction\nlibrary(microbiome) #normalization\nlibrary(ALDEx2) #stats\nlibrary(DESeq2) #stats\nlibrary(grid) #organizing multiple plots\nlibrary(gridExtra) #organizing multiple plots\nlibrary(scales) #plot aesthetic, comma setting\nlibrary(ANCOMBC) #stats\nlibrary(ggvenn) # vendiagram\nlibrary(gplots) #vendiagram\n\n\n\n\nNext, we read in some custom things such as a theme that we will use for plotting our graphs and a color vectors.\nDefining a custom_theme for our plots is useful because it means that instead of re-writing the commands for our plot over and over again in each plot, we can just use the custom_theme function instead.\n\n#define custom theme for generating figures\ncustom_theme &lt;- function() {\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(), \n    panel.border =element_blank(),\n    axis.line.x = element_line(color=\"black\", size = 0.5),\n    axis.line.y = element_line(color=\"black\", size = 0.5),\n    strip.text.x = element_text(size = 7),\n    strip.text.y = element_text(size = 7),\n    strip.background = element_rect(fil=\"#FFFFFF\", color = \"black\", linewidth = 0.5),\n    axis.text.x = element_text(size = 7),\n    legend.text = element_text(size = 8), legend.title = element_text(size = 10)\n  )\n}\n\n#generate color scheme \nc25 &lt;- c(\"dodgerblue2\", \"#E31A1C\", \"green4\", \"#6A3D9A\", \"#FF7F00\", \"black\", \"gold1\", \"skyblue2\", \"#FB9A99\", \n        \"palegreen2\", \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\", \"deeppink1\", \"blue1\", \n        \"steelblue4\", \"darkturquoise\", \"green1\", \"yellow4\", \"yellow3\",\"darkorange4\", \"brown\")\n\n\n\n\n\n\nAn OTU table contains a column with the OTUs (taxonomic ranks in our case) and one column per sample with the counts how often OTU is found in the sample. It might look something like this:\n\n\n\n\n\n\n\n\n\n\n#NAME\nEP1910\nRMT\nKJB3\nTJR\n\n\n\n\nBacteria;Acidobacteria;Acidobacteriia;Acidobacteriales;Acidobacteriaceae (Subgroup 1);NA\n0\n0\n0\n0\n\n\nBacteria;Acidobacteria;Acidobacteriia;Solibacterales;Solibacteraceae (Subgroup 3);PAUC26f\n0\n5\n3\n1\n\n\n\n\n# Provide the path to the otu table\nfile_paths &lt;- c(\"../data/for_ma/otu-table-forMA.txt\")\n\n# Read in otu table\nmerged_otu_table &lt;- read.table(file_paths, header = T, sep = '\\t', comment = \"\")\ncolnames(merged_otu_table)[1] &lt;- \"taxid\"\n\n# Replace NA with 0\nmerged_otu_table[is.na(merged_otu_table)] &lt;- 0\n\n# R does not like - and will replace things with dots, clean up the sample names\ncolnames(merged_otu_table) &lt;- gsub(\"\\\\.\", \"_\", colnames(merged_otu_table)) \n\n#use the taxon as rownames\nrownames(merged_otu_table) &lt;- merged_otu_table$taxid\nmerged_otu_table$taxid &lt;- NULL\n\n#check how many otus and samples we have\ndim(merged_otu_table)\n\n[1] 1248   20\n\n\nWith this example OTU table, we work with dim(merged_otu_table)[2] samples and 1248 OTUs.\n\n\n\nThe metadata table contains information about our samples and can look something like this:\n\n\n\n#NAME\ntreatment\nDate\n\n\n\n\nEP1910\nwood\n2023_1\n\n\nRMT\npaper\n2023_1\n\n\nKJB3\nmix\n2023_1\n\n\nTJR\npaper\n2023_1\n\n\nIB5\nwood\n2023_1\n\n\nALIDNA\nwood\n2023_1\n\n\nIG7\npaper\n2023_1\n\n\nB314\nmix\n2023_1\n\n\n\n\n# Read in metadata file\nmetadata_combined &lt;- read.table(\"../data/for_ma/sample_table.txt\", header = TRUE, row.names = 1, sep = \"\\t\", comment.char = \"\")\n\n# Replace dashes with underscores in row names\nrownames(metadata_combined) &lt;- gsub(\"-\", \"_\", rownames(metadata_combined))\n\n# Ensure that the group data is categorical \nmetadata_combined$Practical_group &lt;- gsub(\"^\", \"Day\", metadata_combined$Practical_group)\n\n# Add extra column for sample names\nmetadata_combined$name &lt;- paste0(metadata_combined$Carbon_source, \"_\", rownames(metadata_combined))\nmetadata_combined$sample_id &lt;- rownames(metadata_combined)\n\n# Order the factors for our names column\nmetadata_combined &lt;- metadata_combined |&gt; \n  arrange(desc(Carbon_source))\n\n# View output\nhead(metadata_combined)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarcode\nCarbon_source\nWino_Column\nPractical_group\nname\nsample_id\n\n\n\n\nEP_SD1\nBC10\nwood\nSD1\nDay1\nwood_EP_SD1\nEP_SD1\n\n\nRK_SD1\nBC12\nwood\nSD1\nDay1\nwood_RK_SD1\nRK_SD1\n\n\nEW_SD1\nBC23\nwood\nSD1\nDay2\nwood_EW_SD1\nEW_SD1\n\n\nDG_SD1\nBC24\nwood\nSD1\nDay2\nwood_DG_SD1\nDG_SD1\n\n\nAW_SD2\nBC05\nwood\nSD2\nDay2\nwood_AW_SD2\nAW_SD2\n\n\nUNZ_SD2\nBC06\nwood\nSD2\nDay2\nwood_UNZ_SD2\nUNZ_SD2\n\n\n\n\n\n\n\n\n\n\nNext, we generate a table that list the taxonomy information for each taxonomic rank. We do this by taking the information from our OTU table. Depending on how you analysed your 16S rRNA gene sequences, you might have an OTU table with IDs (ASV1, ASV2, … or OTU1, OTU2, …) and a separate table with the taxonomy information.\nIf that is the case, you can read in the taxonomy information separate.\n\n#extract taxonomy string\ntemp &lt;- as.data.frame(rownames(merged_otu_table))\ncolnames(temp) &lt;- \"OTU\"\n\n#separate the taxonomic headers                      \ntaxonomy_file &lt;- temp |&gt; \n  distinct(OTU) |&gt; \n  separate(OTU,\n           c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\"), \n           sep = \";\", remove = FALSE) |&gt; \n  column_to_rownames(var = \"OTU\")\n\n#view file\nhead(taxonomy_file)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKingdom\nPhylum\nClass\nOrder\nFamily\nGenus\n\n\n\n\nBacteria;10bav-F6;NA;NA;NA;NA\nBacteria\n10bav-F6\nNA\nNA\nNA\nNA\n\n\nBacteria;Abditibacteriota;Abditibacteria;Abditibacteriales;Abditibacteriaceae;Abditibacterium\nBacteria\nAbditibacteriota\nAbditibacteria\nAbditibacteriales\nAbditibacteriaceae\nAbditibacterium\n\n\nBacteria;Acetothermia;Acetothermiia;NA;NA;NA\nBacteria\nAcetothermia\nAcetothermiia\nNA\nNA\nNA\n\n\nBacteria;Acidobacteriota;Acidobacteriae;AKIW659;NA;NA\nBacteria\nAcidobacteriota\nAcidobacteriae\nAKIW659\nNA\nNA\n\n\nBacteria;Acidobacteriota;Acidobacteriae;Bryobacterales;Bryobacteraceae;Bryobacter\nBacteria\nAcidobacteriota\nAcidobacteriae\nBryobacterales\nBryobacteraceae\nBryobacter\n\n\nBacteria;Acidobacteriota;Acidobacteriae;PAUC26f;NA;NA\nBacteria\nAcidobacteriota\nAcidobacteriae\nPAUC26f\nNA\nNA\n\n\n\n\n\n\n\n\n\nA phyloseq object combines different elements of an analysis (i.e. the OTU table, the list of taxa and the mapping file) into one single object. We can easily generate such an object with the three dataframes we have generated above:\n\n#combine data\nOTU = otu_table(merged_otu_table, taxa_are_rows = TRUE)\nTAX = tax_table(as.matrix(taxonomy_file))\nphyseq = phyloseq(OTU, TAX, sample_data(metadata_combined))\n\n#view structure\nphyseq\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1248 taxa and 20 samples ]\nsample_data() Sample Data:       [ 20 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1248 taxa by 6 taxonomic ranks ]\n\n\n\n\n\n\n\nBelow, we write a custom function to calculate some summary statistics. We easily could do this without a function, however, since we want to compare the statistics before and after filtering the OTU table, the function is useful to have, since we do not need to copy-paste the exact same code in two spots of the workflow:\n\nprint_summary &lt;- function(reads_per_OTU) {\n  total_reads &lt;- sum(reads_per_OTU)\n  otu_number &lt;- length(reads_per_OTU)\n  num_singletons &lt;- length(reads_per_OTU[reads_per_OTU == 1])\n  num_doubletons &lt;- length(reads_per_OTU[reads_per_OTU == 2])\n  num_less_than_10 &lt;- length(reads_per_OTU[reads_per_OTU &lt; 10])\n  total_reads_less_than_10 &lt;- sum(reads_per_OTU[reads_per_OTU &lt; 10])\n  perc_reads_less_than_10 &lt;- (total_reads_less_than_10 / sum(reads_per_OTU)) * 100\n  \n  cat(\"Total number of reads:\", format(total_reads, big.mark = \",\"), \"\\n\")\n  cat(\"Number of OTUs\",  format(otu_number, big.mark = \",\"), \"\\n\")\n  cat(\"Number of singleton OTUs:\",  format(num_singletons, big.mark = \",\"), \"\\n\")\n  cat(\"Number of doubleton OTUs:\",  format(num_doubletons, big.mark = \",\"), \"\\n\")\n  cat(\"Number of OTUs with less than 10 seqs:\",  format(num_less_than_10, big.mark = \",\"), \"\\n\")\n  cat(\"Total reads for OTUs with less than 10 seqs:\",  format(total_reads_less_than_10, big.mark = \",\"), \"\\n\")\n  cat(\"Percentage of reads for OTUs with less than 10 seqs:\",  sprintf(\"%.2f%%\", perc_reads_less_than_10), \"\\n\")\n  \n}\n\n#calculate the number of reads found per otu\nreads_per_OTU &lt;- taxa_sums(physeq)\n\n#summarize the data\nprint_summary(reads_per_OTU)\n\nTotal number of reads: 366,981 \nNumber of OTUs 1,248 \nNumber of singleton OTUs: 245 \nNumber of doubleton OTUs: 95 \nNumber of OTUs with less than 10 seqs: 609 \nTotal reads for OTUs with less than 10 seqs: 1,728 \nPercentage of reads for OTUs with less than 10 seqs: 0.47% \n\n\nFor this workflow, we define *singletons** as reads/OTUs with a sequence that is present exactly once in the dataset.\nNotice that another definition of singletons can be as taxa/OTU present in a single sample.\nIn amplicon data analysis it is useful to remove reads with low counts because they are very likely due to sequencing errors. We generally assume that sequencing errors are independent and randomly distributed, and we can assume that erroneous sequences will occur much less often than the true sequence. We will remove such sequences during the data filtering step.\n\n\n\nNext, let’s explore how many reads we have per sample:\n\n#count the number of reads per sample\nsample_counts &lt;- as.data.frame(colSums(merged_otu_table))\n                  \n#clean up the dataframe\nnames(sample_counts)[1] &lt;- \"counts\"\nsample_counts$sampleID &lt;- rownames(sample_counts)\n\n#plot counts\np_counts &lt;-\n  ggplot(data = sample_counts, aes(x = reorder(sampleID, counts, FUN=sum, decreasing = TRUE), y = counts)) +\n  geom_point() +\n  geom_text(aes(x = , sampleID, y = counts, label = counts),  hjust = 0, nudge_y = 200 , size = 2.5) +\n  coord_flip() +\n  xlab(\"\") + \n  ylab(\"Read counts\") +\n  custom_theme()\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\np_counts\n\n\n\n\n\n\n\n\nIn this example, we see two samples with almost no reads and we want to make sure to remove these samples. We also see that we have a large difference between different samples. To be able to compare for example sample IV (~25,000 reads) with sample MN (~1,000 reads) we need to normalize our data after the data filtering step.\n\n\n\n\nNext, we filter the data. Specifically, we:\n\nRemove OTUs that are not assigned to anything at Phylum rank. The subset_taxa function can be used to remove any taxa you want, i.e. if you have plant DNA in your sample, you could use this to remove chloroplast sequences as well.\n\nRemove samples with total read counts less than 20. This cutoff is arbitrary and depends a bit on your data. To choose a good value, explore the read counts you have per sample and define a cutoff based on that. In this example, we mainly want to remove the two low read samples we have seen in our plot.\n\nRemove low count OTUs: The threshold is up to you; removing singletons or doubletons is common, but you can be more conservative and remove any counts less than 10. Look at the plots before and get a feeling for how many OTUs were removed.\n\nIn our example, we want to remove samples with 20 or less reads, remove singletons only and remove OTUs that occur in less than 10% of our samples (v1). Since there are many different thoughts about OTU table filtering, you can also find two other options (v2 and v3) on how to filter a table.\nFor most analyses we will work with the filtered data but there are some diversity metrics which rely on the presence of singletons within a sample (richness estimates, i.e. Chao), so you might choose to leave them in for those sorts of analyses only.\n\n#define cutoffs\ncounts_per_sample &lt;- 20\notu_nr_cutoff &lt;- 1\nmin_percentage_samples &lt;- 10\n\n#remove taxa without tax assignment at Phylum rank\nphyseq_filt &lt;- subset_taxa(physeq, Phylum != \"NA\")\n\n#remove samples with less than 20 reads\nphyseq_filt &lt;- prune_samples(sample_sums(physeq)&gt;= counts_per_sample, physeq)\n\n#calculate the minimum number of samples an otu should be present in\nmin_samples &lt;- ceiling((min_percentage_samples / 100) * nsamples(physeq_filt))\n\n#remove otus that occur only rarely (v1)\n#here, we calculate the total abundance of each otu across all samples and checks if it's greater than the specified otu_nr_cutoff AND we check if the otu occurs in at least &lt;min_percentage_samples&gt;% of samples\n#we only retain OTUs that satisfy this condition \nphyseq_filt &lt;- prune_taxa(taxa_sums(physeq_filt) &gt; otu_nr_cutoff & taxa_sums(physeq_filt) &gt;= min_samples, physeq_filt)\nphyseq_filt\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 989 taxa and 19 samples ]\nsample_data() Sample Data:       [ 19 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 989 taxa by 6 taxonomic ranks ]\n\n#remove otus that occur only rarely (v2)\n#here, we count the number of samples where the abundance of an otu is &gt; 0. \n#If this count is greater than the specified otu_nr_cutoff, the taxon is retained.\n#physeq_filt &lt;- filter_taxa(physeq, function (x) {sum(x &gt; 0) &gt; otu_nr_cutoff}, prune=TRUE)\n#physeq_filt\n\n#remove otus that occur only rarely (v3)\n#here, we remove taxa not seen more than 1 times in at least 10% of the samples\n#physeq_filt = filter_taxa(physeq, function(x) sum(x &gt; 1) &gt; (0.1*length(x)), TRUE)\n#physeq_filt\n\nNext, we can calculate the summary statistics with the custom taxa_sums function we have defined before:\n\n#calculate the number of reads found per otu\nreads_per_OTU_filt &lt;- taxa_sums(physeq_filt)\n\n#summarize the data\nprint_summary(reads_per_OTU_filt)\n\nTotal number of reads: 366,733 \nNumber of OTUs 989 \nNumber of singleton OTUs: 0 \nNumber of doubleton OTUs: 95 \nNumber of OTUs with less than 10 seqs: 350 \nTotal reads for OTUs with less than 10 seqs: 1,483 \nPercentage of reads for OTUs with less than 10 seqs: 0.40% \n\n\n\n\n\nBelow, we generate three new phyloseq objects using three different normalization approaches: 1. Compositional: transforms the data into relative abundance 2. CLR: stands for centerd log-ratio transform and allows us to compare proportions of OTUs within each sample. After this transformation the values will no longer be counts, but rather the dominance (or lack thereof) for each taxa relative to the geometric mean of all taxa on the logarithmic scale 3. Rarefaction: scales all of your reads down to the lowest total sequencing depth. Notice, that this might drop many OTUs in higher read samples and might lead to under-estimation of low-abundant OTUs\nGenerating different phyloseq objects for different normalization approaches allows us to easily compare analysis steps with different inputs.\n\nphyseq_rel_abundance &lt;- microbiome::transform(physeq_filt, \"compositional\")\nphyseq_clr &lt;- microbiome::transform(physeq_filt, \"clr\")\nphyseq_rarified &lt;- rarefy_even_depth(physeq_filt)\n\n\n\n\n\n\nRarefactions illustrate how well your sample was sampled. The rarefaction function takes a random subsample of each column in your OTU table of a given size, starting with a very small subsample, and counts how many unique OTUs were observed in that subsample. The analysis is repeated with increasing subsample sizes until the maximum actual read depth for your table is reached.\nIn an ideal dataset, each curve should reach a plateau, i.e. horizontal asymptote, ideally around the same depth. While this almost never is the case, exploring these graphs gives us an idea how well each sample was sampled.\n\n#create a df from our otu table\ndf &lt;- as.data.frame(otu_table(physeq_filt))\n\n#rarefy data with a step size of 50, using tidy = TRUE will return a dataframe instead of a plot\ndf_rate &lt;- rarecurve(t(df), step=50, cex=0.5, label = FALSE, tidy = TRUE)\n\n#add metadata\ndf_rate &lt;- merge(df_rate, metadata_combined, by.x = \"Site\", by.y = \"sample_id\")\n\n#plot\ndf_rate %&gt;%\n  group_by(Site) %&gt;%\n  mutate(max_sample = max(Sample)) %&gt;%\n  mutate(label = if_else(Sample == max_sample, as.character(Site), NA_character_)) %&gt;%\n  ggplot(aes(x = Sample, y = Species, color = Carbon_source, group = interaction(Site, Carbon_source))) + \n  geom_line() + \n  facet_grid(cols = vars(Carbon_source)) +\n  geom_text(aes(label = label),\n            position = position_nudge(x = 2000),\n            na.rm = TRUE, size = 3) +\n  custom_theme()\n\n\n\n\n\n\n\n\nIn this example we can see that almost none of the sample reach a plateau, RH is the closest but not there yet. This suggests that we mainly sampled the abundant members of our community and might miss many rare taxa. This is something to keep in mind for other analyses, such as alpha diversity analyses.\n\n\n\nAlpha diversity measures the diversity within our sample and we distinguish between species richness (i.e. the number of species) and species richness (i.e. how relatively abundant each of the species are).\n\n#calculate different alpha diversity indicators\nrichness_meta &lt;-microbiome::alpha(physeq_filt, index = \"all\")\n\n#add the sample id to table\nrichness_meta$sample_id &lt;- rownames(richness_meta)\n\n#add other metadata data\nrichness_meta  &lt;- merge(richness_meta, metadata_combined, by = \"sample_id\")\n\n#check what parameters are calculated\ncolnames(richness_meta)\n\n [1] \"sample_id\"                  \"observed\"                  \n [3] \"chao1\"                      \"diversity_inverse_simpson\" \n [5] \"diversity_gini_simpson\"     \"diversity_shannon\"         \n [7] \"diversity_fisher\"           \"diversity_coverage\"        \n [9] \"evenness_camargo\"           \"evenness_pielou\"           \n[11] \"evenness_simpson\"           \"evenness_evar\"             \n[13] \"evenness_bulla\"             \"dominance_dbp\"             \n[15] \"dominance_dmn\"              \"dominance_absolute\"        \n[17] \"dominance_relative\"         \"dominance_simpson\"         \n[19] \"dominance_core_abundance\"   \"dominance_gini\"            \n[21] \"rarity_log_modulo_skewness\" \"rarity_low_abundance\"      \n[23] \"rarity_rare_abundance\"      \"Barcode\"                   \n[25] \"Carbon_source\"              \"Wino_Column\"               \n[27] \"Practical_group\"            \"name\"                      \n\n\nNext, we can generate a plot. Since we calculated the diversity estimates after removing singletons, we will look at evenness indices, such as the Shannon index.\n\n#generate figure\nalpha_plot &lt;-\n  ggplot(richness_meta, aes(x = Carbon_source, y = chao1)) +\n    geom_boxplot() +\n    geom_jitter(aes(color = Practical_group), width = 0.2, size = 4) +\n    labs(x = \"\", y = \"Chao1 index\") +\n    custom_theme() +\n    theme(axis.title.y = element_text(size = 20),\n          axis.text.x = element_text(size = 20),\n          axis.text.y = element_text(size = 14),\n          legend.key=element_blank(), \n          legend.text = element_text(size=14),\n          legend.title = element_text(size=20)\n          ) \n\nalpha_plot\n\n\n\n\n\n\n\n#save the figure to our computer\n#ggsave(paste0(\"results/plots/alpha-div.png\"), plot=alpha_plot, device=\"png\")\n\nWe see that there is not a difference in terms of species evenness when comparing our different samples.\n\n\n\nIn contrast to alpha diversity, beta diversity quantifies the dissimilarity between communities (multiple samples).\nCommonly used metrics include: - the Bray-Curtis index (for compositional/abundance data) - Jaccard index (for presence/absence data, ignoring abundance information) - Aitchison distance (Euclidean distance for clr transformed abundances, aiming to avoid the compositionality bias) - Unifrac distance (that takes into account the phylogenetic tree information) - …\nAvoid methods that use Euclidean distance as microbiome data are sparse datasets and better suited for the above mentioned methods.\nBased on the type of algorithm, ordination methods can be divided in two categories: - unsupervised, which measure variation in the data without additional information on covariates or other supervision of the model, including: - Principal Coordinate Analysis (PCoA) - Principal Component Analysis (PCA) - Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) - supervised ordination: - distance-based Redundancy Analysis (dbRDA)\n\n\nFor Bray-Curtis our data should not have negative entries, so we will use the relative abundance (or rarefied) data:\n\n#convert otu table to df\ndata_otu_filt_rar &lt;- t(data.frame(otu_table(physeq_rarified)))\n\n#calculate bray-curtis \ndist_bc &lt;- as.matrix(vegdist(data_otu_filt_rar, method = \"bray\")) \n\n# calculate PCOA using Phyloseq package\npcoa_bc &lt;- ordinate(physeq_rarified, \"PCoA\", \"bray\") \n\nplot_ordination(physeq_rarified, pcoa_bc, color = \"Carbon_source\", shape = \"Practical_group\") + \n  geom_point(size = 3) +\n  custom_theme()\n\n\n\n\n\n\n\n\nFirst, this two-dimensions PCOA plot show 34% of the total variance between the samples. Next, we see that the our samples are not forming distinct clusters, i.e. microbiomes from the mixed, paper and wood communities appear very similar.\nNext, we can do a statistical test:\n\n#extract metdatatas\nmetadata2 &lt;- data.frame(meta(physeq_rarified))\n  \n# Permanova test using the vegan package\nadonis2(data_otu_filt_rar~Carbon_source, data = metadata2, permutations=9999, method=\"bray\")\n\n\n\n\n\n\nDf\nSumOfSqs\nR2\nF\nPr(&gt;F)\n\n\n\n\nCarbon_source\n1\n0.6906827\n0.2075976\n4.453746\n0.0025\n\n\nResidual\n17\n2.6363440\n0.7924024\nNA\nNA\n\n\nTotal\n18\n3.3270267\n1.0000000\nNA\nNA\n\n\n\n\n\n\nThis confirms that there do not seem to be any statistically significant differences between our samples.\n\n\n\n\nNext, we generate a beta-diversity ordination using the Aitchison distance. We do this by applying PCA to the centered log-ratio (CLR) transformed counts.\n\n#PCA via phyloseq\n#RDA without constraints is a PCA\nord_clr &lt;- phyloseq::ordinate(physeq_clr, \"RDA\", distance = \"euclidian\")\n\n#Plot scree plot to plot eigenvalues, i.e.the total amount of variance that can be explained by a given principal componen\nphyloseq::plot_scree(ord_clr) + \n  geom_bar(stat=\"identity\", fill = \"blue\") +\n  custom_theme() +\n  labs(x = \"\\nAxis\", y = \"Proportion of Variance\\n\")\n\n\n\n\n\n\n\n\n\n#Scale axes\nclr1 &lt;- ord_clr$CA$eig[1] / sum(ord_clr$CA$eig)\nclr2 &lt;- ord_clr$CA$eig[2] / sum(ord_clr$CA$eig)\n\n#and plot\nphyloseq::plot_ordination(physeq_clr, ord_clr, color = \"Carbon_source\", shape = \"Practical_group\") + \n  geom_point(size = 2,) +\n  coord_fixed(clr2 / clr1) +\n  stat_ellipse(aes(group = Carbon_source), linetype = 2) +\n  custom_theme() \n\n\n\n\n\n\n\n\nWhile PCA is an exploratory data visualization tool, we can test whether the samples cluster beyond that expected by sampling variability using permutational multivariate analysis of variance (PERMANOVA). It does this by partitioning the sums of squares for the within- and between-cluster components using the concept of centroids. Many permutations of the data (i.e. random shuffling) are used to generate the null distribution. The test from ADONIS can be confounded by differences in dispersion (or spread)…so we want to check this as well.\n\n#Generate distance matrix\nclr_dist_matrix &lt;- phyloseq::distance(physeq_clr, method = \"euclidean\") \n\n#ADONIS test\nadonis2(clr_dist_matrix~Carbon_source, data = metadata2, permutations=9999, method=\"bray\")\n\n\n\n\n\n\nDf\nSumOfSqs\nR2\nF\nPr(&gt;F)\n\n\n\n\nCarbon_source\n1\n6379.877\n0.183901\n3.830806\n4e-04\n\n\nResidual\n17\n28312.034\n0.816099\nNA\nNA\n\n\nTotal\n18\n34691.911\n1.000000\nNA\nNA\n\n\n\n\n\n\n\n\n\nNext, we want to plot the taxa distribution. Let us first look at the most abundant phyla and check how similar our different samples are:\n\n#condense data at specific tax rank, i.e. on phylum level\ngrouped_taxa &lt;- tax_glom(physeq_rel_abundance, \"Phylum\")\n  \n#find top19 most abundant taxa \ntop_taxa &lt;- names(sort(taxa_sums(grouped_taxa), TRUE)[1:19])\n\n#make a list for the remaining taxa\nother_taxa &lt;- names(taxa_sums(grouped_taxa))[which(!names(taxa_sums(grouped_taxa)) %in% top_taxa)]\n\n#group the low abundant taxa into one group\nmerged_physeq = merge_taxa(grouped_taxa, other_taxa, 2)\n  \n#transform phyloseq object into dataframe\ndf &lt;- psmelt(merged_physeq)\n\n#cleanup the names in the df\nnames(df)[names(df) == \"Phylum\"] &lt;- \"tax_level\"\n\n#replace NAs, with other (NAs are generated for the other category)\ndf$tax_level[which(is.na(df$tax_level))] &lt;- \"Other\"\n  \n#create a df to sort taxa labels by abundance\nsorted_labels &lt;- df |&gt; \n  group_by(tax_level) |&gt; \n  summarise(sum = sum(Abundance)) |&gt; \n  arrange(desc(sum))\n  \n#Get list of sorted levels excluding \"Other\"\ndesired_levels &lt;- setdiff(sorted_labels$tax_level, \"Other\")\n  \n#sort df using the sorted levels and ensure that \"Other\" is the last category\ndf$tax_level2 &lt;- factor(df$tax_level, levels = c(desired_levels, \"Other\"))\n  \n#generate color scheme\ncols &lt;- c25[1:length(unique(df$tax_level2))]\ncols[levels(df$tax_level2) == \"Other\"] &lt;- \"#CCCCCC\"\n  \n#plot\nfig &lt;-\n  ggplot(df, aes(x = Sample, y = Abundance, fill = tax_level2) ) +\n    geom_bar(position = \"stack\", stat = \"identity\", width = 0.9) +\n    labs(y = \"Relative abundance\", x = \"\", title = paste0(\"Relative abundance at \", \"Phylum\", \" rank\")) +\n    scale_fill_manual(name = paste0(\"Phylum\",\"_rank\"), labels = levels(df$tax_level2), values = cols) +\n    facet_grid(cols =  vars(Carbon_source), scales = \"free\", space = \"free\") +\n    scale_y_continuous(expand = c(0, 0), limits = c(0, 1.01)) +\n    custom_theme() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1 ) ) +\n    guides(fill=guide_legend(title = \"Phylum\"))\n\nfig\n\n\n\n\n\n\n\n\nSince we want to generate one plot for each taxonomic rank, i.e. Phylum, Class, Order,…, we can do this in a loop. The figures will be generated in the folder results/plots/.\nIf you do not feel comfortable with a lob, you can also do this step by step by removing the for statement and replacing all instances of level with the taxonomic rank you want to investigate\n\n#if not there already, create output folder\nimg_path=\"../results/r_analyses/images/\"\ndir.create(img_path, recursive = TRUE)\n\n#generate one barplot for each taxonomic level\nfor (level in colnames(taxonomy_file)){\n  \n  #condense data at specific tax rank, i.e. on phylum level\n  grouped_taxa &lt;- tax_glom(physeq_rel_abundance, level)\n  \n  #find top19 most abundant taxa \n  top_taxa &lt;- names(sort(taxa_sums(grouped_taxa), TRUE)[1:19])\n  \n  #make a list for the remaining taxa\n  other_taxa &lt;- names(taxa_sums(grouped_taxa))[which(!names(taxa_sums(grouped_taxa)) %in% top_taxa)]\n  \n  #group the low abundant taxa into one group\n  merged_physeq = merge_taxa(grouped_taxa, other_taxa, 2)\n  \n  #transform phyloseq object into dataframe\n  df &lt;- psmelt(merged_physeq) \n  \n  #cleanup the dataframe\n  names(df)[names(df) == level] &lt;- \"tax_level\"\n  df$tax_level[which(is.na(df$tax_level))] &lt;- \"Other\"\n  \n  #create a df to sort taxa labels by abundance\n  sorted_labels &lt;- df |&gt; \n    group_by(tax_level) |&gt; \n    summarise(sum = sum(Abundance)) |&gt; \n    arrange(desc(sum))\n  \n  #Get list of sorted levels excluding \"Other\"\n  desired_levels &lt;- setdiff(sorted_labels$tax_level, \"Other\")\n  \n  #sort df using the sorted levels and ensure that \"Other\" is the last category\n  df$tax_level2 &lt;- factor(df$tax_level, levels = c(desired_levels, \"Other\"))\n  \n  #generate color scheme\n  cols &lt;- c25[1:length(unique(df$tax_level2))]\n  cols[levels(df$tax_level2) == \"Other\"] &lt;- \"#CCCCCC\"\n  \n  #plot\n  fig &lt;-\n    ggplot(df, aes(x = Sample, y = Abundance, fill = tax_level2) ) +\n      geom_bar(position = \"stack\", stat = \"identity\", width = 0.9) +\n      labs(y = \"Relative abundance\", x = \"\", title = paste0(\"Relative abundance at \", level, \" rank\")) +\n      scale_fill_manual(name = paste0(level,\"_rank\"), labels = levels(df$tax_level2), values = cols) +\n      facet_grid(cols =  vars(Carbon_source), scales = \"free\", space = \"free\") +\n      scale_y_continuous(expand = c(0, 0), limits = c(0, 1.01)) +\n      custom_theme() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1 ) ) +\n      guides(fill=guide_legend(title=level))\n  \n  ggsave(paste0(img_path, level, \"_barplot_ra.png\"), plot = fig, device=\"png\")\n  }\n\nGenerated plots:\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\nNotice: This is still in development and needs to be optimized, use with care!\n\n\nFirst, let us compare for differences for a category where we compare two factors. For example, we might want to ask whether there are any significant differences when comparing our wood versus paper samples.\nLet’s first tes test this using the non-parametric Wilcoxon rank-sum test.\n\n#Generate data.frame with OTUs and metadata\nps_wilcox &lt;- data.frame(t(data.frame(phyloseq::otu_table(physeq_clr))), check.names = FALSE)\nps_wilcox$Carbon_source &lt;- phyloseq::sample_data(physeq_clr)$Carbon_source\n\n#Define functions to pass to map\nwilcox_model &lt;- function(df){\n  wilcox.test(abund ~ Carbon_source, data = df)\n}\n\nwilcox_pval &lt;- function(df){\n  wilcox.test(abund ~ Carbon_source, data = df)$p.value\n}\n\n#Create nested data frames by OTU and loop over each using map \nwilcox_results &lt;- ps_wilcox %&gt;%\n  gather(key = OTU, value = abund, -Carbon_source) %&gt;%\n  group_by(OTU) %&gt;%\n  nest() %&gt;%\n  mutate(wilcox_test = map(data, wilcox_model),\n         p_value = map(data, wilcox_pval))  \n\n#Show results\nwilcox_results$wilcox_test[[1]]\n\n\n    Wilcoxon rank sum exact test\n\ndata:  abund by Carbon_source\nW = 44, p-value = 1\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe can also test for differences per OTU:\n\n#Unnesting\nwilcox_results &lt;- wilcox_results %&gt;%\n  dplyr::select(OTU, p_value) %&gt;%\n  unnest(cols = c(p_value))\n\nhead(wilcox_results)\n\n\n\n\n\n\n\n\n\nOTU\np_value\n\n\n\n\nBacteria;10bav-F6;NA;NA;NA;NA\n1.0000000\n\n\nBacteria;Abditibacteriota;Abditibacteria;Abditibacteriales;Abditibacteriaceae;Abditibacterium\n0.9038925\n\n\nBacteria;Acetothermia;Acetothermiia;NA;NA;NA\n0.0120664\n\n\nBacteria;Acidobacteriota;Acidobacteriae;Bryobacterales;Bryobacteraceae;Bryobacter\n0.2375433\n\n\nBacteria;Acidobacteriota;Acidobacteriae;PAUC26f;NA;NA\n0.0754148\n\n\nBacteria;Acidobacteriota;Acidobacteriae;TSBb06;NA;NA\n0.0068006\n\n\n\n\n\n#Computing FDR corrected p-values (since we do multiple statistical comparisons)\nwilcox_results &lt;- wilcox_results %&gt;%\n  arrange(p_value) %&gt;%\n  mutate(BH_FDR = p.adjust(p_value, \"BH\")) %&gt;%\n  filter(BH_FDR &lt; 0.001) %&gt;%\n  dplyr::select(OTU, p_value, BH_FDR, everything())\n\nhead(wilcox_results)  \n\n\n\n\n\n\n\n\n\n\nOTU\np_value\nBH_FDR\n\n\n\n\nBacteria;Bacteroidota;Bacteroidia;Bacteroidales;Ika33;NA\n2.65e-05\n2.65e-05\n\n\nBacteria;Firmicutes;Clostridia;Eubacteriales;Alkalibacteraceae;Alkalibacter\n2.65e-05\n2.65e-05\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;Acetivibrio\n2.65e-05\n2.65e-05\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;Ruminiclostridium\n2.65e-05\n2.65e-05\n\n\nBacteria;Firmicutes;Clostridia;Peptostreptococcales-Tissierellales;NA;JTB215\n2.65e-05\n2.65e-05\n\n\nBacteria;Verrucomicrobiota;Lentisphaeria;Oligosphaerales;Lenti-02;NA\n2.65e-05\n2.65e-05\n\n\n\n\n\n\nWilcoxon has some down-sites when it comes to treating zero values, an alternative approach is to use the ANOVA-like differential expression (ALDEx2) method. The aldex function is a wrapper that performs log-ratio transformation and statistical testing in a single line of code, which is why we feed the non-normalized data into it:\n\naldex2_da &lt;- ALDEx2::aldex(data.frame(phyloseq::otu_table(physeq_filt)), phyloseq::sample_data(physeq_filt)$Carbon_source, test=\"t\", effect = TRUE, denom=\"iqlr\")\n\naldex.clr: generating Monte-Carlo instances and clr values\n\n\noperating in serial mode\n\n\ncomputing iqlr centering\n\n\naldex.ttest: doing t-test\n\n\naldex.effect: calculating effect sizes\n\n\nSpecifically, this function:\n\ngenerates Monte Carlo samples of the Dirichlet distribution for each sample,\nconverts each instance using a log-ratio transform,\n\nreturns test results for two sample (Welch’s t, Wilcoxon) or multi-sample (glm, Kruskal-Wallace) tests.\n\niqlr” accounts for data with systematic variation and centers the features on the set features that have variance that is between the lower and upper quartile of variance. This provides results that are more robust to asymmetric features between groups.\nNext, we can plot the effect size. The effect size plot shows the median log2 fold difference by the median log2 dispersion. This is a measure of the effect size by the variability. Differentially abundant taxon will be those where the difference most exceeds the dispersion. Points toward the top of the figure are more abundant in CF samples while those towards the bottom are more abundant in healthy controls. Taxa with BH-FDR corrected p-values are shown in red.\n\n#plot effect sizes\nALDEx2::aldex.plot(aldex2_da, type=\"MW\", test=\"wilcox\", called.cex = 1, cutoff.pval = 0.001)\n\n\n\n\n\n\n\n\nFinally, we can print the output with the taxa information added.\n\n#Clean up presentation\nsig_aldex2 &lt;- aldex2_da %&gt;%\n  rownames_to_column(var = \"OTU\") %&gt;%\n  filter(wi.eBH &lt; 0.05) %&gt;%\n  arrange(effect, wi.eBH) %&gt;%\n  dplyr::select(OTU, diff.btw, diff.win, effect, wi.ep, wi.eBH)\n\nhead(sig_aldex2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOTU\ndiff.btw\ndiff.win\neffect\nwi.ep\nwi.eBH\n\n\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;Ruminiclostridium\n-7.603371\n2.1386233\n-3.540289\n0.0000265\n0.0049666\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;Acetivibrio\n-9.397042\n3.2100297\n-2.922520\n0.0000265\n0.0049666\n\n\nBacteria;Firmicutes;Clostridia;Peptostreptococcales-Tissierellales;NA;JTB215\n-2.130290\n0.9848773\n-2.024687\n0.0000376\n0.0055734\n\n\nBacteria;SAR324 clade(Marine group B);NA;NA;NA;NA\n-2.565471\n1.1989183\n-2.016483\n0.0001385\n0.0102657\n\n\nBacteria;Verrucomicrobiota;Lentisphaeria;Oligosphaerales;Lenti-02;NA\n-2.327235\n1.2450759\n-1.902249\n0.0000920\n0.0072225\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;uncultured\n-2.139205\n1.1714844\n-1.793109\n0.0000761\n0.0071607\n\n\n\n\n\n\nIn our case an empty dataframe is returned, no significant differences were detected.\n\n\n\nDESeq2 performs a differential expression analysis based on the Negative Binomial (a.k.a. Gamma-Poisson) distribution. DeSeq normalizes the data throughout its analysis, so we input only the filtered data. For more theory, visit this page.\n\n#convert treatment column to a factor\nsample_data(physeq_filt)$Carbon_source &lt;- as.factor(sample_data(physeq_filt)$Carbon_source)\n\n#Convert the phyloseq object to a DESeqDataSet and run DESeq2:\nds &lt;- phyloseq_to_deseq2(physeq_filt, ~ Carbon_source)\n\nconverting counts to integer mode\n\n#since our samples contain a lot of 0s (something DeSeq is NOT designed for) we use some alternative means to estimate the size factor\nds &lt;-estimateSizeFactors(ds, type = 'poscounts')\nds &lt;- DESeq(ds)\n\nusing pre-existing size factors\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n\n-- replacing outliers and refitting for 56 genes\n-- DESeq argument 'minReplicatesForReplace' = 7 \n-- original counts are preserved in counts(dds)\n\n\nestimating dispersions\n\n\nfitting model and testing\n\n#extract the results and filter by a FDR cutoff of 0.01 and\n#find significantly differentially abundant OTU between the seasons “paper” and “wood”\nalpha = 0.001\nres_desq = results(ds, contrast=c(\"Carbon_source\", \"paper\", \"wood\"), alpha=alpha)\nres_desq = res_desq[order(res_desq$padj, na.last=NA), ]\nres_sig_deseq = as.data.frame(res_desq[(res_desq$padj &lt; alpha), ])\n\n#print number of significant results\ndim(res_sig_deseq)\n\n[1] 39  6\n\n\nPlot significant OTUs (counts):\n\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_filt)[rownames(res_sig_deseq), ], \"matrix\")),as.data.frame(as(otu_table(physeq_filt)[rownames(res_sig_deseq), ], \"matrix\")))\n\ndf_long &lt;- reshape2::melt(df)\n\nUsing Kingdom, Phylum, Class, Order, Family, Genus as id variables\n\ndf_long &lt;- merge(df_long, metadata2, by.x = \"variable\", by.y = \"sample_id\")\n\nggplot(df_long, aes(x=Genus, y=value, color = Carbon_source)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n  scale_color_manual(values = c( \"orange\", \"purple\")) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_rel_abundance)[rownames(res_sig_deseq), ], \"matrix\")),as.data.frame(as(otu_table(physeq_rel_abundance)[rownames(res_sig_deseq), ], \"matrix\")))\n\ndf_long &lt;- reshape2::melt(df)\n\nUsing Kingdom, Phylum, Class, Order, Family, Genus as id variables\n\ndf_long &lt;- merge(df_long, metadata2, by.x = \"variable\", by.y = \"sample_id\")\n\nggplot(df_long, aes(x=Genus, y=value, color = Carbon_source)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n  scale_color_manual(values = c(\"orange\", \"purple\")) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\n\n# Run ancom-bc2\n# Settings:\n# prv_cut: a numerical fraction between 0 and 1. Taxa with prevalences less than prv_cut will be excluded in the analysis\n# a numerical threshold for filtering samples based on library sizes. Samples with library sizes less than lib_cut\nancom_out = ancombc2(data = physeq_filt,\n               fix_formula = \"Carbon_source\",\n               rand_formula = NULL,\n               p_adj_method = \"holm\", \n               pseudo_sens = FALSE,  #FALSE to speed things up for testing, but makes things less sensitive\n               prv_cut = 0.10, lib_cut = 1000, s0_perc = 0.05,\n               group = \"Carbon_source\",\n               alpha = 0.05, n_cl = 2, verbose = TRUE,\n               global = TRUE)\n\n`tax_level` is not speficified \nNo agglomeration will be performed\nOtherwise, please speficy `tax_level` by one of the following: \nKingdom, Phylum, Class, Order, Family, Genus\n\n\nWarning: The group variable has &lt; 3 categories \nThe multi-group comparisons (global/pairwise/dunnet/trend) will be deactivated\n\n\nObtaining initial estimates ...\n\n\nEstimating sample-specific biases ...\n\n\nANCOM-BC2 primary results ...\n\nres_ancom = ancom_out$res\n\nres_f_fig = res_ancom %&gt;%\n    dplyr::filter(diff_Carbon_sourcewood == TRUE) %&gt;% \n    dplyr::arrange(desc(lfc_Carbon_sourcewood)) %&gt;%\n    dplyr::mutate(direct = ifelse(lfc_Carbon_sourcewood &gt; 0, \"Positive LFC\", \"Negative LFC\"),\n                  color = ifelse(lfc_Carbon_sourcewood &gt; 0, \"aquamarine3\", \"black\"))\n\nres_f_fig$taxon = factor(res_f_fig$taxon, levels = res_f_fig$taxon)\nres_f_fig$direct = factor(res_f_fig$direct, \n                           levels = c(\"Positive LFC\", \"Negative LFC\"))\n\nfig_ancom = res_f_fig %&gt;%\n    ggplot(aes(x = lfc_Carbon_sourcewood, y = taxon, fill = direct)) + \n    geom_bar(stat = \"identity\", width = 0.7, color = \"black\", \n             position = position_dodge(width = 0.4)) +\n    geom_errorbar(aes(xmin = lfc_Carbon_sourcewood - se_Carbon_sourcewood, xmax = lfc_Carbon_sourcewood + se_Carbon_sourcewood), \n                  width = 0.2, position = position_dodge(0.05), color = \"black\") + \n    labs(y = NULL, x = \"Log fold change Wood versus carbon\", \n         title = \"Log fold changes\") + \n    scale_fill_discrete(name = NULL) +\n    scale_color_discrete(name = NULL) +\n    custom_theme() + \n    theme(plot.title = element_text(hjust = 0.5),\n          panel.grid.minor.y = element_blank(),\n          axis.text.y = element_text(size=6))\nfig_ancom\n\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- list(\n  ancom = as.character(res_f_fig$taxon), \n  deseq = as.character(rownames(res_sig_deseq)), \n  wilcox = as.character(wilcox_results$OTU),\n  aldex = sig_aldex2$OTU\n  )\n\nggvenn(\n  x, \n  fill_color = c(\"#0073C2FF\", \"#EFC000FF\", \"#868686FF\", \"#CD534CFF\"),\n  stroke_size = 0.5, set_name_size = 4\n  )\n\n\n\n\n\n\n\n# Get intersection\nv_table &lt;- venn(x, show.plot=FALSE)\nintersections&lt;-attr(v_table,\"intersections\")\nsig_otus_ven &lt;- intersections$`ancom:deseq:wilcox:aldex`\n\n# Plot\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_filt)[sig_otus_ven, ], \"matrix\")),as.data.frame(as(otu_table(physeq_filt)[sig_otus_ven, ], \"matrix\")))\n\ndf_long &lt;- reshape2::melt(df)\n\nUsing Kingdom, Phylum, Class, Order, Family, Genus as id variables\n\ndf_long &lt;- merge(df_long, metadata2, by.x = \"variable\", by.y = \"sample_id\")\n\nggplot(df_long, aes(x=Genus, y=value, color = Carbon_source)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n   facet_wrap(~Genus, scales = \"free\") +\n  scale_color_manual(values = c( \"orange\", \"purple\")) +\n  theme(axis.text.x = element_text())",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#setup",
    "href": "scripts/OTU_table_analysis.html#setup",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "We start with setting a path to working directory and setting a seed seed for normalization protocol.\nSetting a set is not essential but this way we make sure that we get the same results when normalizing our OTU table. If we randomly select some observations for any task in R or in any statistical software it results in different values all the time and this happens because of randomization. If we want to keep the values that are produced at first random selection then we can do this by storing them in an object after randomization or we can fix the randomization procedure so that we get the same results all the time.",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#installation-notes",
    "href": "scripts/OTU_table_analysis.html#installation-notes",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "Some packages required for this workflow are installed with BiocManager or devtools, if you need to install any of these tools, remove the # from the code and run it.\n\n#if (!require(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n\n#BiocManager::install(\"phyloseq\")\n#BiocManager::install(\"microbiome\")\n#BiocManager::install(\"ALDEx2\")\n#BiocManager::install(\"DESeq2\")",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#load-packages",
    "href": "scripts/OTU_table_analysis.html#load-packages",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "library(tidyverse) #general parsing\nlibrary(data.table) #general parsing\nlibrary(phyloseq) #phyloseq object loading\nlibrary(vegan) #rarefaction\nlibrary(microbiome) #normalization\nlibrary(ALDEx2) #stats\nlibrary(DESeq2) #stats\nlibrary(grid) #organizing multiple plots\nlibrary(gridExtra) #organizing multiple plots\nlibrary(scales) #plot aesthetic, comma setting\nlibrary(ANCOMBC) #stats\nlibrary(ggvenn) # vendiagram\nlibrary(gplots) #vendiagram",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#custom-functions",
    "href": "scripts/OTU_table_analysis.html#custom-functions",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "Next, we read in some custom things such as a theme that we will use for plotting our graphs and a color vectors.\nDefining a custom_theme for our plots is useful because it means that instead of re-writing the commands for our plot over and over again in each plot, we can just use the custom_theme function instead.\n\n#define custom theme for generating figures\ncustom_theme &lt;- function() {\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(), \n    panel.border =element_blank(),\n    axis.line.x = element_line(color=\"black\", size = 0.5),\n    axis.line.y = element_line(color=\"black\", size = 0.5),\n    strip.text.x = element_text(size = 7),\n    strip.text.y = element_text(size = 7),\n    strip.background = element_rect(fil=\"#FFFFFF\", color = \"black\", linewidth = 0.5),\n    axis.text.x = element_text(size = 7),\n    legend.text = element_text(size = 8), legend.title = element_text(size = 10)\n  )\n}\n\n#generate color scheme \nc25 &lt;- c(\"dodgerblue2\", \"#E31A1C\", \"green4\", \"#6A3D9A\", \"#FF7F00\", \"black\", \"gold1\", \"skyblue2\", \"#FB9A99\", \n        \"palegreen2\", \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\", \"deeppink1\", \"blue1\", \n        \"steelblue4\", \"darkturquoise\", \"green1\", \"yellow4\", \"yellow3\",\"darkorange4\", \"brown\")",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#read-in-the-data",
    "href": "scripts/OTU_table_analysis.html#read-in-the-data",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "An OTU table contains a column with the OTUs (taxonomic ranks in our case) and one column per sample with the counts how often OTU is found in the sample. It might look something like this:\n\n\n\n\n\n\n\n\n\n\n#NAME\nEP1910\nRMT\nKJB3\nTJR\n\n\n\n\nBacteria;Acidobacteria;Acidobacteriia;Acidobacteriales;Acidobacteriaceae (Subgroup 1);NA\n0\n0\n0\n0\n\n\nBacteria;Acidobacteria;Acidobacteriia;Solibacterales;Solibacteraceae (Subgroup 3);PAUC26f\n0\n5\n3\n1\n\n\n\n\n# Provide the path to the otu table\nfile_paths &lt;- c(\"../data/for_ma/otu-table-forMA.txt\")\n\n# Read in otu table\nmerged_otu_table &lt;- read.table(file_paths, header = T, sep = '\\t', comment = \"\")\ncolnames(merged_otu_table)[1] &lt;- \"taxid\"\n\n# Replace NA with 0\nmerged_otu_table[is.na(merged_otu_table)] &lt;- 0\n\n# R does not like - and will replace things with dots, clean up the sample names\ncolnames(merged_otu_table) &lt;- gsub(\"\\\\.\", \"_\", colnames(merged_otu_table)) \n\n#use the taxon as rownames\nrownames(merged_otu_table) &lt;- merged_otu_table$taxid\nmerged_otu_table$taxid &lt;- NULL\n\n#check how many otus and samples we have\ndim(merged_otu_table)\n\n[1] 1248   20\n\n\nWith this example OTU table, we work with dim(merged_otu_table)[2] samples and 1248 OTUs.\n\n\n\nThe metadata table contains information about our samples and can look something like this:\n\n\n\n#NAME\ntreatment\nDate\n\n\n\n\nEP1910\nwood\n2023_1\n\n\nRMT\npaper\n2023_1\n\n\nKJB3\nmix\n2023_1\n\n\nTJR\npaper\n2023_1\n\n\nIB5\nwood\n2023_1\n\n\nALIDNA\nwood\n2023_1\n\n\nIG7\npaper\n2023_1\n\n\nB314\nmix\n2023_1\n\n\n\n\n# Read in metadata file\nmetadata_combined &lt;- read.table(\"../data/for_ma/sample_table.txt\", header = TRUE, row.names = 1, sep = \"\\t\", comment.char = \"\")\n\n# Replace dashes with underscores in row names\nrownames(metadata_combined) &lt;- gsub(\"-\", \"_\", rownames(metadata_combined))\n\n# Ensure that the group data is categorical \nmetadata_combined$Practical_group &lt;- gsub(\"^\", \"Day\", metadata_combined$Practical_group)\n\n# Add extra column for sample names\nmetadata_combined$name &lt;- paste0(metadata_combined$Carbon_source, \"_\", rownames(metadata_combined))\nmetadata_combined$sample_id &lt;- rownames(metadata_combined)\n\n# Order the factors for our names column\nmetadata_combined &lt;- metadata_combined |&gt; \n  arrange(desc(Carbon_source))\n\n# View output\nhead(metadata_combined)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarcode\nCarbon_source\nWino_Column\nPractical_group\nname\nsample_id\n\n\n\n\nEP_SD1\nBC10\nwood\nSD1\nDay1\nwood_EP_SD1\nEP_SD1\n\n\nRK_SD1\nBC12\nwood\nSD1\nDay1\nwood_RK_SD1\nRK_SD1\n\n\nEW_SD1\nBC23\nwood\nSD1\nDay2\nwood_EW_SD1\nEW_SD1\n\n\nDG_SD1\nBC24\nwood\nSD1\nDay2\nwood_DG_SD1\nDG_SD1\n\n\nAW_SD2\nBC05\nwood\nSD2\nDay2\nwood_AW_SD2\nAW_SD2\n\n\nUNZ_SD2\nBC06\nwood\nSD2\nDay2\nwood_UNZ_SD2\nUNZ_SD2",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#generate-taxonomy-file",
    "href": "scripts/OTU_table_analysis.html#generate-taxonomy-file",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "Next, we generate a table that list the taxonomy information for each taxonomic rank. We do this by taking the information from our OTU table. Depending on how you analysed your 16S rRNA gene sequences, you might have an OTU table with IDs (ASV1, ASV2, … or OTU1, OTU2, …) and a separate table with the taxonomy information.\nIf that is the case, you can read in the taxonomy information separate.\n\n#extract taxonomy string\ntemp &lt;- as.data.frame(rownames(merged_otu_table))\ncolnames(temp) &lt;- \"OTU\"\n\n#separate the taxonomic headers                      \ntaxonomy_file &lt;- temp |&gt; \n  distinct(OTU) |&gt; \n  separate(OTU,\n           c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\"), \n           sep = \";\", remove = FALSE) |&gt; \n  column_to_rownames(var = \"OTU\")\n\n#view file\nhead(taxonomy_file)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKingdom\nPhylum\nClass\nOrder\nFamily\nGenus\n\n\n\n\nBacteria;10bav-F6;NA;NA;NA;NA\nBacteria\n10bav-F6\nNA\nNA\nNA\nNA\n\n\nBacteria;Abditibacteriota;Abditibacteria;Abditibacteriales;Abditibacteriaceae;Abditibacterium\nBacteria\nAbditibacteriota\nAbditibacteria\nAbditibacteriales\nAbditibacteriaceae\nAbditibacterium\n\n\nBacteria;Acetothermia;Acetothermiia;NA;NA;NA\nBacteria\nAcetothermia\nAcetothermiia\nNA\nNA\nNA\n\n\nBacteria;Acidobacteriota;Acidobacteriae;AKIW659;NA;NA\nBacteria\nAcidobacteriota\nAcidobacteriae\nAKIW659\nNA\nNA\n\n\nBacteria;Acidobacteriota;Acidobacteriae;Bryobacterales;Bryobacteraceae;Bryobacter\nBacteria\nAcidobacteriota\nAcidobacteriae\nBryobacterales\nBryobacteraceae\nBryobacter\n\n\nBacteria;Acidobacteriota;Acidobacteriae;PAUC26f;NA;NA\nBacteria\nAcidobacteriota\nAcidobacteriae\nPAUC26f\nNA\nNA",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#generate-phyloseq-object",
    "href": "scripts/OTU_table_analysis.html#generate-phyloseq-object",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "A phyloseq object combines different elements of an analysis (i.e. the OTU table, the list of taxa and the mapping file) into one single object. We can easily generate such an object with the three dataframes we have generated above:\n\n#combine data\nOTU = otu_table(merged_otu_table, taxa_are_rows = TRUE)\nTAX = tax_table(as.matrix(taxonomy_file))\nphyseq = phyloseq(OTU, TAX, sample_data(metadata_combined))\n\n#view structure\nphyseq\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1248 taxa and 20 samples ]\nsample_data() Sample Data:       [ 20 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1248 taxa by 6 taxonomic ranks ]",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#exploring-the-raw-data",
    "href": "scripts/OTU_table_analysis.html#exploring-the-raw-data",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "Below, we write a custom function to calculate some summary statistics. We easily could do this without a function, however, since we want to compare the statistics before and after filtering the OTU table, the function is useful to have, since we do not need to copy-paste the exact same code in two spots of the workflow:\n\nprint_summary &lt;- function(reads_per_OTU) {\n  total_reads &lt;- sum(reads_per_OTU)\n  otu_number &lt;- length(reads_per_OTU)\n  num_singletons &lt;- length(reads_per_OTU[reads_per_OTU == 1])\n  num_doubletons &lt;- length(reads_per_OTU[reads_per_OTU == 2])\n  num_less_than_10 &lt;- length(reads_per_OTU[reads_per_OTU &lt; 10])\n  total_reads_less_than_10 &lt;- sum(reads_per_OTU[reads_per_OTU &lt; 10])\n  perc_reads_less_than_10 &lt;- (total_reads_less_than_10 / sum(reads_per_OTU)) * 100\n  \n  cat(\"Total number of reads:\", format(total_reads, big.mark = \",\"), \"\\n\")\n  cat(\"Number of OTUs\",  format(otu_number, big.mark = \",\"), \"\\n\")\n  cat(\"Number of singleton OTUs:\",  format(num_singletons, big.mark = \",\"), \"\\n\")\n  cat(\"Number of doubleton OTUs:\",  format(num_doubletons, big.mark = \",\"), \"\\n\")\n  cat(\"Number of OTUs with less than 10 seqs:\",  format(num_less_than_10, big.mark = \",\"), \"\\n\")\n  cat(\"Total reads for OTUs with less than 10 seqs:\",  format(total_reads_less_than_10, big.mark = \",\"), \"\\n\")\n  cat(\"Percentage of reads for OTUs with less than 10 seqs:\",  sprintf(\"%.2f%%\", perc_reads_less_than_10), \"\\n\")\n  \n}\n\n#calculate the number of reads found per otu\nreads_per_OTU &lt;- taxa_sums(physeq)\n\n#summarize the data\nprint_summary(reads_per_OTU)\n\nTotal number of reads: 366,981 \nNumber of OTUs 1,248 \nNumber of singleton OTUs: 245 \nNumber of doubleton OTUs: 95 \nNumber of OTUs with less than 10 seqs: 609 \nTotal reads for OTUs with less than 10 seqs: 1,728 \nPercentage of reads for OTUs with less than 10 seqs: 0.47% \n\n\nFor this workflow, we define *singletons** as reads/OTUs with a sequence that is present exactly once in the dataset.\nNotice that another definition of singletons can be as taxa/OTU present in a single sample.\nIn amplicon data analysis it is useful to remove reads with low counts because they are very likely due to sequencing errors. We generally assume that sequencing errors are independent and randomly distributed, and we can assume that erroneous sequences will occur much less often than the true sequence. We will remove such sequences during the data filtering step.\n\n\n\nNext, let’s explore how many reads we have per sample:\n\n#count the number of reads per sample\nsample_counts &lt;- as.data.frame(colSums(merged_otu_table))\n                  \n#clean up the dataframe\nnames(sample_counts)[1] &lt;- \"counts\"\nsample_counts$sampleID &lt;- rownames(sample_counts)\n\n#plot counts\np_counts &lt;-\n  ggplot(data = sample_counts, aes(x = reorder(sampleID, counts, FUN=sum, decreasing = TRUE), y = counts)) +\n  geom_point() +\n  geom_text(aes(x = , sampleID, y = counts, label = counts),  hjust = 0, nudge_y = 200 , size = 2.5) +\n  coord_flip() +\n  xlab(\"\") + \n  ylab(\"Read counts\") +\n  custom_theme()\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\np_counts\n\n\n\n\n\n\n\n\nIn this example, we see two samples with almost no reads and we want to make sure to remove these samples. We also see that we have a large difference between different samples. To be able to compare for example sample IV (~25,000 reads) with sample MN (~1,000 reads) we need to normalize our data after the data filtering step.",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#filter-data",
    "href": "scripts/OTU_table_analysis.html#filter-data",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "Next, we filter the data. Specifically, we:\n\nRemove OTUs that are not assigned to anything at Phylum rank. The subset_taxa function can be used to remove any taxa you want, i.e. if you have plant DNA in your sample, you could use this to remove chloroplast sequences as well.\n\nRemove samples with total read counts less than 20. This cutoff is arbitrary and depends a bit on your data. To choose a good value, explore the read counts you have per sample and define a cutoff based on that. In this example, we mainly want to remove the two low read samples we have seen in our plot.\n\nRemove low count OTUs: The threshold is up to you; removing singletons or doubletons is common, but you can be more conservative and remove any counts less than 10. Look at the plots before and get a feeling for how many OTUs were removed.\n\nIn our example, we want to remove samples with 20 or less reads, remove singletons only and remove OTUs that occur in less than 10% of our samples (v1). Since there are many different thoughts about OTU table filtering, you can also find two other options (v2 and v3) on how to filter a table.\nFor most analyses we will work with the filtered data but there are some diversity metrics which rely on the presence of singletons within a sample (richness estimates, i.e. Chao), so you might choose to leave them in for those sorts of analyses only.\n\n#define cutoffs\ncounts_per_sample &lt;- 20\notu_nr_cutoff &lt;- 1\nmin_percentage_samples &lt;- 10\n\n#remove taxa without tax assignment at Phylum rank\nphyseq_filt &lt;- subset_taxa(physeq, Phylum != \"NA\")\n\n#remove samples with less than 20 reads\nphyseq_filt &lt;- prune_samples(sample_sums(physeq)&gt;= counts_per_sample, physeq)\n\n#calculate the minimum number of samples an otu should be present in\nmin_samples &lt;- ceiling((min_percentage_samples / 100) * nsamples(physeq_filt))\n\n#remove otus that occur only rarely (v1)\n#here, we calculate the total abundance of each otu across all samples and checks if it's greater than the specified otu_nr_cutoff AND we check if the otu occurs in at least &lt;min_percentage_samples&gt;% of samples\n#we only retain OTUs that satisfy this condition \nphyseq_filt &lt;- prune_taxa(taxa_sums(physeq_filt) &gt; otu_nr_cutoff & taxa_sums(physeq_filt) &gt;= min_samples, physeq_filt)\nphyseq_filt\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 989 taxa and 19 samples ]\nsample_data() Sample Data:       [ 19 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 989 taxa by 6 taxonomic ranks ]\n\n#remove otus that occur only rarely (v2)\n#here, we count the number of samples where the abundance of an otu is &gt; 0. \n#If this count is greater than the specified otu_nr_cutoff, the taxon is retained.\n#physeq_filt &lt;- filter_taxa(physeq, function (x) {sum(x &gt; 0) &gt; otu_nr_cutoff}, prune=TRUE)\n#physeq_filt\n\n#remove otus that occur only rarely (v3)\n#here, we remove taxa not seen more than 1 times in at least 10% of the samples\n#physeq_filt = filter_taxa(physeq, function(x) sum(x &gt; 1) &gt; (0.1*length(x)), TRUE)\n#physeq_filt\n\nNext, we can calculate the summary statistics with the custom taxa_sums function we have defined before:\n\n#calculate the number of reads found per otu\nreads_per_OTU_filt &lt;- taxa_sums(physeq_filt)\n\n#summarize the data\nprint_summary(reads_per_OTU_filt)\n\nTotal number of reads: 366,733 \nNumber of OTUs 989 \nNumber of singleton OTUs: 0 \nNumber of doubleton OTUs: 95 \nNumber of OTUs with less than 10 seqs: 350 \nTotal reads for OTUs with less than 10 seqs: 1,483 \nPercentage of reads for OTUs with less than 10 seqs: 0.40%",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#normalize-data",
    "href": "scripts/OTU_table_analysis.html#normalize-data",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "Below, we generate three new phyloseq objects using three different normalization approaches: 1. Compositional: transforms the data into relative abundance 2. CLR: stands for centerd log-ratio transform and allows us to compare proportions of OTUs within each sample. After this transformation the values will no longer be counts, but rather the dominance (or lack thereof) for each taxa relative to the geometric mean of all taxa on the logarithmic scale 3. Rarefaction: scales all of your reads down to the lowest total sequencing depth. Notice, that this might drop many OTUs in higher read samples and might lead to under-estimation of low-abundant OTUs\nGenerating different phyloseq objects for different normalization approaches allows us to easily compare analysis steps with different inputs.\n\nphyseq_rel_abundance &lt;- microbiome::transform(physeq_filt, \"compositional\")\nphyseq_clr &lt;- microbiome::transform(physeq_filt, \"clr\")\nphyseq_rarified &lt;- rarefy_even_depth(physeq_filt)",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "scripts/OTU_table_analysis.html#data-exploration",
    "href": "scripts/OTU_table_analysis.html#data-exploration",
    "title": "Analysing 16S rRNA data with R",
    "section": "",
    "text": "Rarefactions illustrate how well your sample was sampled. The rarefaction function takes a random subsample of each column in your OTU table of a given size, starting with a very small subsample, and counts how many unique OTUs were observed in that subsample. The analysis is repeated with increasing subsample sizes until the maximum actual read depth for your table is reached.\nIn an ideal dataset, each curve should reach a plateau, i.e. horizontal asymptote, ideally around the same depth. While this almost never is the case, exploring these graphs gives us an idea how well each sample was sampled.\n\n#create a df from our otu table\ndf &lt;- as.data.frame(otu_table(physeq_filt))\n\n#rarefy data with a step size of 50, using tidy = TRUE will return a dataframe instead of a plot\ndf_rate &lt;- rarecurve(t(df), step=50, cex=0.5, label = FALSE, tidy = TRUE)\n\n#add metadata\ndf_rate &lt;- merge(df_rate, metadata_combined, by.x = \"Site\", by.y = \"sample_id\")\n\n#plot\ndf_rate %&gt;%\n  group_by(Site) %&gt;%\n  mutate(max_sample = max(Sample)) %&gt;%\n  mutate(label = if_else(Sample == max_sample, as.character(Site), NA_character_)) %&gt;%\n  ggplot(aes(x = Sample, y = Species, color = Carbon_source, group = interaction(Site, Carbon_source))) + \n  geom_line() + \n  facet_grid(cols = vars(Carbon_source)) +\n  geom_text(aes(label = label),\n            position = position_nudge(x = 2000),\n            na.rm = TRUE, size = 3) +\n  custom_theme()\n\n\n\n\n\n\n\n\nIn this example we can see that almost none of the sample reach a plateau, RH is the closest but not there yet. This suggests that we mainly sampled the abundant members of our community and might miss many rare taxa. This is something to keep in mind for other analyses, such as alpha diversity analyses.\n\n\n\nAlpha diversity measures the diversity within our sample and we distinguish between species richness (i.e. the number of species) and species richness (i.e. how relatively abundant each of the species are).\n\n#calculate different alpha diversity indicators\nrichness_meta &lt;-microbiome::alpha(physeq_filt, index = \"all\")\n\n#add the sample id to table\nrichness_meta$sample_id &lt;- rownames(richness_meta)\n\n#add other metadata data\nrichness_meta  &lt;- merge(richness_meta, metadata_combined, by = \"sample_id\")\n\n#check what parameters are calculated\ncolnames(richness_meta)\n\n [1] \"sample_id\"                  \"observed\"                  \n [3] \"chao1\"                      \"diversity_inverse_simpson\" \n [5] \"diversity_gini_simpson\"     \"diversity_shannon\"         \n [7] \"diversity_fisher\"           \"diversity_coverage\"        \n [9] \"evenness_camargo\"           \"evenness_pielou\"           \n[11] \"evenness_simpson\"           \"evenness_evar\"             \n[13] \"evenness_bulla\"             \"dominance_dbp\"             \n[15] \"dominance_dmn\"              \"dominance_absolute\"        \n[17] \"dominance_relative\"         \"dominance_simpson\"         \n[19] \"dominance_core_abundance\"   \"dominance_gini\"            \n[21] \"rarity_log_modulo_skewness\" \"rarity_low_abundance\"      \n[23] \"rarity_rare_abundance\"      \"Barcode\"                   \n[25] \"Carbon_source\"              \"Wino_Column\"               \n[27] \"Practical_group\"            \"name\"                      \n\n\nNext, we can generate a plot. Since we calculated the diversity estimates after removing singletons, we will look at evenness indices, such as the Shannon index.\n\n#generate figure\nalpha_plot &lt;-\n  ggplot(richness_meta, aes(x = Carbon_source, y = chao1)) +\n    geom_boxplot() +\n    geom_jitter(aes(color = Practical_group), width = 0.2, size = 4) +\n    labs(x = \"\", y = \"Chao1 index\") +\n    custom_theme() +\n    theme(axis.title.y = element_text(size = 20),\n          axis.text.x = element_text(size = 20),\n          axis.text.y = element_text(size = 14),\n          legend.key=element_blank(), \n          legend.text = element_text(size=14),\n          legend.title = element_text(size=20)\n          ) \n\nalpha_plot\n\n\n\n\n\n\n\n#save the figure to our computer\n#ggsave(paste0(\"results/plots/alpha-div.png\"), plot=alpha_plot, device=\"png\")\n\nWe see that there is not a difference in terms of species evenness when comparing our different samples.\n\n\n\nIn contrast to alpha diversity, beta diversity quantifies the dissimilarity between communities (multiple samples).\nCommonly used metrics include: - the Bray-Curtis index (for compositional/abundance data) - Jaccard index (for presence/absence data, ignoring abundance information) - Aitchison distance (Euclidean distance for clr transformed abundances, aiming to avoid the compositionality bias) - Unifrac distance (that takes into account the phylogenetic tree information) - …\nAvoid methods that use Euclidean distance as microbiome data are sparse datasets and better suited for the above mentioned methods.\nBased on the type of algorithm, ordination methods can be divided in two categories: - unsupervised, which measure variation in the data without additional information on covariates or other supervision of the model, including: - Principal Coordinate Analysis (PCoA) - Principal Component Analysis (PCA) - Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) - supervised ordination: - distance-based Redundancy Analysis (dbRDA)\n\n\nFor Bray-Curtis our data should not have negative entries, so we will use the relative abundance (or rarefied) data:\n\n#convert otu table to df\ndata_otu_filt_rar &lt;- t(data.frame(otu_table(physeq_rarified)))\n\n#calculate bray-curtis \ndist_bc &lt;- as.matrix(vegdist(data_otu_filt_rar, method = \"bray\")) \n\n# calculate PCOA using Phyloseq package\npcoa_bc &lt;- ordinate(physeq_rarified, \"PCoA\", \"bray\") \n\nplot_ordination(physeq_rarified, pcoa_bc, color = \"Carbon_source\", shape = \"Practical_group\") + \n  geom_point(size = 3) +\n  custom_theme()\n\n\n\n\n\n\n\n\nFirst, this two-dimensions PCOA plot show 34% of the total variance between the samples. Next, we see that the our samples are not forming distinct clusters, i.e. microbiomes from the mixed, paper and wood communities appear very similar.\nNext, we can do a statistical test:\n\n#extract metdatatas\nmetadata2 &lt;- data.frame(meta(physeq_rarified))\n  \n# Permanova test using the vegan package\nadonis2(data_otu_filt_rar~Carbon_source, data = metadata2, permutations=9999, method=\"bray\")\n\n\n\n\n\n\nDf\nSumOfSqs\nR2\nF\nPr(&gt;F)\n\n\n\n\nCarbon_source\n1\n0.6906827\n0.2075976\n4.453746\n0.0025\n\n\nResidual\n17\n2.6363440\n0.7924024\nNA\nNA\n\n\nTotal\n18\n3.3270267\n1.0000000\nNA\nNA\n\n\n\n\n\n\nThis confirms that there do not seem to be any statistically significant differences between our samples.\n\n\n\n\nNext, we generate a beta-diversity ordination using the Aitchison distance. We do this by applying PCA to the centered log-ratio (CLR) transformed counts.\n\n#PCA via phyloseq\n#RDA without constraints is a PCA\nord_clr &lt;- phyloseq::ordinate(physeq_clr, \"RDA\", distance = \"euclidian\")\n\n#Plot scree plot to plot eigenvalues, i.e.the total amount of variance that can be explained by a given principal componen\nphyloseq::plot_scree(ord_clr) + \n  geom_bar(stat=\"identity\", fill = \"blue\") +\n  custom_theme() +\n  labs(x = \"\\nAxis\", y = \"Proportion of Variance\\n\")\n\n\n\n\n\n\n\n\n\n#Scale axes\nclr1 &lt;- ord_clr$CA$eig[1] / sum(ord_clr$CA$eig)\nclr2 &lt;- ord_clr$CA$eig[2] / sum(ord_clr$CA$eig)\n\n#and plot\nphyloseq::plot_ordination(physeq_clr, ord_clr, color = \"Carbon_source\", shape = \"Practical_group\") + \n  geom_point(size = 2,) +\n  coord_fixed(clr2 / clr1) +\n  stat_ellipse(aes(group = Carbon_source), linetype = 2) +\n  custom_theme() \n\n\n\n\n\n\n\n\nWhile PCA is an exploratory data visualization tool, we can test whether the samples cluster beyond that expected by sampling variability using permutational multivariate analysis of variance (PERMANOVA). It does this by partitioning the sums of squares for the within- and between-cluster components using the concept of centroids. Many permutations of the data (i.e. random shuffling) are used to generate the null distribution. The test from ADONIS can be confounded by differences in dispersion (or spread)…so we want to check this as well.\n\n#Generate distance matrix\nclr_dist_matrix &lt;- phyloseq::distance(physeq_clr, method = \"euclidean\") \n\n#ADONIS test\nadonis2(clr_dist_matrix~Carbon_source, data = metadata2, permutations=9999, method=\"bray\")\n\n\n\n\n\n\nDf\nSumOfSqs\nR2\nF\nPr(&gt;F)\n\n\n\n\nCarbon_source\n1\n6379.877\n0.183901\n3.830806\n4e-04\n\n\nResidual\n17\n28312.034\n0.816099\nNA\nNA\n\n\nTotal\n18\n34691.911\n1.000000\nNA\nNA\n\n\n\n\n\n\n\n\n\nNext, we want to plot the taxa distribution. Let us first look at the most abundant phyla and check how similar our different samples are:\n\n#condense data at specific tax rank, i.e. on phylum level\ngrouped_taxa &lt;- tax_glom(physeq_rel_abundance, \"Phylum\")\n  \n#find top19 most abundant taxa \ntop_taxa &lt;- names(sort(taxa_sums(grouped_taxa), TRUE)[1:19])\n\n#make a list for the remaining taxa\nother_taxa &lt;- names(taxa_sums(grouped_taxa))[which(!names(taxa_sums(grouped_taxa)) %in% top_taxa)]\n\n#group the low abundant taxa into one group\nmerged_physeq = merge_taxa(grouped_taxa, other_taxa, 2)\n  \n#transform phyloseq object into dataframe\ndf &lt;- psmelt(merged_physeq)\n\n#cleanup the names in the df\nnames(df)[names(df) == \"Phylum\"] &lt;- \"tax_level\"\n\n#replace NAs, with other (NAs are generated for the other category)\ndf$tax_level[which(is.na(df$tax_level))] &lt;- \"Other\"\n  \n#create a df to sort taxa labels by abundance\nsorted_labels &lt;- df |&gt; \n  group_by(tax_level) |&gt; \n  summarise(sum = sum(Abundance)) |&gt; \n  arrange(desc(sum))\n  \n#Get list of sorted levels excluding \"Other\"\ndesired_levels &lt;- setdiff(sorted_labels$tax_level, \"Other\")\n  \n#sort df using the sorted levels and ensure that \"Other\" is the last category\ndf$tax_level2 &lt;- factor(df$tax_level, levels = c(desired_levels, \"Other\"))\n  \n#generate color scheme\ncols &lt;- c25[1:length(unique(df$tax_level2))]\ncols[levels(df$tax_level2) == \"Other\"] &lt;- \"#CCCCCC\"\n  \n#plot\nfig &lt;-\n  ggplot(df, aes(x = Sample, y = Abundance, fill = tax_level2) ) +\n    geom_bar(position = \"stack\", stat = \"identity\", width = 0.9) +\n    labs(y = \"Relative abundance\", x = \"\", title = paste0(\"Relative abundance at \", \"Phylum\", \" rank\")) +\n    scale_fill_manual(name = paste0(\"Phylum\",\"_rank\"), labels = levels(df$tax_level2), values = cols) +\n    facet_grid(cols =  vars(Carbon_source), scales = \"free\", space = \"free\") +\n    scale_y_continuous(expand = c(0, 0), limits = c(0, 1.01)) +\n    custom_theme() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1 ) ) +\n    guides(fill=guide_legend(title = \"Phylum\"))\n\nfig\n\n\n\n\n\n\n\n\nSince we want to generate one plot for each taxonomic rank, i.e. Phylum, Class, Order,…, we can do this in a loop. The figures will be generated in the folder results/plots/.\nIf you do not feel comfortable with a lob, you can also do this step by step by removing the for statement and replacing all instances of level with the taxonomic rank you want to investigate\n\n#if not there already, create output folder\nimg_path=\"../results/r_analyses/images/\"\ndir.create(img_path, recursive = TRUE)\n\n#generate one barplot for each taxonomic level\nfor (level in colnames(taxonomy_file)){\n  \n  #condense data at specific tax rank, i.e. on phylum level\n  grouped_taxa &lt;- tax_glom(physeq_rel_abundance, level)\n  \n  #find top19 most abundant taxa \n  top_taxa &lt;- names(sort(taxa_sums(grouped_taxa), TRUE)[1:19])\n  \n  #make a list for the remaining taxa\n  other_taxa &lt;- names(taxa_sums(grouped_taxa))[which(!names(taxa_sums(grouped_taxa)) %in% top_taxa)]\n  \n  #group the low abundant taxa into one group\n  merged_physeq = merge_taxa(grouped_taxa, other_taxa, 2)\n  \n  #transform phyloseq object into dataframe\n  df &lt;- psmelt(merged_physeq) \n  \n  #cleanup the dataframe\n  names(df)[names(df) == level] &lt;- \"tax_level\"\n  df$tax_level[which(is.na(df$tax_level))] &lt;- \"Other\"\n  \n  #create a df to sort taxa labels by abundance\n  sorted_labels &lt;- df |&gt; \n    group_by(tax_level) |&gt; \n    summarise(sum = sum(Abundance)) |&gt; \n    arrange(desc(sum))\n  \n  #Get list of sorted levels excluding \"Other\"\n  desired_levels &lt;- setdiff(sorted_labels$tax_level, \"Other\")\n  \n  #sort df using the sorted levels and ensure that \"Other\" is the last category\n  df$tax_level2 &lt;- factor(df$tax_level, levels = c(desired_levels, \"Other\"))\n  \n  #generate color scheme\n  cols &lt;- c25[1:length(unique(df$tax_level2))]\n  cols[levels(df$tax_level2) == \"Other\"] &lt;- \"#CCCCCC\"\n  \n  #plot\n  fig &lt;-\n    ggplot(df, aes(x = Sample, y = Abundance, fill = tax_level2) ) +\n      geom_bar(position = \"stack\", stat = \"identity\", width = 0.9) +\n      labs(y = \"Relative abundance\", x = \"\", title = paste0(\"Relative abundance at \", level, \" rank\")) +\n      scale_fill_manual(name = paste0(level,\"_rank\"), labels = levels(df$tax_level2), values = cols) +\n      facet_grid(cols =  vars(Carbon_source), scales = \"free\", space = \"free\") +\n      scale_y_continuous(expand = c(0, 0), limits = c(0, 1.01)) +\n      custom_theme() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1 ) ) +\n      guides(fill=guide_legend(title=level))\n  \n  ggsave(paste0(img_path, level, \"_barplot_ra.png\"), plot = fig, device=\"png\")\n  }\n\nGenerated plots:\n\n\n\n\n\n\n \n\n\n \n\n\n\n\n\n\nNotice: This is still in development and needs to be optimized, use with care!\n\n\nFirst, let us compare for differences for a category where we compare two factors. For example, we might want to ask whether there are any significant differences when comparing our wood versus paper samples.\nLet’s first tes test this using the non-parametric Wilcoxon rank-sum test.\n\n#Generate data.frame with OTUs and metadata\nps_wilcox &lt;- data.frame(t(data.frame(phyloseq::otu_table(physeq_clr))), check.names = FALSE)\nps_wilcox$Carbon_source &lt;- phyloseq::sample_data(physeq_clr)$Carbon_source\n\n#Define functions to pass to map\nwilcox_model &lt;- function(df){\n  wilcox.test(abund ~ Carbon_source, data = df)\n}\n\nwilcox_pval &lt;- function(df){\n  wilcox.test(abund ~ Carbon_source, data = df)$p.value\n}\n\n#Create nested data frames by OTU and loop over each using map \nwilcox_results &lt;- ps_wilcox %&gt;%\n  gather(key = OTU, value = abund, -Carbon_source) %&gt;%\n  group_by(OTU) %&gt;%\n  nest() %&gt;%\n  mutate(wilcox_test = map(data, wilcox_model),\n         p_value = map(data, wilcox_pval))  \n\n#Show results\nwilcox_results$wilcox_test[[1]]\n\n\n    Wilcoxon rank sum exact test\n\ndata:  abund by Carbon_source\nW = 44, p-value = 1\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe can also test for differences per OTU:\n\n#Unnesting\nwilcox_results &lt;- wilcox_results %&gt;%\n  dplyr::select(OTU, p_value) %&gt;%\n  unnest(cols = c(p_value))\n\nhead(wilcox_results)\n\n\n\n\n\n\n\n\n\nOTU\np_value\n\n\n\n\nBacteria;10bav-F6;NA;NA;NA;NA\n1.0000000\n\n\nBacteria;Abditibacteriota;Abditibacteria;Abditibacteriales;Abditibacteriaceae;Abditibacterium\n0.9038925\n\n\nBacteria;Acetothermia;Acetothermiia;NA;NA;NA\n0.0120664\n\n\nBacteria;Acidobacteriota;Acidobacteriae;Bryobacterales;Bryobacteraceae;Bryobacter\n0.2375433\n\n\nBacteria;Acidobacteriota;Acidobacteriae;PAUC26f;NA;NA\n0.0754148\n\n\nBacteria;Acidobacteriota;Acidobacteriae;TSBb06;NA;NA\n0.0068006\n\n\n\n\n\n#Computing FDR corrected p-values (since we do multiple statistical comparisons)\nwilcox_results &lt;- wilcox_results %&gt;%\n  arrange(p_value) %&gt;%\n  mutate(BH_FDR = p.adjust(p_value, \"BH\")) %&gt;%\n  filter(BH_FDR &lt; 0.001) %&gt;%\n  dplyr::select(OTU, p_value, BH_FDR, everything())\n\nhead(wilcox_results)  \n\n\n\n\n\n\n\n\n\n\nOTU\np_value\nBH_FDR\n\n\n\n\nBacteria;Bacteroidota;Bacteroidia;Bacteroidales;Ika33;NA\n2.65e-05\n2.65e-05\n\n\nBacteria;Firmicutes;Clostridia;Eubacteriales;Alkalibacteraceae;Alkalibacter\n2.65e-05\n2.65e-05\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;Acetivibrio\n2.65e-05\n2.65e-05\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;Ruminiclostridium\n2.65e-05\n2.65e-05\n\n\nBacteria;Firmicutes;Clostridia;Peptostreptococcales-Tissierellales;NA;JTB215\n2.65e-05\n2.65e-05\n\n\nBacteria;Verrucomicrobiota;Lentisphaeria;Oligosphaerales;Lenti-02;NA\n2.65e-05\n2.65e-05\n\n\n\n\n\n\nWilcoxon has some down-sites when it comes to treating zero values, an alternative approach is to use the ANOVA-like differential expression (ALDEx2) method. The aldex function is a wrapper that performs log-ratio transformation and statistical testing in a single line of code, which is why we feed the non-normalized data into it:\n\naldex2_da &lt;- ALDEx2::aldex(data.frame(phyloseq::otu_table(physeq_filt)), phyloseq::sample_data(physeq_filt)$Carbon_source, test=\"t\", effect = TRUE, denom=\"iqlr\")\n\naldex.clr: generating Monte-Carlo instances and clr values\n\n\noperating in serial mode\n\n\ncomputing iqlr centering\n\n\naldex.ttest: doing t-test\n\n\naldex.effect: calculating effect sizes\n\n\nSpecifically, this function:\n\ngenerates Monte Carlo samples of the Dirichlet distribution for each sample,\nconverts each instance using a log-ratio transform,\n\nreturns test results for two sample (Welch’s t, Wilcoxon) or multi-sample (glm, Kruskal-Wallace) tests.\n\niqlr” accounts for data with systematic variation and centers the features on the set features that have variance that is between the lower and upper quartile of variance. This provides results that are more robust to asymmetric features between groups.\nNext, we can plot the effect size. The effect size plot shows the median log2 fold difference by the median log2 dispersion. This is a measure of the effect size by the variability. Differentially abundant taxon will be those where the difference most exceeds the dispersion. Points toward the top of the figure are more abundant in CF samples while those towards the bottom are more abundant in healthy controls. Taxa with BH-FDR corrected p-values are shown in red.\n\n#plot effect sizes\nALDEx2::aldex.plot(aldex2_da, type=\"MW\", test=\"wilcox\", called.cex = 1, cutoff.pval = 0.001)\n\n\n\n\n\n\n\n\nFinally, we can print the output with the taxa information added.\n\n#Clean up presentation\nsig_aldex2 &lt;- aldex2_da %&gt;%\n  rownames_to_column(var = \"OTU\") %&gt;%\n  filter(wi.eBH &lt; 0.05) %&gt;%\n  arrange(effect, wi.eBH) %&gt;%\n  dplyr::select(OTU, diff.btw, diff.win, effect, wi.ep, wi.eBH)\n\nhead(sig_aldex2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOTU\ndiff.btw\ndiff.win\neffect\nwi.ep\nwi.eBH\n\n\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;Ruminiclostridium\n-7.603371\n2.1386233\n-3.540289\n0.0000265\n0.0049666\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;Acetivibrio\n-9.397042\n3.2100297\n-2.922520\n0.0000265\n0.0049666\n\n\nBacteria;Firmicutes;Clostridia;Peptostreptococcales-Tissierellales;NA;JTB215\n-2.130290\n0.9848773\n-2.024687\n0.0000376\n0.0055734\n\n\nBacteria;SAR324 clade(Marine group B);NA;NA;NA;NA\n-2.565471\n1.1989183\n-2.016483\n0.0001385\n0.0102657\n\n\nBacteria;Verrucomicrobiota;Lentisphaeria;Oligosphaerales;Lenti-02;NA\n-2.327235\n1.2450759\n-1.902249\n0.0000920\n0.0072225\n\n\nBacteria;Firmicutes;Clostridia;Oscillospirales;Hungateiclostridiaceae;uncultured\n-2.139205\n1.1714844\n-1.793109\n0.0000761\n0.0071607\n\n\n\n\n\n\nIn our case an empty dataframe is returned, no significant differences were detected.\n\n\n\nDESeq2 performs a differential expression analysis based on the Negative Binomial (a.k.a. Gamma-Poisson) distribution. DeSeq normalizes the data throughout its analysis, so we input only the filtered data. For more theory, visit this page.\n\n#convert treatment column to a factor\nsample_data(physeq_filt)$Carbon_source &lt;- as.factor(sample_data(physeq_filt)$Carbon_source)\n\n#Convert the phyloseq object to a DESeqDataSet and run DESeq2:\nds &lt;- phyloseq_to_deseq2(physeq_filt, ~ Carbon_source)\n\nconverting counts to integer mode\n\n#since our samples contain a lot of 0s (something DeSeq is NOT designed for) we use some alternative means to estimate the size factor\nds &lt;-estimateSizeFactors(ds, type = 'poscounts')\nds &lt;- DESeq(ds)\n\nusing pre-existing size factors\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n\n-- replacing outliers and refitting for 56 genes\n-- DESeq argument 'minReplicatesForReplace' = 7 \n-- original counts are preserved in counts(dds)\n\n\nestimating dispersions\n\n\nfitting model and testing\n\n#extract the results and filter by a FDR cutoff of 0.01 and\n#find significantly differentially abundant OTU between the seasons “paper” and “wood”\nalpha = 0.001\nres_desq = results(ds, contrast=c(\"Carbon_source\", \"paper\", \"wood\"), alpha=alpha)\nres_desq = res_desq[order(res_desq$padj, na.last=NA), ]\nres_sig_deseq = as.data.frame(res_desq[(res_desq$padj &lt; alpha), ])\n\n#print number of significant results\ndim(res_sig_deseq)\n\n[1] 39  6\n\n\nPlot significant OTUs (counts):\n\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_filt)[rownames(res_sig_deseq), ], \"matrix\")),as.data.frame(as(otu_table(physeq_filt)[rownames(res_sig_deseq), ], \"matrix\")))\n\ndf_long &lt;- reshape2::melt(df)\n\nUsing Kingdom, Phylum, Class, Order, Family, Genus as id variables\n\ndf_long &lt;- merge(df_long, metadata2, by.x = \"variable\", by.y = \"sample_id\")\n\nggplot(df_long, aes(x=Genus, y=value, color = Carbon_source)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n  scale_color_manual(values = c( \"orange\", \"purple\")) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_rel_abundance)[rownames(res_sig_deseq), ], \"matrix\")),as.data.frame(as(otu_table(physeq_rel_abundance)[rownames(res_sig_deseq), ], \"matrix\")))\n\ndf_long &lt;- reshape2::melt(df)\n\nUsing Kingdom, Phylum, Class, Order, Family, Genus as id variables\n\ndf_long &lt;- merge(df_long, metadata2, by.x = \"variable\", by.y = \"sample_id\")\n\nggplot(df_long, aes(x=Genus, y=value, color = Carbon_source)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n  scale_color_manual(values = c(\"orange\", \"purple\")) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\n\n# Run ancom-bc2\n# Settings:\n# prv_cut: a numerical fraction between 0 and 1. Taxa with prevalences less than prv_cut will be excluded in the analysis\n# a numerical threshold for filtering samples based on library sizes. Samples with library sizes less than lib_cut\nancom_out = ancombc2(data = physeq_filt,\n               fix_formula = \"Carbon_source\",\n               rand_formula = NULL,\n               p_adj_method = \"holm\", \n               pseudo_sens = FALSE,  #FALSE to speed things up for testing, but makes things less sensitive\n               prv_cut = 0.10, lib_cut = 1000, s0_perc = 0.05,\n               group = \"Carbon_source\",\n               alpha = 0.05, n_cl = 2, verbose = TRUE,\n               global = TRUE)\n\n`tax_level` is not speficified \nNo agglomeration will be performed\nOtherwise, please speficy `tax_level` by one of the following: \nKingdom, Phylum, Class, Order, Family, Genus\n\n\nWarning: The group variable has &lt; 3 categories \nThe multi-group comparisons (global/pairwise/dunnet/trend) will be deactivated\n\n\nObtaining initial estimates ...\n\n\nEstimating sample-specific biases ...\n\n\nANCOM-BC2 primary results ...\n\nres_ancom = ancom_out$res\n\nres_f_fig = res_ancom %&gt;%\n    dplyr::filter(diff_Carbon_sourcewood == TRUE) %&gt;% \n    dplyr::arrange(desc(lfc_Carbon_sourcewood)) %&gt;%\n    dplyr::mutate(direct = ifelse(lfc_Carbon_sourcewood &gt; 0, \"Positive LFC\", \"Negative LFC\"),\n                  color = ifelse(lfc_Carbon_sourcewood &gt; 0, \"aquamarine3\", \"black\"))\n\nres_f_fig$taxon = factor(res_f_fig$taxon, levels = res_f_fig$taxon)\nres_f_fig$direct = factor(res_f_fig$direct, \n                           levels = c(\"Positive LFC\", \"Negative LFC\"))\n\nfig_ancom = res_f_fig %&gt;%\n    ggplot(aes(x = lfc_Carbon_sourcewood, y = taxon, fill = direct)) + \n    geom_bar(stat = \"identity\", width = 0.7, color = \"black\", \n             position = position_dodge(width = 0.4)) +\n    geom_errorbar(aes(xmin = lfc_Carbon_sourcewood - se_Carbon_sourcewood, xmax = lfc_Carbon_sourcewood + se_Carbon_sourcewood), \n                  width = 0.2, position = position_dodge(0.05), color = \"black\") + \n    labs(y = NULL, x = \"Log fold change Wood versus carbon\", \n         title = \"Log fold changes\") + \n    scale_fill_discrete(name = NULL) +\n    scale_color_discrete(name = NULL) +\n    custom_theme() + \n    theme(plot.title = element_text(hjust = 0.5),\n          panel.grid.minor.y = element_blank(),\n          axis.text.y = element_text(size=6))\nfig_ancom\n\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- list(\n  ancom = as.character(res_f_fig$taxon), \n  deseq = as.character(rownames(res_sig_deseq)), \n  wilcox = as.character(wilcox_results$OTU),\n  aldex = sig_aldex2$OTU\n  )\n\nggvenn(\n  x, \n  fill_color = c(\"#0073C2FF\", \"#EFC000FF\", \"#868686FF\", \"#CD534CFF\"),\n  stroke_size = 0.5, set_name_size = 4\n  )\n\n\n\n\n\n\n\n# Get intersection\nv_table &lt;- venn(x, show.plot=FALSE)\nintersections&lt;-attr(v_table,\"intersections\")\nsig_otus_ven &lt;- intersections$`ancom:deseq:wilcox:aldex`\n\n# Plot\ndf &lt;- cbind(as.data.frame(as(tax_table(physeq_filt)[sig_otus_ven, ], \"matrix\")),as.data.frame(as(otu_table(physeq_filt)[sig_otus_ven, ], \"matrix\")))\n\ndf_long &lt;- reshape2::melt(df)\n\nUsing Kingdom, Phylum, Class, Order, Family, Genus as id variables\n\ndf_long &lt;- merge(df_long, metadata2, by.x = \"variable\", by.y = \"sample_id\")\n\nggplot(df_long, aes(x=Genus, y=value, color = Carbon_source)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitterdodge(jitter.width=0.1), size = 0.9)  +\n  custom_theme() +\n   facet_wrap(~Genus, scales = \"free\") +\n  scale_color_manual(values = c( \"orange\", \"purple\")) +\n  theme(axis.text.x = element_text())",
    "crumbs": [
      "Data analysis",
      "OTU table analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Example workflow",
    "section": "",
    "text": "On this page you can find documentation about the workflow to:\n\nClassify 16S amplicon sequences\nAnalyse the data using R",
    "crumbs": [
      "Welcome page"
    ]
  }
]