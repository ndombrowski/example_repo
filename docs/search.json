[
  {
    "objectID": "code/nanoclass_workflow.html",
    "href": "code/nanoclass_workflow.html",
    "title": "Classify Winogradksy samples",
    "section": "",
    "text": "NanoClass2 is used to classify 16S rRNA amplicon sequences from 21 samples. DNA samples were taken from Winogradksy columns with wood or paper as substrate. For each substrate 3 independent columns were sampled and for each columns there are 3-4 replicates. DNA was sampled on two days, by two different groups of students (i.e. Practical_groups 1 and 2):\n\n\n\n#NAME\nSample\nCarbon_source\nWino_Column\nPractical_group\n\n\n\n\nBC22\nGM11\npaper\nNo\n2\n\n\nBC17\nKB-P1\npaper\nP1\n1\n\n\nBC18\nWB-P1\npaper\nP1\n1\n\n\nBC2\nSB-P1\npaper\nP1\n2\n\n\nBC1\nLK-P2\npaper\nP2\n2\n\n\nBC16\nRvO-P2\npaper\nP2\n1\n\n\nBC20\nIV-P2\npaper\nP2\n1\n\n\nBC15\nEK-P3\npaper\nP3\n1\n\n\nBC19\nNK-P3\npaper\nP3\n1\n\n\nBC3\nPM-P3\npaper\nP3\n2\n\n\nBC10\nEP-SD1\nwood\nSD1\n1\n\n\nBC12\nRK-SD1\nwood\nSD1\n1\n\n\nBC23\nEW-SD1\nwood\nSD1\n2\n\n\nBC24\nDG-SD1\nwood\nSD1\n2\n\n\nBC5\nAW-SD2\nwood\nSD2\n2\n\n\nBC6\nUNZ-SD2\nwood\nSD2\n2\n\n\nBC7\nDH-SD2\nwood\nSD2\n2\n\n\nBC11\nAL-SD3\nwood\nSD3\n1\n\n\nBC13\nBS-SD3\nwood\nSD3\n1\n\n\nBC14\nDSR-SD3\nwood\nSD3\n1\n\n\nBC9\nBF-SD3\nwood\nSD3\n1\n\n\n\n\n\n\nFirst, setup a working directory on Crunchomics and move the sequencing data there.\n\n# Go to working directory (on Crunchomics HPC)\nwdir=\"/home/ndombro/personal/teaching/2024/miceco\"\ncd $wdir\n\n# Upload sequencing data from local PC to Crunchomics\nmkdir data \nscp data/mic2024.zip crunchomics:/home/ndombro/personal/teaching/2024/miceco/data\n\n\n\n\n\n\nFirst, let us inspect what we have for each sample:\n\nmkdir filelists \n\n# Unzip data folder\nunzip data/mic2024.zip\nmv mic2024 data\n\n# Make a list of barcode to Day\nls data/mic2024/*/fastq_pass/barcode*/*fastq.gz | \\\n  cut -f3,5 -d \"/\" | \\\n  awk -F \"/\" '{print $2 \"\\t\" $1}' | \\\n  sort -u &gt; filelists/barcode_to_date.txt\n\n# Make a list of barcodes\nls data/mic2024/*/fastq_pass/barcode*/*fastq.gz | \\\n  sed 's/.*barcode\\([0-9]\\+\\).*/barcode\\1/' \\\n  | sort -u &gt; filelists/barcodes.txt\n\n# We work with so many barcodes: 21\nwc -l filelists/barcodes.txt\n\n#create a list of barcodes we work with (based on the excel sheet)\n#nano filelists/barcodes.txt\n\n# Count number of files we work with per barcode\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/mic2024/*/fastq_pass/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\nBarcode barcode01 has 145 fastq files\nBarcode barcode02 has 145 fastq files\nBarcode barcode03 has 145 fastq files\nBarcode barcode05 has 145 fastq files\nBarcode barcode06 has 144 fastq files\nBarcode barcode07 has 145 fastq files\nBarcode barcode09 has 132 fastq files\nBarcode barcode10 has 132 fastq files\nBarcode barcode11 has 132 fastq files\nBarcode barcode12 has 132 fastq files\nBarcode barcode13 has 132 fastq files\nBarcode barcode14 has 132 fastq files\nBarcode barcode15 has 132 fastq files\nBarcode barcode16 has 132 fastq files\nBarcode barcode17 has 132 fastq files\nBarcode barcode18 has 132 fastq files\nBarcode barcode19 has 3 fastq files\nBarcode barcode20 has 132 fastq files\nBarcode barcode22 has 89 fastq files\nBarcode barcode23 has 145 fastq files\nBarcode barcode24 has 144 fastq files\nWe can see that:\n\nfor each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\nBarcode19 seems to have less files, thus we need to keep this in mind and observe read counts more carefully for that sample\n\n\n\n\n\n# Generate folders\nmkdir data/combined_data\n\n# Combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/mic2024/*/fastq_pass/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n# Sanity check: We work with 21 combined files\n# If this would be less than what we originally had, then we might want to check for issues\nll data/combined_data/* | wc -l\n\n\n\n\nNext, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/mic2024/*/fastq_pass/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 21\nWe have so many reads in total: 413,614\nOn average we have: 19,695 reads\nNotice that for barcode 19 and 22 we retain only very few read counts\n\nRead counts before and after merging individual files:\nTotal reads for barcode01 - Before: 60759, After: 60759\nTotal reads for barcode02 - Before: 78436, After: 78436\nTotal reads for barcode03 - Before: 28991, After: 28991\nTotal reads for barcode05 - Before: 4852, After: 4852\nTotal reads for barcode06 - Before: 17883, After: 17883\nTotal reads for barcode07 - Before: 18625, After: 18625\nTotal reads for barcode09 - Before: 12756, After: 12756\nTotal reads for barcode10 - Before: 15666, After: 15666\nTotal reads for barcode11 - Before: 15038, After: 15038\nTotal reads for barcode12 - Before: 7512, After: 7512\nTotal reads for barcode13 - Before: 11099, After: 11099\nTotal reads for barcode14 - Before: 6846, After: 6846\nTotal reads for barcode15 - Before: 14785, After: 14785\nTotal reads for barcode16 - Before: 18654, After: 18654\nTotal reads for barcode17 - Before: 16957, After: 16957\nTotal reads for barcode18 - Before: 30101, After: 30101\nTotal reads for barcode19 - Before: 3, After: 3\nTotal reads for barcode20 - Before: 16945, After: 16945\nTotal reads for barcode22 - Before: 109, After: 109\nTotal reads for barcode23 - Before: 31428, After: 31428\nTotal reads for barcode24 - Before: 6169, After: 6169\n\n\n\n\nmkdir -p results/seqkit\nmkdir -p results/nanoplot\n\n# Run seqkit\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/combined_data/*fastq.gz --threads 10\n\n# Generate plots to visualize the statistics better\nconda activate nanoplot_1.42.0\n\nfor file in data/combined_data/*fastq.gz; do\n  sample=$(echo $file | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\")\n  echo \"Starting analysis for \"$file\"\"\n  mkdir results/nanoplot/\"$sample\"\n  srun --cpus-per-task 10 --mem=50G \\\n    NanoPlot --fastq $file -o results/nanoplot/\"$sample\" --threads 10\ndone\n\nconda deactivate\n\nSummary\n\nReads are on average 1358 bp long with Q1: 1374, Q2: 1425 and Q3: 1454\nVisualizing the plots,\n\nthe majority of the data was hovering around 1400 bp\nThe read quality was more spread and and went from 10 to max 18\n\n\n\n\n\nTo run NanoClass we need:\n\nThe sequence files in fastq.gz format (one file per sample, which we already generated)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor our analysis the file looks something like this:\nrun,sample,barcode,path\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode01.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode02.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode03.fastq.gz\nWe could do this in excel, but we can also extract all the info we need from the file path:\n\necho \"run,sample,barcode,path\" &gt; filelists/mapping_file.csv\n\nls data/combined_data/*fastq.gz | \\\nwhile read path; do\n  run=\"mic2024\" # Set static run name\n  sample=$(echo \"$path\" | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\") # Extract sampleID\n  barcode=$(echo \"$sample\" | sed \"s/barcode/BC/g\")\n  fullpath=$(realpath \"$path\")  # To get the full absolute path\n  echo \"$run,$barcode,,$fullpath\" # Combine all data\ndone &gt;&gt; filelists/mapping_file.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use\n\nThe other file that is useful to have when running NanoClass2 on an HPC is the jobscript.sh, which is pre-configured to submit a job on the Crunchomics server.\nWe first get a copy of the config file (and some other useful files):\n\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/jobscript.sh .\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/config.yaml .\n\nAnd then do the following changes to the file:\nsamples:                           \"filelists/mapping_file.csv\"\nSince I am only running one out of the different classifiers I also change the contents of:\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\nBased on the quality checking I also made some changes. I used the lowest/highest seqkit Q1/Q3 data to set thresholds for the length.\n    minlen:                        1100\n    maxlen:                        1600\n    quality:                       10\n\n\n\n\nSome steps, such as the initial quality filtering and some classifiers, require more computational resources. Therefore, it is beneficial to run NanoClass on a HPC with more memory and cpus and not on your own computer. The code should also run on a standard computer, however, the analyses might take quite some time.\nOn crunchomics we start NanoClass by simply submitting the job script we copied early. This script is pre-configured to do everything for you such as:\n\nLoad the correct environments that have all required tools installed\nStart NanoClass2 using snakemake, a job scheduler that takes care of what software is run when. Snakemake is useful as it allows to run things in parallel as long as there are enough resources available. Snakemake also allows to re-start a job and it will simply pick things up from whereever things went wrong.\nStart job: Tue Oct 24 16:07:12\nFinished job: Tue Oct 24 18:53:32\n\n\n# Do a dry-run to see if we set everything correctly \n# Since this is just a test, we can run this on the headnode \nconda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\n\nsnakemake \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml --use-conda \\\n    --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --cores 1 --nolock --rerun-incomplete -np\n\nconda deactivate \n\n# Submit job via a job script to the compute node: job id is 74581\nsbatch jobscript.sh\n\n# Run seqkit on the cleaned data\nseqkit stats -a -To results/seqkit/seqkit_stats_filtered.tsv results/data/mic2024/chopper/BC*gz --threads 10\n\n\n# cleanup (not needed)\nrm -r results/benchmarks\nrm -r results/data\nrm -r results/logs\nrm -r results/plots\nrm -r results/stats",
    "crumbs": [
      "Classify Winogradksy samples"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#project-description",
    "href": "code/nanoclass_workflow.html#project-description",
    "title": "Classify Winogradksy samples",
    "section": "",
    "text": "NanoClass2 is used to classify 16S rRNA amplicon sequences from 21 samples. DNA samples were taken from Winogradksy columns with wood or paper as substrate. For each substrate 3 independent columns were sampled and for each columns there are 3-4 replicates. DNA was sampled on two days, by two different groups of students (i.e. Practical_groups 1 and 2):\n\n\n\n#NAME\nSample\nCarbon_source\nWino_Column\nPractical_group\n\n\n\n\nBC22\nGM11\npaper\nNo\n2\n\n\nBC17\nKB-P1\npaper\nP1\n1\n\n\nBC18\nWB-P1\npaper\nP1\n1\n\n\nBC2\nSB-P1\npaper\nP1\n2\n\n\nBC1\nLK-P2\npaper\nP2\n2\n\n\nBC16\nRvO-P2\npaper\nP2\n1\n\n\nBC20\nIV-P2\npaper\nP2\n1\n\n\nBC15\nEK-P3\npaper\nP3\n1\n\n\nBC19\nNK-P3\npaper\nP3\n1\n\n\nBC3\nPM-P3\npaper\nP3\n2\n\n\nBC10\nEP-SD1\nwood\nSD1\n1\n\n\nBC12\nRK-SD1\nwood\nSD1\n1\n\n\nBC23\nEW-SD1\nwood\nSD1\n2\n\n\nBC24\nDG-SD1\nwood\nSD1\n2\n\n\nBC5\nAW-SD2\nwood\nSD2\n2\n\n\nBC6\nUNZ-SD2\nwood\nSD2\n2\n\n\nBC7\nDH-SD2\nwood\nSD2\n2\n\n\nBC11\nAL-SD3\nwood\nSD3\n1\n\n\nBC13\nBS-SD3\nwood\nSD3\n1\n\n\nBC14\nDSR-SD3\nwood\nSD3\n1\n\n\nBC9\nBF-SD3\nwood\nSD3\n1",
    "crumbs": [
      "Classify Winogradksy samples"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#folder-setup",
    "href": "code/nanoclass_workflow.html#folder-setup",
    "title": "Classify Winogradksy samples",
    "section": "",
    "text": "First, setup a working directory on Crunchomics and move the sequencing data there.\n\n# Go to working directory (on Crunchomics HPC)\nwdir=\"/home/ndombro/personal/teaching/2024/miceco\"\ncd $wdir\n\n# Upload sequencing data from local PC to Crunchomics\nmkdir data \nscp data/mic2024.zip crunchomics:/home/ndombro/personal/teaching/2024/miceco/data",
    "crumbs": [
      "Classify Winogradksy samples"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#prepare-input-files",
    "href": "code/nanoclass_workflow.html#prepare-input-files",
    "title": "Classify Winogradksy samples",
    "section": "",
    "text": "First, let us inspect what we have for each sample:\n\nmkdir filelists \n\n# Unzip data folder\nunzip data/mic2024.zip\nmv mic2024 data\n\n# Make a list of barcode to Day\nls data/mic2024/*/fastq_pass/barcode*/*fastq.gz | \\\n  cut -f3,5 -d \"/\" | \\\n  awk -F \"/\" '{print $2 \"\\t\" $1}' | \\\n  sort -u &gt; filelists/barcode_to_date.txt\n\n# Make a list of barcodes\nls data/mic2024/*/fastq_pass/barcode*/*fastq.gz | \\\n  sed 's/.*barcode\\([0-9]\\+\\).*/barcode\\1/' \\\n  | sort -u &gt; filelists/barcodes.txt\n\n# We work with so many barcodes: 21\nwc -l filelists/barcodes.txt\n\n#create a list of barcodes we work with (based on the excel sheet)\n#nano filelists/barcodes.txt\n\n# Count number of files we work with per barcode\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/mic2024/*/fastq_pass/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\nBarcode barcode01 has 145 fastq files\nBarcode barcode02 has 145 fastq files\nBarcode barcode03 has 145 fastq files\nBarcode barcode05 has 145 fastq files\nBarcode barcode06 has 144 fastq files\nBarcode barcode07 has 145 fastq files\nBarcode barcode09 has 132 fastq files\nBarcode barcode10 has 132 fastq files\nBarcode barcode11 has 132 fastq files\nBarcode barcode12 has 132 fastq files\nBarcode barcode13 has 132 fastq files\nBarcode barcode14 has 132 fastq files\nBarcode barcode15 has 132 fastq files\nBarcode barcode16 has 132 fastq files\nBarcode barcode17 has 132 fastq files\nBarcode barcode18 has 132 fastq files\nBarcode barcode19 has 3 fastq files\nBarcode barcode20 has 132 fastq files\nBarcode barcode22 has 89 fastq files\nBarcode barcode23 has 145 fastq files\nBarcode barcode24 has 144 fastq files\nWe can see that:\n\nfor each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\nBarcode19 seems to have less files, thus we need to keep this in mind and observe read counts more carefully for that sample\n\n\n\n\n\n# Generate folders\nmkdir data/combined_data\n\n# Combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/mic2024/*/fastq_pass/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n# Sanity check: We work with 21 combined files\n# If this would be less than what we originally had, then we might want to check for issues\nll data/combined_data/* | wc -l\n\n\n\n\nNext, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/mic2024/*/fastq_pass/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 21\nWe have so many reads in total: 413,614\nOn average we have: 19,695 reads\nNotice that for barcode 19 and 22 we retain only very few read counts\n\nRead counts before and after merging individual files:\nTotal reads for barcode01 - Before: 60759, After: 60759\nTotal reads for barcode02 - Before: 78436, After: 78436\nTotal reads for barcode03 - Before: 28991, After: 28991\nTotal reads for barcode05 - Before: 4852, After: 4852\nTotal reads for barcode06 - Before: 17883, After: 17883\nTotal reads for barcode07 - Before: 18625, After: 18625\nTotal reads for barcode09 - Before: 12756, After: 12756\nTotal reads for barcode10 - Before: 15666, After: 15666\nTotal reads for barcode11 - Before: 15038, After: 15038\nTotal reads for barcode12 - Before: 7512, After: 7512\nTotal reads for barcode13 - Before: 11099, After: 11099\nTotal reads for barcode14 - Before: 6846, After: 6846\nTotal reads for barcode15 - Before: 14785, After: 14785\nTotal reads for barcode16 - Before: 18654, After: 18654\nTotal reads for barcode17 - Before: 16957, After: 16957\nTotal reads for barcode18 - Before: 30101, After: 30101\nTotal reads for barcode19 - Before: 3, After: 3\nTotal reads for barcode20 - Before: 16945, After: 16945\nTotal reads for barcode22 - Before: 109, After: 109\nTotal reads for barcode23 - Before: 31428, After: 31428\nTotal reads for barcode24 - Before: 6169, After: 6169\n\n\n\n\nmkdir -p results/seqkit\nmkdir -p results/nanoplot\n\n# Run seqkit\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/combined_data/*fastq.gz --threads 10\n\n# Generate plots to visualize the statistics better\nconda activate nanoplot_1.42.0\n\nfor file in data/combined_data/*fastq.gz; do\n  sample=$(echo $file | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\")\n  echo \"Starting analysis for \"$file\"\"\n  mkdir results/nanoplot/\"$sample\"\n  srun --cpus-per-task 10 --mem=50G \\\n    NanoPlot --fastq $file -o results/nanoplot/\"$sample\" --threads 10\ndone\n\nconda deactivate\n\nSummary\n\nReads are on average 1358 bp long with Q1: 1374, Q2: 1425 and Q3: 1454\nVisualizing the plots,\n\nthe majority of the data was hovering around 1400 bp\nThe read quality was more spread and and went from 10 to max 18\n\n\n\n\n\nTo run NanoClass we need:\n\nThe sequence files in fastq.gz format (one file per sample, which we already generated)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor our analysis the file looks something like this:\nrun,sample,barcode,path\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode01.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode02.fastq.gz\nmic2024,BC01,,/home/ndombro/personal/teaching/2024/miceco/data/combined_data/barcode03.fastq.gz\nWe could do this in excel, but we can also extract all the info we need from the file path:\n\necho \"run,sample,barcode,path\" &gt; filelists/mapping_file.csv\n\nls data/combined_data/*fastq.gz | \\\nwhile read path; do\n  run=\"mic2024\" # Set static run name\n  sample=$(echo \"$path\" | cut -d \"/\" -f3 | sed \"s/\\.fastq.gz//g\") # Extract sampleID\n  barcode=$(echo \"$sample\" | sed \"s/barcode/BC/g\")\n  fullpath=$(realpath \"$path\")  # To get the full absolute path\n  echo \"$run,$barcode,,$fullpath\" # Combine all data\ndone &gt;&gt; filelists/mapping_file.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use\n\nThe other file that is useful to have when running NanoClass2 on an HPC is the jobscript.sh, which is pre-configured to submit a job on the Crunchomics server.\nWe first get a copy of the config file (and some other useful files):\n\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/jobscript.sh .\ncp /zfs/omics/projects/amplicomics/bin/NanoClass2/config.yaml .\n\nAnd then do the following changes to the file:\nsamples:                           \"filelists/mapping_file.csv\"\nSince I am only running one out of the different classifiers I also change the contents of:\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\nBased on the quality checking I also made some changes. I used the lowest/highest seqkit Q1/Q3 data to set thresholds for the length.\n    minlen:                        1100\n    maxlen:                        1600\n    quality:                       10",
    "crumbs": [
      "Classify Winogradksy samples"
    ]
  },
  {
    "objectID": "code/nanoclass_workflow.html#run-nanoclass",
    "href": "code/nanoclass_workflow.html#run-nanoclass",
    "title": "Classify Winogradksy samples",
    "section": "",
    "text": "Some steps, such as the initial quality filtering and some classifiers, require more computational resources. Therefore, it is beneficial to run NanoClass on a HPC with more memory and cpus and not on your own computer. The code should also run on a standard computer, however, the analyses might take quite some time.\nOn crunchomics we start NanoClass by simply submitting the job script we copied early. This script is pre-configured to do everything for you such as:\n\nLoad the correct environments that have all required tools installed\nStart NanoClass2 using snakemake, a job scheduler that takes care of what software is run when. Snakemake is useful as it allows to run things in parallel as long as there are enough resources available. Snakemake also allows to re-start a job and it will simply pick things up from whereever things went wrong.\nStart job: Tue Oct 24 16:07:12\nFinished job: Tue Oct 24 18:53:32\n\n\n# Do a dry-run to see if we set everything correctly \n# Since this is just a test, we can run this on the headnode \nconda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake_nanoclass2\n\nsnakemake \\\n    -s /zfs/omics/projects/amplicomics/bin/NanoClass2/Snakefile \\\n    --configfile config.yaml --use-conda \\\n    --conda-prefix /zfs/omics/projects/amplicomics/bin/NanoClass2/.snakemake/conda \\\n    --cores 1 --nolock --rerun-incomplete -np\n\nconda deactivate \n\n# Submit job via a job script to the compute node: job id is 74581\nsbatch jobscript.sh\n\n# Run seqkit on the cleaned data\nseqkit stats -a -To results/seqkit/seqkit_stats_filtered.tsv results/data/mic2024/chopper/BC*gz --threads 10\n\n\n# cleanup (not needed)\nrm -r results/benchmarks\nrm -r results/data\nrm -r results/logs\nrm -r results/plots\nrm -r results/stats",
    "crumbs": [
      "Classify Winogradksy samples"
    ]
  },
  {
    "objectID": "old/nanoclass_readme.html",
    "href": "old/nanoclass_readme.html",
    "title": "NanoClass",
    "section": "",
    "text": "NanoClass is a taxonomic meta-classifier for meta-barcoding sequencing data generated with the Oxford Nanopore Technology. With a single command, this Snakemake pipeline installs all programs and databases and runs and evaluates 10 popular taxonomic classification tools.\nThe tool was written by Evelyn Jongepier. For the github code go here and check out the documentation found here.\nThe notes found here are specific instructions to set up Nanoclass on the UvA Crunchomics HPC. In theory this should run on your own computers as well.\n\n\n\n\nNanoClass uses the workflow manager Snakemake. To run the Nanoclass we need to download Nanoclass and install Snakemake. Afterwards, Snakemake will install any other required dependencies by itself. Note, that Snakemake requires conda/mamba to manage dependenices and need to be installed as well if they are not already available on your computing system.\n\nwdir=\"/home/ndombro/personal/teaching/2023/miceco\"\ncd $wdir\n\n#download nanoclass\n#git clone https://github.com/ejongepier/NanoClass\n\n#install snakemake (if not already installed)\n#mamba create -c conda-forge -c bioconda -n snakemake_nanoclass python=3.9.7 snakemake=6.8.0 tabulate=0.8\n\n\n\n\n\n\nFirst, let us inspect what we have for each sample:\n\n#create a list of barcodes we work with (based on the excel sheet)\n#nano filelists/barcodes.txt\n\n#combine individual files\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/raw_data/*/*/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\n**Group 1**\nBarcode Abarcode01 has 4 fastq files\nBarcode Abarcode02 has 9 fastq files\nBarcode Abarcode03 has 10 fastq files\nBarcode Abarcode04 has 7 fastq files\nBarcode Abarcode05 has 12 fastq files\nBarcode Abarcode06 has 1 fastq files\nBarcode Abarcode07 has 10 fastq files\nBarcode Abarcode14 has 4 fastq files\nBarcode Abarcode15 has 3 fastq files\nBarcode Abarcode19 has 11 fastq files\nBarcode Abarcode20 has 11 fastq files\nBarcode Abarcode21 has 12 fastq files\nBarcode Abarcode23 has 11 fastq files\nBarcode Abarcode24 has 13 fastq files\n\n**Group 2**\nBarcode Bbarcode02 has 25 fastq files\nBarcode Bbarcode03 has 37 fastq files\nBarcode Bbarcode04 has 8 fastq files\nBarcode Bbarcode05 has 9 fastq files\nBarcode Bbarcode08 has 10 fastq files\nBarcode Bbarcode13 has 47 fastq files\nBarcode Bbarcode15 has 26 fastq files\nBarcode Bbarcode16 has 37 fastq files\nBarcode Bbarcode17 has 5 fastq files\nBarcode Bbarcode18 has 1 fastq files\nBarcode Bbarcode19 has 10 fastq files\nBarcode Bbarcode20 has 25 fastq files\nBarcode Bbarcode21 has 2 fastq files\nBarcode Bbarcode22 has 8 fastq files\nWe can see that for each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\n\n\n\n\n#generate folders\nmkdir -p data/raw_data \nmkdir data/combined_data\nmkdir NanoClass/cluster\nmkdir filelists\n\n#move data from local to crunchomics\nscp -r mic23_* crunchomics:/home/ndombro/personal/teaching/2023/miceco/data/raw_data\n\n#give unique ids for barcodes for different days (A=day1 , B=day2)\nfind data/raw_data/mic23_1*/fastq_pass/ -type d -exec sh -c 'mv \"$0\" \"${0%/*}/A${0##*/}\"' {} \\;\nfind data/raw_data/mic23_2*/fastq_pass/ -type d -exec sh -c 'mv \"$0\" \"${0%/*}/B${0##*/}\"' {} \\;\n\n#combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/raw_data/*/*/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n#control file content\nll data/combined_data/* \n\n#rm not existing/empty files\nrm data/combined_data/Abarcode08.fastq.gz\nrm data/combined_data/Abarcode22.fastq.gz\nrm data/combined_data/Bbarcode07.fastq.gz\n\n#count number of final files: 28 \nll data/combined_data/* \nll data/combined_data/* | wc -l\n\nNote:\nWe had missing files for:\ncat: data/raw_data/*/*/Abarcode08/*fastq.gz: No such file or directory (no seq data, deleted)\ncat: data/raw_data/*/*/Abarcode22/*fastq.gz: No such file or directory (no seq data, deleted)\ncat: data/raw_data/*/*/Bbarcode07/*fastq.gz: No such file or directory (not in list, deleted)\nThese files are missing because the sequencing yielded now results. Therefore, the IDs will be deleted from the mapping file and proceeded with the workflow.\n\n\n\nNext, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/raw_data/*/*/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 28\nWe have so many reads in total: 353,448\nOn average we have: 12,623 reads\n\nRead counts before and after merging individual files:\n**Group 1**\nTotal reads for Abarcode01 - Before: 3858, After: 3858\nTotal reads for Abarcode02 - Before: 8730, After: 8730\nTotal reads for Abarcode03 - Before: 9531, After: 9531\nTotal reads for Abarcode04 - Before: 6267, After: 6267\nTotal reads for Abarcode05 - Before: 11550, After: 11550\nTotal reads for Abarcode06 - Before: 2, After: 2\nTotal reads for Abarcode07 - Before: 9787, After: 9787\nTotal reads for Abarcode14 - Before: 3822, After: 3822\nTotal reads for Abarcode15 - Before: 2648, After: 2648\nTotal reads for Abarcode19 - Before: 10097, After: 10097\nTotal reads for Abarcode20 - Before: 10168, After: 10168\nTotal reads for Abarcode21 - Before: 11911, After: 11911\nTotal reads for Abarcode23 - Before: 10520, After: 10520\nTotal reads for Abarcode24 - Before: 12009, After: 12009\n\n**Group 2**\nTotal reads for Bbarcode02 - Before: 24355, After: 24355\nTotal reads for Bbarcode03 - Before: 36716, After: 36716\nTotal reads for Bbarcode04 - Before: 7327, After: 7327\nTotal reads for Bbarcode05 - Before: 8748, After: 8748\nTotal reads for Bbarcode08 - Before: 9199, After: 9199\nTotal reads for Bbarcode13 - Before: 46139, After: 46139\nTotal reads for Bbarcode15 - Before: 25380, After: 25380\nTotal reads for Bbarcode16 - Before: 36357, After: 36357\nTotal reads for Bbarcode17 - Before: 4911, After: 4911\nTotal reads for Bbarcode18 - Before: 26, After: 26\nTotal reads for Bbarcode19 - Before: 9899, After: 9899\nTotal reads for Bbarcode20 - Before: 24549, After: 24549\nTotal reads for Bbarcode21 - Before: 1358, After: 1358\nTotal reads for Bbarcode22 - Before: 7584, After: 7584\n\n\n\nTo run NanoClass we need:\n\nThe sequence files in fastq.gz format (one file per sample)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor our analysis the file looks something like this:\nrun,sample,barcode,path\nmiceco,EP1910,bc01,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode01.fastq.gz\nmiceco,RMT,bc02,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode02.fastq.gz\nmiceco,KJB3,bc03,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode03.fastq.gz\nSince I copy/pasted that information from excel, I ran a small line of code to convert it from a tab to a comma separated file:\n\nsed -i 's/\\t/,/g'  filelists/samples.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use (more details on that a bit later)\n\nFor this data I change the following line:\nsamples:                           \"/home/ndombro/personal/teaching/2023/miceco/filelists/samples.csv\"\nSince I am only running one out of the different classifiers I also change the contents of:\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\n\n\n\nThe jobscript.sh file allows to run jobs in an efficient manner on an HPC and make use of the high numbers of CPUs available on HPCs. I edited the file as follows:\n\nadd a # before configfile: “config.yaml” (this we need to separate the generated output files better from the workflow)\nchange cmd=\"srun --cores $SLURM_CPUS_ON_NODE snakemake --use-conda --cores $SLURM_CPUS_ON_NODE --nolock --rerun-incomplete\" to cmd=\"srun --cores $SLURM_CPUS_ON_NODE snakemake --use-conda -s Snakefile --configfile config.yaml --conda-prefix .snakemake/conda --cores 1 --nolock --rerun-incomplete\" (change was done for different file organization)\nIf you use mamba and do not have conda installed: change instances of miniconda3 to mambaforge (since I do not have conda installed)\nChange path to snakemake from conda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake-ampl to where your snakemake tool is installed, i.e. conda activate /home/ndombro/personal/mambaforge/envs/snakemake_nanoclass.\nadd LC_ALL info to bash script (this solves an issue with running R) if LC_ALL is not set in your HPC environment. To check if LC_ALL is set run echo $LC_ALL. If it returns something you do not need to add the following lines to your jobscript.\n\n# Set LC_ALL and export it\nexport LC_ALL=en_US.UTF-8\n\n\n\n\nEdited the envs/R4.0.yml to include the python version and rlang\n\nname: R4.0\nchannels:\n  - defaults\n  - bioconda\n  - conda-forge\ndependencies:\n  - python=3.9.7\n  - r-essentials\n  - r-base\n  - r-rlang=1.0.5\n  - r=4.0\n  - bioconductor-phyloseq\n  - bioconductor-dada2\n  - r-seqinr\n  - bioconductor-decipher\n  - r-vroom\n  - pip\n  - pip:\n    - biopython==1.78\nEdits for scripts/tomat.py (this ensures that the taxonomy strings in the OTU table are separated by a ;)\n\nin def get_otumat_dict(self) change taxid = '_'.join(tax) to taxid = ';'.join(tax)\nin def get_taxmat_dict(self) change taxid = '_'.join(tax) to taxid = ';'.join(tax)\n\n\n\n\n\nSome steps, such as the initial quality filtering and some classifiers, require more computational resources. Therefore, it is beneficial to run NanoClass on a HPC with more memory and cpus. The code should also run on a standard computer, however, the analyses might take quite some time.\nOn crunchomics:\n\nStart job: Tue Oct 24 16:07:12\nFinished job: Tue Oct 24 18:53:32\n\n\n#go into the folder with the pipeline\ncd NanoClass \n\n#test run to see if everything works\n#if returns no error run jobscript\nsnakemake --use-conda -s Snakefile --configfile config.yaml --conda-prefix .snakemake/conda --cores 1 --nolock --rerun-incomplete -np\n\n#submit job\nsbatch jobscript.sh\n\n#create report\nsnakemake --report\nconda deactivate\n\n#organize the files (i.e. separate workflow from output)\nmkdir ../results \nmv classifications ../results/\nmv benchmarks ../results/\nmv logs ../results/\nmv plots ../results/\nmv stats ../results/"
  },
  {
    "objectID": "old/nanoclass_readme.html#description",
    "href": "old/nanoclass_readme.html#description",
    "title": "NanoClass",
    "section": "",
    "text": "NanoClass is a taxonomic meta-classifier for meta-barcoding sequencing data generated with the Oxford Nanopore Technology. With a single command, this Snakemake pipeline installs all programs and databases and runs and evaluates 10 popular taxonomic classification tools.\nThe tool was written by Evelyn Jongepier. For the github code go here and check out the documentation found here.\nThe notes found here are specific instructions to set up Nanoclass on the UvA Crunchomics HPC. In theory this should run on your own computers as well."
  },
  {
    "objectID": "old/nanoclass_readme.html#installation",
    "href": "old/nanoclass_readme.html#installation",
    "title": "NanoClass",
    "section": "",
    "text": "NanoClass uses the workflow manager Snakemake. To run the Nanoclass we need to download Nanoclass and install Snakemake. Afterwards, Snakemake will install any other required dependencies by itself. Note, that Snakemake requires conda/mamba to manage dependenices and need to be installed as well if they are not already available on your computing system.\n\nwdir=\"/home/ndombro/personal/teaching/2023/miceco\"\ncd $wdir\n\n#download nanoclass\n#git clone https://github.com/ejongepier/NanoClass\n\n#install snakemake (if not already installed)\n#mamba create -c conda-forge -c bioconda -n snakemake_nanoclass python=3.9.7 snakemake=6.8.0 tabulate=0.8"
  },
  {
    "objectID": "old/nanoclass_readme.html#prepare-input-files",
    "href": "old/nanoclass_readme.html#prepare-input-files",
    "title": "NanoClass",
    "section": "",
    "text": "First, let us inspect what we have for each sample:\n\n#create a list of barcodes we work with (based on the excel sheet)\n#nano filelists/barcodes.txt\n\n#combine individual files\nfor i in `cat filelists/barcodes.txt`; do\n    count=$((ll data/raw_data/*/*/${i}/*fastq.gz) | wc -l)\n    echo \"Barcode ${i} has ${count} fastq files\"\ndone\n\nResults\n**Group 1**\nBarcode Abarcode01 has 4 fastq files\nBarcode Abarcode02 has 9 fastq files\nBarcode Abarcode03 has 10 fastq files\nBarcode Abarcode04 has 7 fastq files\nBarcode Abarcode05 has 12 fastq files\nBarcode Abarcode06 has 1 fastq files\nBarcode Abarcode07 has 10 fastq files\nBarcode Abarcode14 has 4 fastq files\nBarcode Abarcode15 has 3 fastq files\nBarcode Abarcode19 has 11 fastq files\nBarcode Abarcode20 has 11 fastq files\nBarcode Abarcode21 has 12 fastq files\nBarcode Abarcode23 has 11 fastq files\nBarcode Abarcode24 has 13 fastq files\n\n**Group 2**\nBarcode Bbarcode02 has 25 fastq files\nBarcode Bbarcode03 has 37 fastq files\nBarcode Bbarcode04 has 8 fastq files\nBarcode Bbarcode05 has 9 fastq files\nBarcode Bbarcode08 has 10 fastq files\nBarcode Bbarcode13 has 47 fastq files\nBarcode Bbarcode15 has 26 fastq files\nBarcode Bbarcode16 has 37 fastq files\nBarcode Bbarcode17 has 5 fastq files\nBarcode Bbarcode18 has 1 fastq files\nBarcode Bbarcode19 has 10 fastq files\nBarcode Bbarcode20 has 25 fastq files\nBarcode Bbarcode21 has 2 fastq files\nBarcode Bbarcode22 has 8 fastq files\nWe can see that for each sample, we have several fastq files. Therefore, we first want to combine them into one single file per sample before running NanoClass.\n\n\n\n\n#generate folders\nmkdir -p data/raw_data \nmkdir data/combined_data\nmkdir NanoClass/cluster\nmkdir filelists\n\n#move data from local to crunchomics\nscp -r mic23_* crunchomics:/home/ndombro/personal/teaching/2023/miceco/data/raw_data\n\n#give unique ids for barcodes for different days (A=day1 , B=day2)\nfind data/raw_data/mic23_1*/fastq_pass/ -type d -exec sh -c 'mv \"$0\" \"${0%/*}/A${0##*/}\"' {} \\;\nfind data/raw_data/mic23_2*/fastq_pass/ -type d -exec sh -c 'mv \"$0\" \"${0%/*}/B${0##*/}\"' {} \\;\n\n#combine individual files/sample\nfor i in `cat filelists/barcodes.txt`; do\n    cat data/raw_data/*/*/${i}/*fastq.gz &gt; data/combined_data/${i}.fastq.gz\ndone\n\n#control file content\nll data/combined_data/* \n\n#rm not existing/empty files\nrm data/combined_data/Abarcode08.fastq.gz\nrm data/combined_data/Abarcode22.fastq.gz\nrm data/combined_data/Bbarcode07.fastq.gz\n\n#count number of final files: 28 \nll data/combined_data/* \nll data/combined_data/* | wc -l\n\nNote:\nWe had missing files for:\ncat: data/raw_data/*/*/Abarcode08/*fastq.gz: No such file or directory (no seq data, deleted)\ncat: data/raw_data/*/*/Abarcode22/*fastq.gz: No such file or directory (no seq data, deleted)\ncat: data/raw_data/*/*/Bbarcode07/*fastq.gz: No such file or directory (not in list, deleted)\nThese files are missing because the sequencing yielded now results. Therefore, the IDs will be deleted from the mapping file and proceeded with the workflow.\n\n\n\nNext, we calculate the read counts before and after combining the individual files. This is useful to know how many reads we work with but also to see whether the merge worked correctly.\n\n# count total number of reads\ntotal_after_reads=0\ntotal_files=0\n\nfor i in $(cat filelists/barcodes.txt); do\n    #read counts before combining\n    before_lines=$(zcat data/raw_data/*/*/${i}/*fastq.gz | wc -l)\n    before_count=$((before_lines / 4))\n\n    #read counts after combining\n    after_lines=$(zcat data/combined_data/${i}.fastq.gz | wc -l)\n    after_count=$((after_lines / 4))\n\n    # accumulate total after reads and files\n    total_after_reads=$((total_after_reads + after_count))\n    total_files=$((total_files + 1))\n\n    echo \"Total reads for ${i} - Before: ${before_count}, After: ${after_count}\"\ndone\n\n# calculate average after count\naverage_after_count=$((total_after_reads / total_files))\n\necho \"We have so many samples: ${total_files}\"\necho \"We have so many reads in total: ${total_after_reads}\"\necho \"Average read count for the combined barcodes: ${average_after_count}\"\n\nResults\n\nWe have so many samples: 28\nWe have so many reads in total: 353,448\nOn average we have: 12,623 reads\n\nRead counts before and after merging individual files:\n**Group 1**\nTotal reads for Abarcode01 - Before: 3858, After: 3858\nTotal reads for Abarcode02 - Before: 8730, After: 8730\nTotal reads for Abarcode03 - Before: 9531, After: 9531\nTotal reads for Abarcode04 - Before: 6267, After: 6267\nTotal reads for Abarcode05 - Before: 11550, After: 11550\nTotal reads for Abarcode06 - Before: 2, After: 2\nTotal reads for Abarcode07 - Before: 9787, After: 9787\nTotal reads for Abarcode14 - Before: 3822, After: 3822\nTotal reads for Abarcode15 - Before: 2648, After: 2648\nTotal reads for Abarcode19 - Before: 10097, After: 10097\nTotal reads for Abarcode20 - Before: 10168, After: 10168\nTotal reads for Abarcode21 - Before: 11911, After: 11911\nTotal reads for Abarcode23 - Before: 10520, After: 10520\nTotal reads for Abarcode24 - Before: 12009, After: 12009\n\n**Group 2**\nTotal reads for Bbarcode02 - Before: 24355, After: 24355\nTotal reads for Bbarcode03 - Before: 36716, After: 36716\nTotal reads for Bbarcode04 - Before: 7327, After: 7327\nTotal reads for Bbarcode05 - Before: 8748, After: 8748\nTotal reads for Bbarcode08 - Before: 9199, After: 9199\nTotal reads for Bbarcode13 - Before: 46139, After: 46139\nTotal reads for Bbarcode15 - Before: 25380, After: 25380\nTotal reads for Bbarcode16 - Before: 36357, After: 36357\nTotal reads for Bbarcode17 - Before: 4911, After: 4911\nTotal reads for Bbarcode18 - Before: 26, After: 26\nTotal reads for Bbarcode19 - Before: 9899, After: 9899\nTotal reads for Bbarcode20 - Before: 24549, After: 24549\nTotal reads for Bbarcode21 - Before: 1358, After: 1358\nTotal reads for Bbarcode22 - Before: 7584, After: 7584\n\n\n\nTo run NanoClass we need:\n\nThe sequence files in fastq.gz format (one file per sample)\nA csv mapping file that lists what samples you want to have analysed and how the samples are named\n\nThe csv mapping file needs to include the following:\n\nA run name\nA unique sample name\nThe barcode name (optional)\nThe path to your fastq.gz files (should be already demultiplexed)\nNotice: Sample and run labels can only contain letters and numbers\n\nFor our analysis the file looks something like this:\nrun,sample,barcode,path\nmiceco,EP1910,bc01,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode01.fastq.gz\nmiceco,RMT,bc02,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode02.fastq.gz\nmiceco,KJB3,bc03,/home/ndombro/personal/teaching/2023/miceco/data/combined_data/Abarcode03.fastq.gz\nSince I copy/pasted that information from excel, I ran a small line of code to convert it from a tab to a comma separated file:\n\nsed -i 's/\\t/,/g'  filelists/samples.csv\n\n\n\n\nThe Snakemake configuration file, i.e. config.yaml, allows to adjust parameters, such as the name of the mapping file or the parameters used by different tools, outside of Snakemake. The key things to change are:\n\nThe location of your sample mapping file under samples:\nThe methods you want to use (more details on that a bit later)\n\nFor this data I change the following line:\nsamples:                           \"/home/ndombro/personal/teaching/2023/miceco/filelists/samples.csv\"\nSince I am only running one out of the different classifiers I also change the contents of:\nmethods:                           [\"minimap\"]\nSince I do not want to subsample (useful if we want to reduce the data size for testing more than one classifier), I also edited this line:\nsubsample   \n  skip:                          true\n\n\n\nThe jobscript.sh file allows to run jobs in an efficient manner on an HPC and make use of the high numbers of CPUs available on HPCs. I edited the file as follows:\n\nadd a # before configfile: “config.yaml” (this we need to separate the generated output files better from the workflow)\nchange cmd=\"srun --cores $SLURM_CPUS_ON_NODE snakemake --use-conda --cores $SLURM_CPUS_ON_NODE --nolock --rerun-incomplete\" to cmd=\"srun --cores $SLURM_CPUS_ON_NODE snakemake --use-conda -s Snakefile --configfile config.yaml --conda-prefix .snakemake/conda --cores 1 --nolock --rerun-incomplete\" (change was done for different file organization)\nIf you use mamba and do not have conda installed: change instances of miniconda3 to mambaforge (since I do not have conda installed)\nChange path to snakemake from conda activate /zfs/omics/projects/amplicomics/miniconda3/envs/snakemake-ampl to where your snakemake tool is installed, i.e. conda activate /home/ndombro/personal/mambaforge/envs/snakemake_nanoclass.\nadd LC_ALL info to bash script (this solves an issue with running R) if LC_ALL is not set in your HPC environment. To check if LC_ALL is set run echo $LC_ALL. If it returns something you do not need to add the following lines to your jobscript.\n\n# Set LC_ALL and export it\nexport LC_ALL=en_US.UTF-8\n\n\n\n\nEdited the envs/R4.0.yml to include the python version and rlang\n\nname: R4.0\nchannels:\n  - defaults\n  - bioconda\n  - conda-forge\ndependencies:\n  - python=3.9.7\n  - r-essentials\n  - r-base\n  - r-rlang=1.0.5\n  - r=4.0\n  - bioconductor-phyloseq\n  - bioconductor-dada2\n  - r-seqinr\n  - bioconductor-decipher\n  - r-vroom\n  - pip\n  - pip:\n    - biopython==1.78\nEdits for scripts/tomat.py (this ensures that the taxonomy strings in the OTU table are separated by a ;)\n\nin def get_otumat_dict(self) change taxid = '_'.join(tax) to taxid = ';'.join(tax)\nin def get_taxmat_dict(self) change taxid = '_'.join(tax) to taxid = ';'.join(tax)"
  },
  {
    "objectID": "old/nanoclass_readme.html#run-nanoclass",
    "href": "old/nanoclass_readme.html#run-nanoclass",
    "title": "NanoClass",
    "section": "",
    "text": "Some steps, such as the initial quality filtering and some classifiers, require more computational resources. Therefore, it is beneficial to run NanoClass on a HPC with more memory and cpus. The code should also run on a standard computer, however, the analyses might take quite some time.\nOn crunchomics:\n\nStart job: Tue Oct 24 16:07:12\nFinished job: Tue Oct 24 18:53:32\n\n\n#go into the folder with the pipeline\ncd NanoClass \n\n#test run to see if everything works\n#if returns no error run jobscript\nsnakemake --use-conda -s Snakefile --configfile config.yaml --conda-prefix .snakemake/conda --cores 1 --nolock --rerun-incomplete -np\n\n#submit job\nsbatch jobscript.sh\n\n#create report\nsnakemake --report\nconda deactivate\n\n#organize the files (i.e. separate workflow from output)\nmkdir ../results \nmv classifications ../results/\nmv benchmarks ../results/\nmv logs ../results/\nmv plots ../results/\nmv stats ../results/"
  }
]